{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Retreival QA With Multiple Files and File Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader, DirectoryLoader\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"..\\openai.yaml\") as f:\n",
    "    spec = yaml.safe_load(f)\n",
    "    key = spec['openai']['key']\n",
    "    serp_key = spec['serpapi']['key']\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = key    \n",
    "os.environ['SERPAPI_API_KEY'] = serp_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# !wget -q https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip ./docs/\n",
    "# !unzip -q new_articles.zip new_articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Attention Is All You Need\\nAshish Vaswani\\x03\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\x03\\nGoogle Brain\\nnoam@google.comNiki Parmar\\x03\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\x03\\nGoogle Research\\nusz@google.com\\nLlion Jones\\x03\\nGoogle Research\\nllion@google.comAidan N. Gomez\\x03y\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser\\x03\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\x03z\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\nyWork performed while at Google Brain.\\nzWork performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v5  [cs.CL]  6 Dec 2017', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 0}), Document(page_content='transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstatesht, as a function of the previous hidden state ht\\x001and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence\\nof continuous representations z= (z1;:::;z n). Given z, the decoder then generates an output\\nsequence (y1;:::;y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n2', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 1}), Document(page_content='Figure 1: The Transformer - model architecture.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 2}), Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention(Q;K;V ) = softmax(QKT\\npdk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1pdk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\x01k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 3}), Document(page_content='Multi-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO\\nwhere head i= Attention( QWQ\\ni;KWK\\ni;VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni2Rdmodel\\x02dk,WK\\ni2Rdmodel\\x02dk,WV\\ni2Rdmodel\\x02dv\\nandWO2Rhdv\\x02dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel=h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\x0fIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n\\x0fThe encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\x0fSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\x001) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0;xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\n5', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 4}), Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2\\x01d) O(1) O(1)\\nRecurrent O(n\\x01d2) O(n) O(n)\\nConvolutional O(k\\x01n\\x01d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r\\x01n\\x01d)O(1) O(n=r)\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos;2i)=sin(pos=100002i=d model)\\nPE(pos;2i+1)=cos(pos=100002i=d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\x19to10000\\x012\\x19. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 5}), Document(page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n=r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n=k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k\\x01n\\x01d+n\\x01d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with\\x0c1= 0:9,\\x0c2= 0:98and\\x0f= 10\\x009. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d\\x000:5\\nmodel\\x01min(step_num\\x000:5;step _num\\x01warmup _steps\\x001:5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0:1.\\n7', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 6}), Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1:0\\x011020\\nGNMT + RL [38] 24.6 39.92 2:3\\x0110191:4\\x011020\\nConvS2S [9] 25.16 40.46 9:6\\x0110181:5\\x011020\\nMoE [32] 26.03 40.56 2:0\\x0110191:2\\x011020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8:0\\x011020\\nGNMT + RL Ensemble [38] 26.30 41.16 1:8\\x0110201:1\\x011021\\nConvS2S Ensemble [9] 26.36 41.29 7:7\\x0110191:2\\x011021\\nTransformer (base model) 27.3 38.1 3:3\\x011018\\nTransformer (big) 28.4 41.8 2:3\\x011019\\nLabel Smoothing During training, we employed label smoothing of value \\x0fls= 0:1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0\\nBLEU, establishing a new state-of-the-art BLEU score of 28:4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3:5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0,\\noutperforming all of the previously published single models, at less than 1=4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0:1, instead of 0:3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty \\x0b= 0:6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 7}), Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdrop\\x0flstrain PPL BLEU params\\nsteps (dev) (dev)\\x02106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents speciﬁc challenges: the output is subject to strong structural\\n9', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 8}), Document(page_content='constraints and is signiﬁcantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-conﬁdence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\nincreased the maximum output length to input length + 300. We used a beam size of 21and\\x0b= 0:3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-speciﬁc tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n10', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 9}), Document(page_content='[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n11', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 10}), Document(page_content='[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 11}), Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difﬁcult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 12}), Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 13}), Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 14}), Document(page_content='Learning  \\n Spark\\nLightning-Fast Data Analytics\\nJules S. Damji,\\nBrooke Wenig,\\nTathagata Das\\n & Denny Lee  \\nForeword by Matei Zaharia\\n2nd Edition\\nCovers \\n Apache Spark 3.0\\nCompliments of\\n', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 0}), Document(page_content='', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 1}), Document(page_content='Praise for Learning Spark , Second Edition\\nThis book offers a structured approach to learning Apache Spark,\\ncovering new developments in the project. It is a great way for Spark developers\\nto get started with big data.\\n—Reynold Xin, Databricks Chief Architect and\\nCofounder and Apache Spark PMC Member\\nFor data scientists and data engineers looking to learn Apache Spark and how to build\\nscalable and reliable big data applications, this book is an essential guide!\\n—Ben Lorica, Databricks Chief Data Scientist,\\nPast Program Chair O’Reilly Strata Conferences,\\nProgram Chair for Spark + AI Summit', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 2}), Document(page_content='', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 3}), Document(page_content='Jules S. Damji, Brooke Wenig,\\nTathagata Das, and Denny LeeLearning Spark\\nLightning-Fast Data AnalyticsSECOND EDITION\\nBoston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 4}), Document(page_content='978-1-492-05004-9\\n[GP]Learning Spark\\nby Jules S. Damji, Brooke Wenig, Tathagata Das, and Denny Lee\\nCopyright © 2020 Databricks, Inc. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com .\\nAcquisitions Editor:  Jonathan Hassell\\nDevelopment Editor:  Michele Cronin\\nProduction Editor:  Deborah Baker\\nCopyeditor:  Rachel Head\\nProofreader:  Penelope PerkinsIndexer:  Potomac Indexing, LLC\\nInterior Designer:  David Futato\\nCover Designer:  Karen Montgomery\\nIllustrator:  Rebecca Demarest\\nJanuary 2015:  First Edition\\nJuly 2020:  Second Edition\\nRevision History for the Second Edition\\n2020-06-24: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492050049  for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Learning Spark , the cover image, and\\nrelated trade dress are trademarks of O’Reilly Media, Inc.\\nThe views expressed in this work are those of the authors, and do not represent the publisher’s views.\\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.\\nThis work is part of a collaboration between O’Reilly and Databricks. See our statement of editorial inde‐\\npendence .', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 5}), Document(page_content='Table of Contents\\nForeword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xiii\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xv\\n1.Introduction to Apache Spark: A Unified  Analytics Engine. . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nThe Genesis of Spark                                                                                                         1\\nBig Data and Distributed Computing at Google                                                       1\\nHadoop at Y ahoo!                                                                                                           2\\nSpark’s Early Y ears at AMPLab                                                                                     3\\nWhat Is Apache Spark?                                                                                                      4\\nSpeed                                                                                                                                4\\nEase of Use                                                                                                                       5\\nModularity                                                                                                                       5\\nExtensibility                                                                                                                     5\\nUnified Analytics                                                                                                                6\\nApache Spark Components as a Unified Stack                                                          6\\nApache Spark’s Distributed Execution                                                                      10\\nThe Developer’s Experience                                                                                           14\\nWho Uses Spark, and for What?                                                                                14\\nCommunity Adoption and Expansion                                                                      16\\n2.Downloading Apache Spark and Getting Started. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  19\\nStep 1: Downloading Apache Spark                                                                              19\\nSpark’s Directories and Files                                                                                       21\\nStep 2: Using the Scala or PySpark Shell                                                                       22\\nUsing the Local Machine                                                                                             23\\nStep 3: Understanding Spark Application Concepts                                                   25\\nSpark Application and SparkSession                                                                         26\\nv', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 6}), Document(page_content='Spark Jobs                                                                                                                      27\\nSpark Stages                                                                                                                   28\\nSpark Tasks                                                                                                                    28\\nTransformations, Actions, and Lazy Evaluation                                                          28\\nNarrow and Wide Transformations                                                                          30\\nThe Spark UI                                                                                                                    31\\nY our First Standalone Application                                                                                34\\nCounting M&Ms for the Cookie Monster                                                                35\\nBuilding Standalone Applications in Scala                                                               40\\nSummary                                                                                                                           42\\n3.Apache Spark’s Structured APIs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  43\\nSpark: What’s Underneath an RDD?                                                                             43\\nStructuring Spark                                                                                                             44\\nKey Merits and Benefits                                                                                              45\\nThe DataFrame API                                                                                                         47\\nSpark’s Basic Data Types                                                                                              48\\nSpark’s Structured and Complex Data Types                                                           49\\nSchemas and Creating DataFrames                                                                           50\\nColumns and Expressions                                                                                           54\\nRows                                                                                                                               57\\nCommon DataFrame Operations                                                                              58\\nEnd-to-End DataFrame Example                                                                              68\\nThe Dataset API                                                                                                               69\\nTyped Objects, Untyped Objects, and Generic Rows                                             69\\nCreating Datasets                                                                                                          71\\nDataset Operations                                                                                                       72\\nEnd-to-End Dataset Example                                                                                     74\\nDataFrames Versus Datasets                                                                                          74\\nWhen to Use RDDs                                                                                                      75\\nSpark SQL and the Underlying Engine                                                                         76\\nThe Catalyst Optimizer                                                                                               77\\nSummary                                                                                                                           82\\n4.Spark SQL and DataFrames: Introduction to Built-in Data Sources. . . . . . . . . . . . . . . . .  83\\nUsing Spark SQL in Spark Applications                                                                       84\\nBasic Query Examples                                                                                                 85\\nSQL Tables and Views                                                                                                     89\\nManaged Versus UnmanagedTables                                                                          89\\nCreating SQL Databases and Tables                                                                          90\\nCreating Views                                                                                                              91\\nViewing the Metadata                                                                                                  93\\nvi | Table of Contents', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 7}), Document(page_content='Caching SQL Tables                                                                                                     93\\nReading Tables into DataFrames                                                                                93\\nData Sources for DataFrames and SQL Tables                                                            94\\nDataFrameReader                                                                                                         94\\nDataFrameWriter                                                                                                         96\\nParquet                                                                                                                           97\\nJSON                                                                                                                            100\\nCSV                                                                                                                              102\\nAvro                                                                                                                              104\\nORC                                                                                                                              106\\nImages                                                                                                                          108\\nBinary Files                                                                                                                  110\\nSummary                                                                                                                         111\\n5.Spark SQL and DataFrames: Interacting with External Data Sources. . . . . . . . . . . . . . .  113\\nSpark SQL and Apache Hive                                                                                        113\\nUser-Defined Functions                                                                                            114\\nQuerying with the Spark SQL Shell, Beeline, and Tableau                                      119\\nUsing the Spark SQL Shell                                                                                        119\\nWorking with Beeline                                                                                                120\\nWorking with Tableau                                                                                               122\\nExternal Data Sources                                                                                                   129\\nJDBC and SQL Databases                                                                                         129\\nPostgreSQL                                                                                                                  132\\nMySQL                                                                                                                         133\\nAzure Cosmos DB                                                                                                      134\\nMS SQL Server                                                                                                            136\\nOther External Sources                                                                                             137\\nHigher-Order Functions in DataFrames and Spark SQL                                        138\\nOption 1: Explode and Collect                                                                                 138\\nOption 2: User-Defined Function                                                                            138\\nBuilt-in Functions for Complex Data Types                                                          139\\nHigher-Order Functions                                                                                           141\\nCommon DataFrames and Spark SQL Operations                                                  144\\nUnions                                                                                                                          147\\nJoins                                                                                                                              148\\nWindowing                                                                                                                  149\\nModifications                                                                                                              151\\nSummary                                                                                                                         155\\n6.Spark SQL and Datasets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  157\\nSingle API for Java and Scala                                                                                       157\\nTable of Contents | vii', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 8}), Document(page_content='Scala Case Classes and JavaBeans for Datasets                                                      158\\nWorking with Datasets                                                                                                  160\\nCreating Sample Data                                                                                                160\\nTransforming Sample Data                                                                                       162\\nMemory Management for Datasets and DataFrames                                               167\\nDataset Encoders                                                                                                           168\\nSpark’s Internal Format Versus Java Object Format                                              168\\nSerialization and Deserialization (SerDe)                                                               169\\nCosts of Using Datasets                                                                                                 170\\nStrategies to Mitigate Costs                                                                                       170\\nSummary                                                                                                                         172\\n7.Optimizing and Tuning Spark Applications. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  173\\nOptimizing and Tuning Spark for Efficiency                                                             173\\nViewing and Setting Apache Spark Configurations                                              173\\nScaling Spark for Large Workloads                                                                          177\\nCaching and Persistence of Data                                                                                 183\\nDataFrame.cache()                                                                                                     183\\nDataFrame.persist()                                                                                                   184\\nWhen to Cache and Persist                                                                                       187\\nWhen Not to Cache and Persist                                                                               187\\nA Family of Spark Joins                                                                                                187\\nBroadcast Hash Join                                                                                                   188\\nShuffle Sort Merge Join                                                                                             189\\nInspecting the Spark UI                                                                                                197\\nJourney Through the Spark UI Tabs                                                                       197\\nSummary                                                                                                                         205\\n8.Structured Streaming. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  207\\nEvolution of the Apache Spark Stream Processing Engine                                     207\\nThe Advent of Micro-Batch Stream Processing                                                    208\\nLessons Learned from Spark Streaming (DStreams)                                            209\\nThe Philosophy of Structured Streaming                                                               210\\nThe Programming Model of Structured Streaming                                                  211\\nThe Fundamentals of a Structured Streaming Query                                              213\\nFive Steps to Define a Streaming Query                                                                 213\\nUnder the Hood of an Active Streaming Query                                                    219\\nRecovering from Failures with Exactly-Once Guarantees                                   221\\nMonitoring an Active Query                                                                                    223\\nStreaming Data Sources and Sinks                                                                              226\\nFiles                                                                                                                              226\\nApache Kafka                                                                                                              228\\nviii | Table of Contents', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 9}), Document(page_content='Custom Streaming Sources and Sinks                                                                     230\\nData Transformations                                                                                                   234\\nIncremental Execution and Streaming State                                                          234\\nStateless Transformations                                                                                         235\\nStateful Transformations                                                                                           235\\nStateful Streaming Aggregations                                                                                 238\\nAggregations Not Based on Time                                                                            238\\nAggregations with Event-Time Windows                                                               239\\nStreaming Joins                                                                                                              246\\nStream–Static Joins                                                                                                    246\\nStream–Stream Joins                                                                                                 248\\nArbitrary Stateful Computations                                                                                 253\\nModeling Arbitrary Stateful Operations with mapGroupsWithState()             254\\nUsing Timeouts to Manage Inactive Groups                                                         257\\nGeneralization with flatMapGroupsWithState()                                                   261\\nPerformance Tuning                                                                                                      262\\nSummary                                                                                                                         264\\n9.Building Reliable Data Lakes with Apache Spark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  265\\nThe Importance of an Optimal Storage Solution                                                      265\\nDatabases                                                                                                                         266\\nA Brief Introduction to Databases                                                                           266\\nReading from and Writing to Databases Using Apache Spark                            267\\nLimitations of Databases                                                                                           267\\nData Lakes                                                                                                                       268\\nA Brief Introduction to Data Lakes                                                                         268\\nReading from and Writing to Data Lakes using Apache Spark                           269\\nLimitations of Data Lakes                                                                                         270\\nLakehouses: The Next Step in the Evolution of Storage Solutions                         271\\nApache Hudi                                                                                                               272\\nApache Iceberg                                                                                                           272\\nDelta Lake                                                                                                                    273\\nBuilding Lakehouses with Apache Spark and Delta Lake                                        274\\nConfiguring Apache Spark with Delta Lake                                                           274\\nLoading Data into a Delta Lake Table                                                                     275\\nLoading Data Streams into a Delta Lake Table                                                      277\\nEnforcing Schema on Write to Prevent Data Corruption                                    278\\nEvolving Schemas to Accommodate Changing Data                                            279\\nTransforming Existing Data                                                                                     279\\nAuditing Data Changes with Operation History                                                   282\\nQuerying Previous Snapshots of a Table with Time Travel                                 283\\nSummary                                                                                                                         284\\nTable of Contents | ix', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 10}), Document(page_content='10. Machine Learning with MLlib. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  285\\nWhat Is Machine Learning?                                                                                         286\\nSupervised Learning                                                                                                  286\\nUnsupervised Learning                                                                                             288\\nWhy Spark for Machine Learning?                                                                          289\\nDesigning Machine Learning Pipelines                                                                      289\\nData Ingestion and Exploration                                                                               290\\nCreating Training and Test Data Sets                                                                      291\\nPreparing Features with Transformers                                                                   293\\nUnderstanding Linear Regression                                                                           294\\nUsing Estimators to Build Models                                                                           295\\nCreating a Pipeline                                                                                                     296\\nEvaluating Models                                                                                                      302\\nSaving and Loading Models                                                                                      306\\nHyperparameter Tuning                                                                                               307\\nTree-Based Models                                                                                                     307\\nk-Fold Cross-Validation                                                                                            316\\nOptimizing Pipelines                                                                                                 320\\nSummary                                                                                                                         321\\n11. Managing, Deploying, and Scaling Machine Learning Pipelines with Apache Spark. .  323\\nModel Management                                                                                                       323\\nMLflow                                                                                                                         324\\nModel Deployment Options with MLlib                                                                    330\\nBatch                                                                                                                             332\\nStreaming                                                                                                                    333\\nModel Export Patterns for Real-Time Inference                                                   334\\nLeveraging Spark for Non-MLlib Models                                                                  336\\nPandas UDFs                                                                                                              336\\nSpark for Distributed Hyperparameter Tuning                                                     337\\nSummary                                                                                                                         341\\n12. Epilogue: Apache Spark 3.0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  343\\nSpark Core and Spark SQL                                                                                           343\\nDynamic Partition Pruning                                                                                      343\\nAdaptive Query Execution                                                                                        345\\nSQL Join Hints                                                                                                            348\\nCatalog Plugin API and DataSourceV2                                                                  349\\nAccelerator-Aware Scheduler                                                                                   351\\nStructured Streaming                                                                                                    352\\nPySpark, Pandas UDFs, and Pandas Function APIs                                                 354\\nRedesigned Pandas UDFs with Python Type Hints                                              354\\nx | Table of Contents', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 11}), Document(page_content='Iterator Support in Pandas UDFs                                                                            355\\nNew Pandas Function APIs                                                                                      356\\nChanged Functionality                                                                                                  357\\nLanguages Supported and Deprecated                                                                    357\\nChanges to the DataFrame and Dataset APIs                                                        357\\nDataFrame and SQL Explain Commands                                                              358\\nSummary                                                                                                                         360\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  361\\nTable of Contents | xi', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 12}), Document(page_content='', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 13}), Document(page_content='Foreword\\nApache Spark has evolved significantly since I first started the project at UC Berkeley\\nin 2009. After moving to the Apache Software Foundation, the open source project\\nhas had over 1,400 contributors from hundreds of companies, and the global Spark\\nmeetup group  has grown to over half a million members. Spark’s user base has also\\nbecome highly diverse, encompassing Python, R, SQL, and JVM developers, with use\\ncases ranging from data science to business intelligence to data engineering. I have\\nbeen working closely with the Apache Spark community to help continue its develop‐\\nment, and I am thrilled to see the progress thus far.\\nThe release of Spark 3.0 marks an important milestone for the project and has\\nsparked the need for updated learning material. The idea of a second edition of\\nLearning Spark  has come up many times—and it was overdue. Even though I coau‐\\nthored both Learning Spark  and Spark: The Definitive  Guide  (both O’Reilly), it was\\ntime for me to let the next generation of Spark contributors pick up the narrative. I’m\\ndelighted that four experienced practitioners and developers, who have been working\\nclosely with Apache Spark from its early days, have teamed up to write this second\\nedition of the book, incorporating the most recent APIs and best practices for Spark\\ndevelopers in a clear and informative guide.\\nThe authors’ approach to this edition is highly conducive to hands-on learning. The\\nkey concepts in Spark and distributed big data processing have been distilled into\\neasy-to-follow chapters. Through the book’s illustrative code examples, developers\\ncan build confidence using Spark and gain a greater understanding of its Structured\\nAPIs and how to leverage them. I hope that this second edition of Learning Spark  will\\nguide you on your large-scale data processing journey, whatever problems you wish\\nto tackle using Spark.\\n— Matei Zaharia, Chief Technologist,\\nCofounder of Databricks, Asst. Professor at Stanford,\\nand original creator of Apache Spark\\nxiii', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 14}), Document(page_content='', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 15}), Document(page_content='Preface\\nWe welcome you to the second edition of Learning Spark.  It’s been five years since the\\nfirst edition was published in 2015, originally authored by Holden Karau, Andy Kon‐\\nwinski, Patrick Wendell, and Matei Zaharia. This new edition has been updated to\\nreflect Apache Spark’s evolution through Spark 2.x and Spark 3.0, including its\\nexpanded ecosystem of built-in and external data sources, machine learning, and\\nstreaming technologies with which Spark is tightly integrated.\\nOver the years since its first 1.x release, Spark has become the de facto big data uni‐\\nfied processing engine. Along the way, it has extended its scope to include support for\\nvarious analytic workloads. Our intent is to capture and curate this evolution for\\nreaders, showing not only how you can use Spark but how it fits into the new era of\\nbig data and machine learning. Hence, we have designed each chapter to build pro‐\\ngressively on the foundations laid by the previous chapters, ensuring that the content\\nis suited for our intended audience.\\nWho This Book Is For\\nMost developers who grapple with big data are data engineers, data scientists, or\\nmachine learning engineers. This book is aimed at those professionals who are look‐\\ning to use Spark to scale their applications to handle massive amounts of data.\\nIn particular, data engineers will learn how to use Spark’s Structured APIs to perform\\ncomplex data exploration and analysis on both batch and streaming data; use Spark\\nSQL for interactive queries; use Spark’s built-in and external data sources to read,\\nrefine, and write data in different file formats as part of their extract, transform, and\\nload (ETL) tasks; and build reliable data lakes with Spark and the open source Delta\\nLake table format.\\nFor data scientists and machine learning engineers, Spark’s MLlib library offers many\\ncommon algorithms to build distributed machine learning models. We will cover\\nhow to build pipelines with MLlib, best practices for distributed machine learning,\\nxv', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 16}), Document(page_content='how to use Spark to scale single-node models, and how to manage and deploy these\\nmodels using the open source library MLflow.\\nWhile the book is focused on learning Spark as an analytical engine for diverse work‐\\nloads, we will not cover all of the languages that Spark supports. Most of the examples\\nin the chapters are written in Scala, Python, and SQL. Where necessary, we have\\ninfused a bit of Java. For those interested in learning Spark with R, we recommend\\nJavier Luraschi, Kevin Kuo, and Edgar Ruiz’s Mastering Spark with R  (O’Reilly).\\nFinally, because Spark is a distributed engine, building an understanding of Spark\\napplication concepts is critical. We will guide you through how your Spark applica‐\\ntion interacts with Spark’s distributed components and how execution is decomposed\\ninto parallel tasks on a cluster. We will also cover which deployment modes are sup‐\\nported and in what environments.\\nWhile there are many topics we have chosen to cover, there are a few that we have\\nopted to not focus on. These include the older low-level Resilient Distributed Dataset\\n(RDD) APIs and GraphX, Spark’s API for graphs and graph-parallel computation.\\nNor have we covered advanced topics such as how to extend Spark’s Catalyst opti‐\\nmizer to implement your own operations, how to implement your own catalog, or\\nhow to write your own DataSource V2 data sinks and sources. Though part of Spark,\\nthese are beyond the scope of your first book on learning Spark.\\nInstead, we have focused and organized the book around Spark’s Structured APIs,\\nacross all its components, and how you can use Spark to process structured data at\\nscale to perform your data engineering or data science tasks.\\nHow the Book Is Organized\\nWe organized the book in a way that leads you from chapter to chapter by introduc‐\\ning concepts, demonstrating these concepts via example code snippets, and providing\\nfull code examples or notebooks in the book’s GitHub repo .\\nChapter 1, Introduction to Apache Spark: A Unified  Analytics Engine\\nIntroduces you to the evolution of big data and provides a high-level overview of\\nApache Spark and its application to big data.\\nChapter 2, Downloading Apache Spark and Getting Started\\nWalks you through downloading and setting up Apache Spark on your local\\nmachine.\\nChapter 3, Apache Spark’s Structured APIs  through Chapter 6, Spark SQL and Datasets\\nThese chapters focus on using the DataFrame and Dataset Structured APIs to\\ningest data from built-in and external data sources, apply built-in and custom\\nfunctions, and utilize Spark SQL. These chapters comprise the foundation for\\nlater chapters, incorporating all the latest Spark 3.0 changes where appropriate.\\nxvi | Preface', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 17}), Document(page_content='Chapter 7, Optimizing and Tuning Spark Applications\\nProvides you with best practices for tuning, optimizing, debugging, and inspect‐\\ning your Spark applications through the Spark UI, as well as details on the con‐\\nfigurations you can tune to increase performance.\\nChapter 8, Structured Streaming\\nGuides you through the evolution of the Spark Streaming engine and the Struc‐\\ntured Streaming programming model. It examines the anatomy of a typical\\nstreaming query and discusses the different ways to transform streaming data—\\nstateful aggregations, stream joins, and arbitrary stateful aggregation—while pro‐\\nviding guidance on how to design performant streaming queries.\\nChapter 9, Building Reliable Data Lakes with Apache Spark\\nSurveys three open source table format storage solutions, as part of the Spark\\necosystem, that employ Apache Spark to build reliable data lakes with transac‐\\ntional guarantees. Due to Delta Lake’s tight integration with Spark for both batch\\nand streaming workloads, we focus on that solution and explore how it facilitates\\na new paradigm in data management, the lakehouse.\\nChapter 10, Machine Learning with MLlib\\nIntroduces MLlib, the distributed machine learning library for Spark, and walks\\nyou through an end-to-end example of how to build a machine learning pipeline,\\nincluding topics such as feature engineering, hyperparameter tuning, evaluation\\nmetrics, and saving and loading models.\\nChapter 11, Managing, Deploying, and Scaling Machine Learning Pipelines with Apache\\nSpark\\nCovers how to track and manage your MLlib models with MLflow, compares and\\ncontrasts different model deployment options, and explores how to leverage\\nSpark for non-MLlib models for distributed model inference, feature engineer‐\\ning, and/or hyperparameter tuning.\\nChapter 12, Epilogue: Apache Spark 3.0\\nThe epilogue highlights notable features and changes in Spark 3.0. While the full\\nrange of enhancements and features is too extensive to fit in a single chapter, we\\nhighlight the major changes you should be aware of and recommend you check\\nthe release notes when Spark 3.0 is officially released.\\nThroughout these chapters, we have incorporated or noted Spark 3.0 features where\\nneeded and tested all the code examples and notebooks against Spark 3.0.0-preview2.\\nPreface | xvii', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 18}), Document(page_content='How to Use the Code Examples\\nThe code examples in the book range from brief snippets to complete Spark applica‐\\ntions and end-to-end notebooks, in Scala, Python, SQL, and, where necessary, Java.\\nWhile some short code snippets in a chapter are self-contained and can be copied and\\npasted to run in a Spark shell ( pyspark  or spark-shell ), others are fragments from\\nstandalone Spark applications or end-to-end notebooks. To run standalone Spark\\napplications in Scala, Python, or Java, read the instructions in the respective chapter’s\\nREADME files in this book’s GitHub repo .\\nAs for the notebooks, to run these you will need to register for a free Databricks\\nCommunity Edition  account. We detail how to import the notebooks and create a\\ncluster using Spark 3.0 in the README .\\nSoftware and Configuration  Used\\nMost of the code in this book and the accompanying notebooks were written in and\\ntested against Apache Spark 3.0.0-preview2, which was available to us at the time we\\nwere writing the final chapters.\\nBy the time this book is published, Apache Spark 3.0 will have been released and be\\navailable to the community for general use. We recommend that you download  and\\nuse the official release with the following configurations for your operating system:\\n•Apache Spark 3.0 (prebuilt for Apache Hadoop 2.7)\\n•Java Development Kit (JDK) 1.8.0\\nIf you intend to use only Python, then you can simply run pip install pyspark .\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nxviii | Preface', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 19}), Document(page_content='Constant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nThis element signifies a general note.\\nUsing Code Examples\\nIf you have a technical question or a problem using the code examples, please send an\\nemail to bookquestions@oreilly.com .\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing examples from O’Reilly\\nbooks does require permission. Answering a question by citing this book and quoting\\nexample code does not require permission. Incorporating a significant amount of\\nexample code from this book into your product’s documentation does require\\npermission.\\nWe appreciate, but generally do not require, attribution. An attribution usually\\nincludes the title, author, publisher, and ISBN. For example: “ Learning Spark , 2nd\\nEdition, by Jules S. Damji, Brooke Wenig, Tathagata Das, and Denny Lee. Copyright\\n2020 Databricks, Inc., 978-1-492-05004-9. ”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com .\\nO’Reilly Online Learning\\nFor more than 40 years, O’Reilly Media  has provided technol‐\\nogy and business training, knowledge, and insight to help\\ncompanies succeed.\\nOur unique network of experts and innovators share their knowledge and expertise\\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\\nplatform gives you on-demand access to live training courses, in-depth learning\\npaths, interactive coding environments, and a vast collection of text and video from\\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com .\\nPreface | xix', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 20}), Document(page_content='How to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nVisit our web page for this book, where we list errata, examples, and any additional\\ninformation, at https://oreil.ly/LearningSpark2 .\\nEmail bookquestions@oreilly.com  to comment or ask technical questions about this\\nbook.\\nFor news and information about our books and courses, visit http://oreilly.com .\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\\nAcknowledgments\\nThis project was truly a team effort involving many people, and without their support\\nand feedback we would not have been able to finish this book, especially in today’s\\nunprecedented COVID-19 times.\\nFirst and foremost, we want to thank our employer, Databricks, for supporting us and\\nallocating us dedicated time as part of our jobs to finish this book. In particular, we\\nwant to thank Matei Zaharia, Reynold Xin, Ali Ghodsi, Ryan Boyd, and Rick Schultz\\nfor encouraging us to write the second edition.\\nSecond, we would like to thank our technical reviewers: Adam Breindel, Amir Issaei,\\nJacek Laskowski, Sean Owen, and Vishwanath Subramanian. Their diligent and con‐\\nstructive feedback, informed by their technical expertise in the community and\\nindustry point of view, made this book what it is: a valuable resource to learn Spark.\\nBesides the formal book reviewers, we received invaluable feedback from others\\nknowledgeable about specific topics and sections of the chapters, and we want to\\nacknowledge their contributions. Many thanks to: Conor Murphy, Hyukjin Kwon,\\nMaryann Xue, Niall Turbitt, Wenchen Fan, Xiao Li, and Yuanjian Li.\\nxx | Preface', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 21}), Document(page_content='Finally, we would like to thank our colleagues at Databricks (for their tolerance of us\\nmissing or neglecting project deadlines), our families and loved ones (for their\\npatience and empathy as we wrote in the early light of day or late into the night on\\nweekdays and weekends), and the entire open source Spark community. Without\\ntheir continued contributions, Spark would not be where it is today—and we authors\\nwould not have had much to write about.\\nThank you all!\\nPreface | xxi', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 22}), Document(page_content='', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 23}), Document(page_content='CHAPTER 1\\nIntroduction to Apache Spark:\\nA Unified  Analytics Engine\\nThis chapter lays out the origins of Apache Spark and its underlying philosophy. It\\nalso surveys the main components of the project and its distributed architecture. If\\nyou are familiar with Spark’s history and the high-level concepts, you can skip this\\nchapter.\\nThe Genesis of Spark\\nIn this section, we’ll chart the course of Apache Spark’s short evolution: its genesis,\\ninspiration, and adoption in the community as a de facto big data unified processing\\nengine.\\nBig Data and Distributed Computing at Google\\nWhen we think of scale, we can’t help but think of the ability of Google’s search\\nengine to index and search the world’s data on the internet at lightning speed. The\\nname Google is synonymous with scale. In fact, Google is a deliberate misspelling of\\nthe mathematical term googol : that’s 1 plus 100 zeros!\\nNeither traditional storage systems such as relational database management systems\\n(RDBMSs) nor imperative ways of programming were able to handle the scale at\\nwhich Google wanted to build and search the internet’s indexed documents. The\\nresulting need for new approaches led to the creation of the  Google File System  (GFS) ,\\nMapReduce  (MR) , and Bigtable .\\nWhile GFS provided a fault-tolerant and distributed filesystem across many com‐\\nmodity hardware servers in a cluster farm, Bigtable offered scalable storage of\\nstructured  data across GFS. MR introduced a new parallel programming paradigm,\\n1', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 24}), Document(page_content='based on functional programming, for large-scale processing of data distributed over\\nGFS and Bigtable.\\nIn essence, your MR applications interact with the MapReduce system  that sends\\ncomputation code (map and reduce functions) to where the data resides, favoring\\ndata locality and cluster rack affinity rather than bringing data to your application.\\nThe workers in the cluster aggregate and reduce the intermediate computations and\\nproduce a final appended output from the reduce function, which is then written to a\\ndistributed storage where it is accessible to your application. This approach signifi‐\\ncantly reduces network traffic and keeps most of the input/output (I/O) local to disk\\nrather than distributing it over the network.\\nMost of the work Google did was proprietary, but the ideas expressed in the afore‐\\nmentioned three papers  spurred innovative ideas elsewhere in the open source com‐\\nmunity—especially at Y ahoo!, which was dealing with similar big data challenges of\\nscale for its search engine.\\nHadoop at Yahoo!\\nThe computational challenges and solutions expressed in Google’s GFS paper pro‐\\nvided a blueprint for the Hadoop File System (HDFS) , including the MapReduce\\nimplementation as a framework for distributed computing. Donated to the Apache\\nSoftware Foundation (ASF) , a vendor-neutral non-profit organization, in April 2006,\\nit became part of the Apache Hadoop  framework of related modules: Hadoop Com‐\\nmon, MapReduce, HDFS, and Apache Hadoop YARN.\\nAlthough Apache Hadoop had garnered widespread adoption outside Y ahoo!, inspir‐\\ning a large open source community of contributors and two open source–based com‐\\nmercial companies (Cloudera and Hortonworks, now merged), the MapReduce\\nframework on HDFS had a few shortcomings.\\nFirst, it was hard to manage and administer, with cumbersome operational complex‐\\nity. Second, its general batch-processing MapReduce API was verbose and required a\\nlot of boilerplate setup code, with brittle fault tolerance. Third, with large batches of\\ndata jobs with many pairs of MR tasks, each pair’s intermediate computed result is\\nwritten to the local disk for the subsequent stage of its operation (see Figure 1-1 ).\\nThis repeated performance of disk I/O took its toll: large MR jobs could run for hours\\non end, or even days.\\n2 | Chapter 1: Introduction to Apache Spark: A Unified  Analytics Engine', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 25}), Document(page_content='Figure 1-1. Intermittent iteration of reads and writes between map and reduce\\ncomputations\\nAnd finally, even though Hadoop MR was conducive to large-scale jobs for general\\nbatch processing, it fell short for combining other workloads such as machine learn‐\\ning, streaming, or interactive SQL-like queries.\\nTo handle these new workloads, engineers developed bespoke systems (Apache Hive,\\nApache Storm, Apache Impala, Apache Giraph, Apache Drill, Apache Mahout, etc.),\\neach with their own APIs and cluster configurations, further adding to the opera‐\\ntional complexity of Hadoop and the steep learning curve for developers.\\nThe question then became (bearing in mind Alan Kay’s adage, “Simple things should\\nbe simple, complex things should be possible”), was there a way to make Hadoop and\\nMR simpler and faster?\\nSpark’s Early Years at AMPLab\\nResearchers at UC Berkeley who had previously worked on Hadoop MapReduce took\\non this challenge with a project they called Spark . They acknowledged that MR was\\ninefficient (or intractable) for interactive or iterative computing jobs and a complex\\nframework to learn, so from the onset they embraced the idea of making Spark sim‐\\npler, faster, and easier. This endeavor started in 2009 at the RAD Lab, which later\\nbecame the AMPLab (and now is known as the RISELab).\\nEarly papers  published on Spark demonstrated that it was 10 to 20 times faster than\\nHadoop MapReduce for certain jobs. Today, it’s many orders of magnitude faster . The\\ncentral thrust of the Spark project was to bring in ideas borrowed from Hadoop Map‐\\nReduce, but to enhance the system: make it highly fault tolerant and embarrassingly\\nparallel, support in-memory storage for intermediate results between iterative and\\ninteractive map and reduce computations, offer easy and composable APIs in multi‐\\nple languages as a programming model, and support other workloads in a unified\\nmanner. We’ll come back to this idea of unification shortly, as it’s an important theme\\nin Spark.\\nBy 2013 Spark had gained widespread use, and some of its original creators and\\nresearchers—Matei Zaharia, Ali Ghodsi, Reynold Xin, Patrick Wendell, Ion Stoica,\\nand Andy Konwinski—donated the Spark project to the ASF and formed a company\\ncalled Databricks.\\nThe Genesis of Spark | 3', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 26}), Document(page_content='Databricks and the community of open source developers worked to release Apache\\nSpark 1.0  in May 2014, under the governance of the ASF. This first major release\\nestablished the momentum for frequent future releases and contributions of notable\\nfeatures to Apache Spark from Databricks and over 100 commercial vendors.\\nWhat Is Apache Spark?\\nApache Spark  is a unified engine designed for large-scale distributed data processing,\\non premises in data centers or in the cloud.\\nSpark provides in-memory storage for intermediate computations, making it much\\nfaster than Hadoop MapReduce. It incorporates libraries with composable APIs for\\nmachine learning (MLlib), SQL for interactive queries (Spark SQL), stream process‐\\ning (Structured Streaming) for interacting with real-time data, and graph processing\\n(GraphX).\\nSpark’s design philosophy centers around four key characteristics:\\n•Speed\\n•Ease of use\\n•Modularity\\n•Extensibility\\nLet’s take a look at what this means for the framework.\\nSpeed\\nSpark has pursued the goal of speed in several ways. First, its internal implementation\\nbenefits immensely from the hardware industry’s recent huge strides in improving\\nthe price and performance of CPUs and memory. Today’s commodity servers come\\ncheap, with hundreds of gigabytes of memory, multiple cores, and the underlying\\nUnix-based operating system taking advantage of efficient multithreading and paral‐\\nlel processing. The framework is optimized to take advantage of all of these factors.\\nSecond, Spark builds its query computations as a directed acyclic graph (DAG); its\\nDAG scheduler and query optimizer construct an efficient computational graph that\\ncan usually be decomposed into tasks that are executed in parallel across workers on\\nthe cluster. And third, its physical execution engine, Tungsten, uses whole-stage code\\ngeneration to generate compact code for execution (we will cover SQL optimization\\nand whole-stage code generation in Chapter 3 ).\\nWith all the intermediate results retained in memory and its limited disk I/O, this\\ngives it a huge performance boost.\\n4 | Chapter 1: Introduction to Apache Spark: A Unified  Analytics Engine', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 27}), Document(page_content='Ease of Use\\nSpark achieves simplicity by providing a fundamental abstraction of a simple logical\\ndata structure called a Resilient Distributed Dataset (RDD) upon which all other\\nhigher-level structured data abstractions, such as DataFrames and Datasets, are con‐\\nstructed. By providing a set of transformations  and actions  as operations , Spark offers\\na simple programming model that you can use to build big data applications in famil‐\\niar languages.\\nModularity\\nSpark operations can be applied across many types of workloads and expressed in any\\nof the supported programming languages: Scala, Java, Python, SQL, and R. Spark\\noffers unified libraries with well-documented APIs that include the following mod‐\\nules as core components: Spark SQL, Spark Structured Streaming, Spark MLlib, and\\nGraphX, combining all the workloads running under one engine. We’ll take a closer\\nlook at all of these in the next section.\\nY ou can write a single Spark application that can do it all—no need for distinct\\nengines for disparate workloads, no need to learn separate APIs. With Spark, you get\\na unified processing engine for your workloads.\\nExtensibility\\nSpark focuses on its fast, parallel computation engine rather than on storage. Unlike\\nApache Hadoop, which included both storage and compute, Spark decouples the two.\\nThat means you can use Spark to read data stored in myriad sources—Apache\\nHadoop, Apache Cassandra, Apache HBase, MongoDB, Apache Hive, RDBMSs, and\\nmore—and process it all in memory. Spark’s DataFrameReader s and DataFrame\\nWriter s can also be extended to read data from other sources, such as Apache Kafka,\\nKinesis, Azure Storage, and Amazon S3, into its logical data abstraction, on which it\\ncan operate.\\nThe community of Spark developers maintains a list of third-party Spark packages  as\\npart of the growing ecosystem (see Figure 1-2 ). This rich ecosystem of packages\\nincludes Spark connectors for a variety of external data sources, performance moni‐\\ntors, and more.\\nWhat Is Apache Spark? | 5', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 28}), Document(page_content='Figure 1-2. Apache Spark’s ecosystem of connectors\\nUnified  Analytics\\nWhile the notion of unification is not unique to Spark, it is a core component of its\\ndesign philosophy and evolution. In November 2016, the Association for Computing\\nMachinery (ACM) recognized Apache Spark and conferred upon its original creators\\nthe prestigious ACM Award for their paper  describing Apache Spark as a “Unified\\nEngine for Big Data Processing. ” The award-winning paper notes that Spark replaces\\nall the separate batch processing, graph, stream, and query engines like Storm,\\nImpala, Dremel, Pregel, etc. with a unified stack of components that addresses diverse\\nworkloads under a single distributed fast engine.\\nApache Spark Components as a Unified  Stack\\nAs shown in Figure 1-3 , Spark offers four distinct components as libraries for diverse\\nworkloads : Spark SQL, Spark MLlib, Spark Structured Streaming, and GraphX. Each\\nof these components is separate from Spark’s core fault-tolerant engine, in that you\\nuse APIs to write your Spark application and Spark converts this into a DAG that is\\nexecuted by the core engine. So whether you write your Spark code using the pro‐\\nvided Structured APIs (which we will cover in Chapter 3 ) in Java, R, Scala, SQL, or\\n6 | Chapter 1: Introduction to Apache Spark: A Unified  Analytics Engine', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 29}), Document(page_content='Python, the underlying code is decomposed into highly compact bytecode that is exe‐\\ncuted in the workers’ JVMs across the cluster.\\nFigure 1-3. Apache Spark components and API stack\\nLet’s look at each of these components in more detail.\\nSpark SQL\\nThis module works well with structured data. Y ou can read data stored in an RDBMS\\ntable or from file formats with structured data (CSV , text, JSON, Avro, ORC, Parquet,\\netc.) and then construct permanent or temporary tables in Spark. Also, when using\\nSpark’s Structured APIs in Java, Python, Scala, or R, you can combine SQL-like quer‐\\nies to query the data just read into a Spark DataFrame. To date, Spark SQL is ANSI\\nSQL:2003-compliant  and it also functions as a pure SQL engine.\\nFor example, in this Scala code snippet, you can read from a JSON file stored on\\nAmazon S3, create a temporary table, and issue a SQL-like query on the results read\\ninto memory as a Spark DataFrame:\\n// In Scala\\n// Read data off Amazon S3 bucket into a Spark DataFrame\\nspark.read.json(\"s3://apache_spark/data/committers.json\" )\\n  .createOrReplaceTempView (\"committers\" )\\n// Issue a SQL query and return the result as a Spark DataFrame\\nval results = spark.sql(\"\"\"SELECT name, org, module, release, num_commits\\n    FROM committers WHERE module = \\'mllib\\' AND num_commits > 10\\n    ORDER BY num_commits DESC\"\"\" )\\nY ou can write similar code snippets in Python, R, or Java, and the generated bytecode\\nwill be identical, resulting in the same performance.\\nSpark MLlib\\nSpark comes with a library containing common machine learning (ML) algorithms\\ncalled MLlib. Since Spark’s first release, the performance of this library component\\nhas improved significantly because of Spark 2.x’s underlying engine enhancements.\\nUnified  Analytics | 7', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 30}), Document(page_content='MLlib provides many popular machine learning algorithms built atop high-level\\nDataFrame-based APIs to build models.\\nStarting with Apache Spark 1.6, the MLlib project  is split between\\ntwo packages: spark.mllib  and spark.ml . The DataFrame-based\\nAPI is the latter while the former contains the RDD-based APIs,\\nwhich are now in maintenance mode. All new features go into\\nspark.ml . This book refers to “MLlib” as the umbrella library for\\nmachine learning in Apache Spark.\\nThese APIs allow you to extract or transform features, build pipelines (for training\\nand evaluating), and persist models (for saving and reloading them) during deploy‐\\nment. Additional utilities include the use of common linear algebra operations and\\nstatistics. MLlib includes other low-level ML primitives, including a generic gradient\\ndescent optimization. The following Python code snippet encapsulates the basic oper‐\\nations a data scientist may do when building a model (more extensive examples will\\nbe discussed in Chapters 10 and 11):\\n# In Python\\nfrom pyspark.ml.classification  import LogisticRegression\\n...\\ntraining  = spark.read.csv(\"s3://...\" )\\ntest = spark.read.csv(\"s3://...\" )\\n# Load training data\\nlr = LogisticRegression (maxIter=10, regParam =0.3, elasticNetParam =0.8)\\n# Fit the model\\nlrModel = lr.fit(training )\\n# Predict\\nlrModel.transform (test)\\n...\\nSpark Structured Streaming\\nApache Spark 2.0 introduced an experimental Continuous Streaming model  and\\nStructured Streaming APIs , built atop the Spark SQL engine and DataFrame-based\\nAPIs. By Spark 2.2, Structured Streaming was generally available, meaning that devel‐\\nopers could use it in their production environments.\\nNecessary for big data developers to combine and react in real time to both static data\\nand streaming data from engines like Apache Kafka and other streaming sources, the\\nnew model views a stream as a continually growing table, with new rows of data\\nappended at the end. Developers can merely treat this as a structured table and issue\\nqueries against it as they would a static table.\\n8 | Chapter 1: Introduction to Apache Spark: A Unified  Analytics Engine', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 31}), Document(page_content='1Contributed to the community by Databricks as an open source project, GraphFrames  is a general graph pro‐\\ncessing library that is similar to Apache Spark’s GraphX but uses DataFrame-based APIs.Underneath the Structured Streaming model, the Spark SQL core engine handles all\\naspects of fault tolerance and late-data semantics, allowing developers to focus on\\nwriting streaming applications with relative ease. This new model obviated the old\\nDStreams model in Spark’s 1.x series, which we will discuss in more detail in Chap‐\\nter 8 . Furthermore, Spark 2.x and Spark 3.0 extended the range of streaming data\\nsources to include Apache Kafka, Kinesis, and HDFS-based or cloud storage.\\nThe following code snippet shows the typical anatomy of a Structured Streaming\\napplication. It reads from a localhost socket and writes the word count results to an\\nApache Kafka topic:\\n# In Python\\n# Read a stream from a local host\\nfrom pyspark.sql.functions  import explode, split\\nlines = (spark \\n  .readStream\\n  .format(\"socket\" )\\n  .option(\"host\", \"localhost\" )\\n  .option(\"port\", 9999)\\n  .load())\\n# Perform transformation\\n# Split the lines into words\\nwords = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\\n# Generate running word count\\nword_counts  = words.groupBy(\"word\").count()\\n# Write out to the stream to Kafka\\nquery = (word_counts\\n  .writeStream  \\n  .format(\"kafka\") \\n  .option(\"topic\", \"output\" ))\\nGraphX\\nAs the name suggests, GraphX is a library for manipulating graphs (e.g., social net‐\\nwork graphs, routes and connection points, or network topology graphs) and per‐\\nforming graph-parallel computations. It offers the standard graph algorithms for\\nanalysis, connections, and traversals, contributed by users in the community: the\\navailable algorithms include PageRank, Connected Components, and Triangle\\nCounting.1\\nThis code snippet shows a simple example of how to join two graphs using the\\nGraphX APIs:\\nUnified  Analytics | 9', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 32}), Document(page_content='// In Scala\\nval graph = Graph(vertices , edges)\\nmessages  = spark.textFile (\"hdfs://...\" )\\nval graph2 = graph.joinVertices (messages ) {\\n  (id, vertex, msg) => ...\\n}\\nApache Spark’s Distributed Execution\\nIf you have read this far, you already know that Spark is a distributed data processing\\nengine with its components working collaboratively on a cluster of machines. Before\\nwe explore programming with Spark in the following chapters of this book, you need\\nto understand how all the components of Spark’s distributed architecture work\\ntogether and communicate, and what deployment modes are available.\\nLet’s start by looking at each of the individual components shown in Figure 1-4  and\\nhow they fit into the architecture. At a high level in the Spark architecture, a Spark\\napplication consists of a driver program that is responsible for orchestrating parallel\\noperations on the Spark cluster. The driver accesses the distributed components in\\nthe cluster—the Spark executors and cluster manager—through a SparkSession .\\nFigure 1-4. Apache Spark components and architecture\\nSpark driver\\nAs the part of the Spark application responsible for instantiating a SparkSession , the\\nSpark driver has multiple roles: it communicates with the cluster manager; it requests\\nresources (CPU, memory, etc.) from the cluster manager for Spark’s executors\\n(JVMs); and it transforms all the Spark operations into DAG computations, schedules\\n10 | Chapter 1: Introduction to Apache Spark: A Unified  Analytics Engine', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 33}), Document(page_content='them, and distributes their execution as tasks across the Spark executors. Once the\\nresources are allocated, it communicates directly with the executors.\\nSparkSession\\nIn Spark 2.0, the SparkSession  became a unified conduit to all Spark operations and\\ndata. Not only did it subsume previous entry points to Spark  like the SparkContext ,\\nSQLContext , HiveContext , SparkConf , and StreamingContext , but it also made\\nworking with Spark simpler and easier.\\nAlthough in Spark 2.x the SparkSession  subsumes all other con‐\\ntexts, you can still access the individual contexts and their respec‐\\ntive methods. In this way, the community maintained backward\\ncompatibility. That is, your old 1.x code with SparkContext  or\\nSQLContext  will still work.\\nThrough this one conduit, you can create JVM runtime parameters, define Data‐\\nFrames and Datasets, read from data sources, access catalog metadata, and issue\\nSpark SQL queries. SparkSession  provides a single unified entry point to all of\\nSpark’s functionality.\\nIn a standalone Spark application, you can create a SparkSession  using one of the\\nhigh-level APIs in the programming language of your choice. In the Spark shell\\n(more on this in the next chapter) the SparkSession  is created for you, and you can\\naccess it via a global variable called spark  or sc.\\nWhereas in Spark 1.x you would have had to create individual contexts (for stream‐\\ning, SQL, etc.), introducing extra boilerplate code, in a Spark 2.x application you can\\ncreate a SparkSession  per JVM and use it to perform a number of Spark operations.\\nLet’s take a look at an example:\\n// In Scala\\nimport org.apache.spark.sql.SparkSession\\n// Build SparkSession\\nval spark = SparkSession\\n  .builder\\n  .appName(\"LearnSpark\" )\\n  .config(\"spark.sql.shuffle.partitions\" , 6)\\n  .getOrCreate ()\\n...\\n// Use the session to read JSON \\nval people = spark.read.json(\"...\")\\n...\\n// Use the session to issue a SQL query\\nval resultsDF  = spark.sql(\"SELECT city, pop, state, zip FROM table_name\" )\\nUnified  Analytics | 11', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 34}), Document(page_content='Cluster manager\\nThe cluster manager is responsible for managing and allocating resources for the\\ncluster of nodes on which your Spark application runs. Currently, Spark supports\\nfour cluster managers: the built-in standalone cluster manager, Apache Hadoop\\nYARN, Apache Mesos, and Kubernetes.\\nSpark executor\\nA Spark executor runs on each worker node in the cluster. The executors communi‐\\ncate with the driver program and are responsible for executing tasks on the workers.\\nIn most deployments modes, only a single executor runs per node.\\nDeployment modes\\nAn attractive feature of Spark is its support for myriad deployment modes, enabling\\nSpark to run in different configurations and environments. Because the cluster man‐\\nager is agnostic to where it runs (as long as it can manage Spark’s executors and\\nfulfill resource requests), Spark can be deployed in some of the most popular envi‐\\nronments—such as Apache Hadoop YARN and Kubernetes—and can operate in dif‐\\nferent modes. Table 1-1  summarizes the available deployment modes.\\nTable 1-1. Cheat sheet for Spark deployment modes\\nMode Spark driver Spark executor Cluster manager\\nLocal Runs on a single JVM, like a\\nlaptop or single nodeRuns on the same JVM as the\\ndriverRuns on the same host\\nStandalone Can run on any node in the\\nclusterEach node in the cluster will\\nlaunch its own executor JVMCan be allocated arbitrarily to any\\nhost in the cluster\\nYARN (client) Runs on a client, not part of the\\nclusterYARN’s NodeManager’s container YARN’s Resource Manager works\\nwith YARN’s Application Master to\\nallocate the containers on\\nNodeManagers for executors\\nYARN\\n(cluster)Runs with the YARN Application\\nMasterSame as YARN client mode Same as YARN client mode\\nKubernetes Runs in a Kubernetes pod Each worker runs within its own\\npodKubernetes Master\\nDistributed data and partitions\\nActual physical data is distributed across storage as partitions residing in either HDFS\\nor cloud storage (see Figure 1-5 ). While the data is distributed as partitions across the\\nphysical cluster, Spark treats each partition as a high-level logical data abstraction—as\\na DataFrame in memory. Though this is not always possible, each Spark executor is\\npreferably allocated a task that requires it to read the partition closest to it in the net‐\\nwork, observing data locality.\\n12 | Chapter 1: Introduction to Apache Spark: A Unified  Analytics Engine', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 35}), Document(page_content='Figure 1-5. Data is distributed across physical machines\\nPartitioning allows for efficient parallelism. A distributed scheme of breaking up data\\ninto chunks or partitions allows Spark executors to process only data that is close to\\nthem, minimizing network bandwidth. That is, each executor’s core is assigned its\\nown data partition to work on (see Figure 1-6 ).\\nFigure 1-6. Each executor’s core gets a partition of data to work on\\nFor example, this code snippet will break up the physical data stored across clusters\\ninto eight partitions, and each executor will get one or more partitions to read into its\\nmemory:\\n# In Python\\nlog_df = spark.read.text(\"path_to_large_text_file\" ).repartition (8)\\nprint(log_df.rdd.getNumPartitions ())\\nAnd this code will create a DataFrame of 10,000 integers distributed over eight parti‐\\ntions in memory:\\nUnified  Analytics | 13', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 36}), Document(page_content='# In Python\\ndf = spark.range(0, 10000, 1, 8)\\nprint(df.rdd.getNumPartitions ())\\nBoth code snippets will print out 8.\\nIn Chapters 3 and 7, we will discuss how to tune and change partitioning configura‐\\ntion for maximum parallelism based on how many cores you have on your executors.\\nThe Developer’s Experience\\nOf all the developers’ delights, none is more attractive than a set of composable APIs\\nthat increase productivity and are easy to use, intuitive, and expressive. One of\\nApache Spark’s principal appeals to developers has been its easy-to-use APIs  for oper‐\\nating on small to large data sets, across languages: Scala, Java, Python, SQL, and R.\\nOne primary motivation behind Spark 2.x was to unify and simplify the framework\\nby limiting the number of concepts that developers have to grapple with. Spark 2.x\\nintroduced higher-level abstraction APIs as domain-specific language constructs,\\nwhich made programming Spark highly expressive and a pleasant developer experi‐\\nence. Y ou express what you want the task or operation to compute, not how to com‐\\npute it, and let Spark ascertain how best to do it for you. We will cover these\\nStructured APIs in Chapter 3 , but first let’s take a look at who the Spark developers\\nare.\\nWho Uses Spark, and for What?\\nNot surprisingly, most developers who grapple with big data are data engineers, data\\nscientists, or machine learning engineers. They are drawn to Spark because it allows\\nthem to build a range of applications using a single engine, with familiar program‐\\nming languages.\\nOf course, developers may wear many hats and sometimes do both data science and\\ndata engineering tasks, especially in startup companies or smaller engineering groups.\\nAmong all these tasks, however, data—massive amounts of data—is the foundation.\\nData science tasks\\nAs a discipline that has come to prominence in the era of big data, data science is\\nabout using data to tell stories. But before they can narrate the stories, data scientists\\nhave to cleanse the data, explore it to discover patterns, and build models to predict\\nor suggest outcomes. Some of these tasks require knowledge of statistics, mathemat‐\\nics, computer science, and programming.\\nMost data scientists are proficient in using analytical tools like SQL, comfortable with\\nlibraries like NumPy and pandas, and conversant in programming languages like R\\n14 | Chapter 1: Introduction to Apache Spark: A Unified  Analytics Engine', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 37}), Document(page_content='and Python. But they must also know how to wrangle  or transform  data, and how to\\nuse established classification, regression, or clustering algorithms for building mod‐\\nels. Often their tasks are iterative, interactive or ad hoc, or experimental to assert their\\nhypotheses.\\nFortunately, Spark supports these different tools. Spark’s MLlib offers a common set\\nof machine learning algorithms to build model pipelines, using high-level estimators,\\ntransformers, and data featurizers. Spark SQL and the Spark shell facilitate interactive\\nand ad hoc exploration of data.\\nAdditionally, Spark enables data scientists to tackle large data sets and scale their\\nmodel training and evaluation. Apache Spark 2.4 introduced a new gang scheduler, as\\npart of Project Hydrogen , to accommodate the fault-tolerant needs of training and\\nscheduling deep learning models in a distributed manner, and Spark 3.0 has intro‐\\nduced the ability to support GPU resource collection in the standalone, YARN, and\\nKubernetes deployment modes. This means developers whose tasks demand deep\\nlearning techniques can use Spark.\\nData engineering tasks\\nAfter building their models, data scientists often need to work with other team mem‐\\nbers, who may be responsible for deploying the models. Or they may need to work\\nclosely with others to build and transform raw, dirty data into clean data that is easily\\nconsumable or usable by other data scientists. For example, a classification or cluster‐\\ning model does not exist in isolation; it works in conjunction with other components\\nlike a web application or a streaming engine such as Apache Kafka, or as part of a\\nlarger data pipeline. This pipeline is often built by data engineers.\\nData engineers have a strong understanding of software engineering principles and\\nmethodologies, and possess skills for building scalable data pipelines for a stated busi‐\\nness use case. Data pipelines enable end-to-end transformations of raw data coming\\nfrom myriad sources—data is cleansed so that it can be consumed downstream by\\ndevelopers, stored in the cloud or in NoSQL or RDBMSs for report generation, or\\nmade accessible to data analysts via business intelligence tools.\\nSpark 2.x introduced an evolutionary streaming model called continuous applications\\nwith Structured Streaming (discussed in detail in Chapter 8 ). With Structured\\nStreaming APIs, data engineers can build complex data pipelines that enable them to\\nETL data from both real-time and static data sources.\\nData engineers use Spark because it provides a simple way to parallelize computations\\nand hides all the complexity of distribution and fault tolerance. This leaves them free\\nto focus on using high-level DataFrame-based APIs and domain-specific language\\n(DSL) queries to do ETL, reading and combining data from multiple sources.\\nThe Developer’s Experience | 15', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 38}), Document(page_content='The performance improvements in Spark 2.x and Spark 3.0, due to the Catalyst opti‐\\nmizer  for SQL and Tungsten  for compact code generation, have made life for data\\nengineers much easier. They can choose to use any of the three Spark APIs —RDDs,\\nDataFrames, or Datasets—that suit the task at hand, and reap the benefits of Spark.\\nPopular Spark use cases\\nWhether you are a data engineer, data scientist, or machine learning engineer, you’ll\\nfind Spark useful for the following use cases:\\n•Processing in parallel large data sets distributed across a cluster\\n•Performing ad hoc or interactive queries to explore and visualize data sets\\n•Building, training, and evaluating machine learning models using MLlib\\n•Implementing end-to-end data pipelines from myriad streams of data\\n•Analyzing graph data sets and social networks\\nCommunity Adoption and Expansion\\nNot surprisingly, Apache Spark struck a chord in the open source community, espe‐\\ncially among data engineers and data scientists. Its design philosophy and its inclu‐\\nsion as an Apache Software Foundation project have fostered immense interest\\namong the developer community.\\nToday, there are over 600 Apache Spark Meetup groups  globally with close to half a\\nmillion members. Every week, someone in the world is giving a talk at a meetup or\\nconference or sharing a blog post on how to use Spark to build data pipelines. The\\nSpark + AI Summit  is the largest conference dedicated to the use of Spark for\\nmachine learning, data engineering, and data science across many verticals.\\nSince Spark’s first 1.0 release in 2014 there have been many minor and major releases,\\nwith the most recent major release of Spark 3.0 coming in 2020. This book will cover\\naspects of Spark 2.x and Spark 3.0. By the time of its publication the community will\\nhave released Spark 3.0, and most of the code in this book has been tested with Spark\\n3.0-preview2.\\nOver the course of its releases, Spark has continued to attract contributors from\\nacross the globe and from numerous organizations. Today, Spark has close to 1,500\\ncontributors, well over 100 releases, 21,000 forks, and some 27,000 commits on Git‐\\nHub, as Figure 1-7  shows. And we hope that when you finish this book, you will feel\\ncompelled to contribute too.\\n16 | Chapter 1: Introduction to Apache Spark: A Unified  Analytics Engine', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 39}), Document(page_content='Figure 1-7. The state of Apache Spark on GitHub (source: https://github.com/apache/\\nspark )\\nNow we can turn our attention to the fun of learning—where and how to start using\\nSpark. In the next chapter, we’ll show you how to get up and running with Spark in\\nthree simple steps.\\nThe Developer’s Experience | 17', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 40}), Document(page_content='', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 41}), Document(page_content='CHAPTER 2\\nDownloading Apache Spark\\nand Getting Started\\nIn this chapter, we will get you set up with Spark and walk through three simple steps\\nyou can take to get started writing your first standalone application.\\nWe will use local mode, where all the processing is done on a single machine in a\\nSpark shell—this is an easy way to learn the framework, providing a quick feedback\\nloop for iteratively performing Spark operations. Using a Spark shell, you can proto‐\\ntype Spark operations with small data sets before writing a complex Spark applica‐\\ntion, but for large data sets or real work where you want to reap the benefits of\\ndistributed execution, local mode is not suitable—you’ll want to use the YARN or\\nKubernetes deployment modes instead.\\nWhile the Spark shell only supports Scala, Python, and R, you can write a Spark\\napplication in any of the supported languages (including Java) and issue queries in\\nSpark SQL. We do expect you to have some familiarity with the language of your\\nchoice.\\nStep 1: Downloading Apache Spark\\nTo get started, go to the Spark download page , select “Pre-built for Apache Hadoop\\n2.7” from the drop-down menu in step 2, and click the “Download Spark” link in step\\n3 (Figure 2-1 ).\\n19', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 42}), Document(page_content='Figure 2-1. The Apache Spark download page\\nThis will download the tarball spark-3.0.0-preview2-bin-hadoop2.7.tgz , which con‐\\ntains all the Hadoop-related binaries you will need to run Spark in local mode on\\nyour laptop. Alternatively, if you’re going to install it on an existing HDFS or Hadoop\\ninstallation, you can select the matching Hadoop version from the drop-down menu.\\nHow to build from source is beyond the scope of this book, but you can read more\\nabout it in the documentation .\\nAt the time this book went to press Apache Spark 3.0 was still in\\npreview mode, but you can download the latest Spark 3.0 using the\\nsame download method and instructions.\\nSince the release of Apache Spark 2.2, developers who only care about learning Spark\\nin Python have the option of installing PySpark from the PyPI repository . If you only\\nprogram in Python, you don’t have to install all the other libraries necessary to run\\nScala, Java, or R; this makes the binary smaller. To install PySpark from PyPI, just run\\npip install pyspark .\\nThere are some extra dependencies that can be installed for SQL, ML, and MLlib, via\\npip install pyspark[sql,ml,mllib]  (or pip install pyspark[sql]  if you only\\nwant the SQL dependencies).\\nY ou will need to install Java 8 or above on your machine and set the\\nJAVA_HOME  environment variable. See the documentation  for\\ninstructions on how to download and install Java.\\n20 | Chapter 2: Downloading Apache Spark and Getting Started', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 43}), Document(page_content='If you want to run R in an interpretive shell mode, you must install R  and then run\\nsparkR . To do distributed computing with R, you can also use the open source project\\nsparklyr , created by the R community.\\nSpark’s Directories and Files\\nWe assume that you are running a version of the Linux or macOS operating system\\non your laptop or cluster, and all the commands and instructions in this book will be\\nin that flavor. Once you have finished downloading the tarball, cd to the downloaded\\ndirectory, extract the tarball contents with tar -xf spark-3.0.0-preview2-bin-\\nhadoop2.7.tgz , and cd into that directory and take a look at the contents:\\n$ cd spark-3.0.0-preview2-bin-hadoop2.7\\n$ ls\\nLICENSE   R          RELEASE   conf    examples   kubernetes  python   yarn\\nNOTICE    README.md  bin       data    jars       licenses    sbin\\nLet’s briefly summarize the intent and purpose of some of these files and directories.\\nNew items were added in Spark 2.x and 3.0, and the contents of some of the existing\\nfiles and directories were changed too:\\nREADME.md\\nThis file contains new detailed instructions on how to use Spark shells, build\\nSpark from source, run standalone Spark examples, peruse links to Spark docu‐\\nmentation and configuration guides, and contribute to Spark.\\nbin\\nThis directory, as the name suggests, contains most of the scripts you’ll employ to\\ninteract with Spark, including  the Spark shells  (spark-sql , pyspark , spark-\\nshell , and sparkR ). We will use these shells and executables in this directory\\nlater in this chapter to submit a standalone Spark application using spark-\\nsubmit , and write a script that builds and pushes Docker images when running\\nSpark with Kubernetes support.\\nsbin\\nMost of the scripts in this directory are administrative in purpose, for starting\\nand stopping Spark components in the cluster in its various deployment\\nmodes. For details on the deployment modes, see the cheat sheet in Table 1-1  in\\nChapter 1 .\\nkubernetes\\nSince the release of Spark 2.4, this directory contains Dockerfiles for creating\\nDocker images for your Spark distribution on a Kubernetes cluster. It also con‐\\ntains a file providing instructions on how to build the Spark distribution before\\nbuilding your Docker images.\\nStep 1: Downloading Apache Spark | 21', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 44}), Document(page_content='data\\nThis directory is populated with *.txt  files that serve as input for Spark’s compo‐\\nnents: MLlib, Structured Streaming, and GraphX.\\nexamples\\nFor any developer, two imperatives that ease the journey to learning any new\\nplatform are loads of “how-to” code examples and comprehensive documenta‐\\ntion. Spark provides examples for Java, Python, R, and Scala, and you’ll want to\\nemploy them when learning the framework. We will allude to some of these\\nexamples in this and subsequent chapters.\\nStep 2: Using the Scala or PySpark Shell\\nAs mentioned earlier, Spark comes with four widely used interpreters that act like\\ninteractive “shells” and enable ad hoc data analysis: pyspark , spark-shell , spark-\\nsql, and sparkR . In many ways, their interactivity imitates shells you’ll already be\\nfamiliar with if you have experience with Python, Scala, R, SQL, or Unix operating\\nsystem shells such as bash or the Bourne shell.\\nThese shells have been augmented to support connecting to the cluster and to allow\\nyou to load distributed data into Spark workers’ memory. Whether you are dealing\\nwith gigabytes of data or small data sets, Spark shells are conducive to learning Spark\\nquickly.\\nTo start PySpark, cd to the bin directory and launch a shell by typing pyspark . If you\\nhave installed PySpark from PyPI, then just typing pyspark  will suffice:\\n$ pyspark\\nPython 3.7.3 (default, Mar 27 2019, 09:23:15)\\n[Clang 10.0.1 (clang-1001.0.46.3)] on darwin\\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n20/02/16 19:28:48 WARN NativeCodeLoader: Unable to load native-hadoop library \\nfor your platform... using builtin-java classes where applicable\\nWelcome to\\n      ____              __\\n     / __/__  ___ _____/ /__\\n    _\\\\ \\\\/ _ \\\\/ _ `/ __/  \\'_/\\n   /__ / .__/\\\\_,_/_/ /_/\\\\_\\\\   version 3.0.0-preview2\\n      /_/\\nUsing Python version 3.7.3 (default, Mar 27 2019 09:23:15)\\nSparkSession available as \\'spark\\'.\\n>>> spark.version\\n\\'3.0.0-preview2\\'\\n>>>\\n22 | Chapter 2: Downloading Apache Spark and Getting Started', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 45}), Document(page_content=\"To start a similar Spark shell with Scala, cd to the bin directory and type\\nspark-shell :\\n$ spark-shell\\n20/05/07 19:30:26 WARN NativeCodeLoader: Unable to load native-hadoop library \\nfor your platform... using builtin-java classes where applicable\\nSpark context Web UI available at http://10.0.1.7:4040\\nSpark context available as 'sc' (master = local[*], app id = local-1581910231902)\\nSpark session available as 'spark'.\\nWelcome to\\n      ____              __\\n     / __/__  ___ _____/ /__\\n    _\\\\ \\\\/ _ \\\\/ _ `/ __/  '_/\\n   /___/ .__/\\\\_,_/_/ /_/\\\\_\\\\   version 3.0.0-preview2\\n      /_/\\nUsing Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_241)\\nType in expressions to have them evaluated.\\nType :help for more information.\\nscala> spark.version\\nres0: String = 3.0.0-preview2\\nscala>\\nUsing the Local Machine\\nNow that you’ve downloaded and installed Spark on your local machine, for the\\nremainder of this chapter you’ll be using Spark interpretive shells locally. That is,\\nSpark will be running in local mode.\\nRefer to Table 1-1  in Chapter 1  for a reminder of which compo‐\\nnents run where in local mode.\\nAs noted in the previous chapter, Spark computations are expressed as operations.\\nThese operations are then converted into low-level RDD-based bytecode as tasks,\\nwhich are distributed to Spark’s executors for execution.\\nLet’s look at a short example where we read in a text file as a DataFrame, show a sam‐\\nple of the strings read, and count the total number of lines in the file. This simple\\nexample illustrates the use of the high-level Structured APIs, which we will cover in\\nthe next chapter. The show(10, false)  operation on the DataFrame only displays the\\nfirst 10 lines without truncating; by default the truncate  Boolean flag is true . Here’s\\nwhat this looks like in the Scala shell:\\nStep 2: Using the Scala or PySpark Shell | 23\", metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 46}), Document(page_content='scala> val strings = spark.read.text(\"../README.md\")\\nstrings: org.apache.spark.sql.DataFrame = [value: string]\\nscala> strings.show(10, false)\\n+------------------------------------------------------------------------------+\\n|value                                                                         |\\n+------------------------------------------------------------------------------+\\n|# Apache Spark                                                                |\\n|                                                                              |\\n|Spark is a unified analytics engine for large-scale data processing. It       |\\n|provides high-level APIs in Scala, Java, Python, and R, and an optimized      |\\n|engine that supports general computation graphs for data analysis. It also    |\\n|supports a rich set of higher-level tools including Spark SQL for SQL and     |\\n|DataFrames, MLlib for machine learning, GraphX for graph processing,          |\\n| and Structured Streaming for stream processing.                              |\\n|                                                                              |\\n|<https://spark.apache.org/>                                                   |\\n+------------------------------------------------------------------------------+\\nonly showing top 10 rows\\nscala> strings.count()\\nres2: Long = 109\\nscala>\\nQuite simple. Let’s look at a similar example using the Python interpretive shell,\\npyspark :\\n$ pyspark\\nPython 3.7.3 (default, Mar 27 2019, 09:23:15)\\n[Clang 10.0.1 (clang-1001.0.46.3)] on darwin\\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\nWARNING: An illegal reflective access operation has occurred\\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform \\nWARNING: Use --illegal-access=warn to enable warnings of further illegal \\nreflective access operations\\nWARNING: All illegal access operations will be denied in a future release\\n20/01/10 11:28:29 WARN NativeCodeLoader: Unable to load native-hadoop library \\nfor your platform... using builtin-java classes where applicable\\nUsing Spark\\'s default log4j profile: org/apache/spark/log4j-defaults.properties\\nSetting default log level to \"WARN\".\\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use \\nsetLogLevel(newLevel).\\nWelcome to\\n      ____              __\\n     / __/__  ___ _____/ /__\\n    _\\\\ \\\\/ _ \\\\/ _ `/ __/  \\'_/\\n   /__ / .__/\\\\_,_/_/ /_/\\\\_\\\\   version 3.0.0-preview2\\n      /_/\\nUsing Python version 3.7.3 (default, Mar 27 2019 09:23:15)\\nSparkSession available as \\'spark\\'.\\n>>> strings = spark.read.text(\"../README.md\")\\n24 | Chapter 2: Downloading Apache Spark and Getting Started', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 47}), Document(page_content='>>> strings.show(10, truncate=False)\\n+------------------------------------------------------------------------------+\\n|value                                                                         |\\n+------------------------------------------------------------------------------+\\n|# Apache Spark                                                                |\\n|                                                                              |\\n|Spark is a unified analytics engine for large-scale data processing. It       |\\n|provides high-level APIs in Scala, Java, Python, and R, and an optimized      |\\n|engine that supports general computation graphs for data analysis. It also    |\\n|supports a rich set of higher-level tools including Spark SQL for SQL and     |\\n|DataFrames, MLlib for machine learning, GraphX for graph processing,          |\\n|and Structured Streaming for stream processing.                               |\\n|                                                                              |\\n|<https://spark.apache.org/>                                                   |\\n+------------------------------------------------------------------------------+\\nonly showing top 10 rows\\n>>> strings.count()\\n109\\n>>>\\nTo exit any of the Spark shells, press Ctrl-D. As you can see, this rapid interactivity\\nwith Spark shells is conducive not only to rapid learning but to rapid prototyping,\\ntoo.\\nIn the preceding examples, notice the API syntax and signature parity across both\\nScala and Python. Throughout Spark’s evolution from 1.x, that has been one (among\\nmany) of the enduring improvements.\\nAlso note that we used the high-level Structured APIs to read a text file into a Spark\\nDataFrame rather than an RDD. Throughout the book, we will focus more on these\\nStructured APIs; since Spark 2.x, RDDs are now consigned to low-level APIs.\\nEvery computation expressed in high-level Structured APIs is\\ndecomposed into low-level optimized and generated RDD opera‐\\ntions and then converted into Scala bytecode for the executors’\\nJVMs. This generated RDD operation code is not accessible to\\nusers, nor is it the same as the user-facing RDD APIs.\\nStep 3: Understanding Spark Application Concepts\\nNow that you have downloaded Spark, installed it on your laptop in standalone\\nmode, launched a Spark shell, and executed some short code examples interactively,\\nyou’re ready to take the final step.\\nTo understand what’s happening under the hood with our sample code, you’ll need to\\nbe familiar with some of the key concepts of a Spark application and how the code is\\nStep 3: Understanding Spark Application Concepts | 25', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 48}), Document(page_content='transformed and executed as tasks across the Spark executors. We’ll begin by defining\\nsome important terms:\\nApplication\\nA user program built on Spark using its APIs. It consists of a driver program and\\nexecutors on the cluster.\\nSparkSession\\nAn object that provides a point of entry to interact with underlying Spark func‐\\ntionality and allows programming Spark with its APIs. In an interactive Spark\\nshell, the Spark driver instantiates a SparkSession  for you, while in a Spark\\napplication, you create a SparkSession  object yourself.\\nJob\\nA parallel computation consisting of multiple tasks that gets spawned in response\\nto a Spark action (e.g., save() , collect() ).\\nStage\\nEach job gets divided into smaller sets of tasks called stages that depend on each\\nother.\\nTask\\nA single unit of work or execution that will be sent to a Spark executor.\\nLet’s dig into these concepts in a little more detail.\\nSpark Application and SparkSession\\nAt the core of every Spark application is the Spark driver program, which creates a\\nSparkSession  object. When you’re working with a Spark shell, the driver is part of\\nthe shell and the SparkSession  object (accessible via the variable spark ) is created for\\nyou, as you saw in the earlier examples when you launched the shells.\\nIn those examples, because you launched the Spark shell locally on your laptop, all the\\noperations ran locally, in a single JVM. But you can just as easily launch a Spark shell\\nto analyze data in parallel on a cluster as in local mode. The commands spark-shell\\n--help  or pyspark --help  will show you how to connect to the Spark cluster man‐\\nager. Figure 2-2  shows how Spark executes on a cluster once you’ve done this.\\n26 | Chapter 2: Downloading Apache Spark and Getting Started', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 49}), Document(page_content='Figure 2-2. Spark components communicate through the Spark driver in Spark’s dis‐\\ntributed architecture\\nOnce you have a SparkSession , you can program Spark using the APIs  to perform\\nSpark operations.\\nSpark Jobs\\nDuring interactive sessions with Spark shells, the driver converts your Spark applica‐\\ntion into one or more Spark jobs ( Figure 2-3 ). It then transforms each job into a\\nDAG. This, in essence, is Spark’s execution plan, where each node within a DAG\\ncould be a single or multiple Spark stages.\\nFigure 2-3. Spark driver creating one or more Spark jobs\\nStep 3: Understanding Spark Application Concepts | 27', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 50}), Document(page_content='Spark Stages\\nAs part of the DAG nodes, stages are created based on what operations can be per‐\\nformed serially or in parallel ( Figure 2-4 ). Not all Spark operations can happen in a\\nsingle stage, so they may be divided into multiple stages. Often stages are delineated\\non the operator’s computation boundaries, where they dictate data transfer among\\nSpark executors.\\nFigure 2-4. Spark job creating one or more stages\\nSpark Tasks\\nEach stage is comprised of Spark tasks (a unit of execution), which are then federated\\nacross each Spark executor; each task maps to a single core and works on a single par‐\\ntition of data ( Figure 2-5 ). As such, an executor with 16 cores can have 16 or more\\ntasks working on 16 or more partitions in parallel, making the execution of Spark’s\\ntasks exceedingly parallel!\\nFigure 2-5. Spark stage creating one or more tasks to be distributed to executors\\nTransformations, Actions, and Lazy Evaluation\\nSpark operations on distributed data can be classified into two types: transformations\\nand actions . Transformations, as the name suggests, transform a Spark DataFrame\\ninto a new DataFrame without altering the original data, giving it the property of\\nimmutability. Put another way, an operation such as select()  or filter()  will not\\nchange the original DataFrame; instead, it will return the transformed results of the\\noperation as a new DataFrame.\\n28 | Chapter 2: Downloading Apache Spark and Getting Started', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 51}), Document(page_content='All transformations are evaluated lazily. That is, their results are not computed imme‐\\ndiately, but they are recorded or remembered as a lineage . A recorded lineage allows\\nSpark, at a later time in its execution plan, to rearrange certain transformations, coa‐\\nlesce them, or optimize transformations into stages for more efficient execution. Lazy\\nevaluation is Spark’s strategy for delaying execution until an action is invoked or data\\nis “touched” (read from or written to disk).\\nAn action triggers the lazy evaluation of all the recorded transformations. In\\nFigure 2-6 , all transformations T are recorded until the action A is invoked. Each\\ntransformation T produces a new DataFrame.\\nFigure 2-6. Lazy transformations and eager actions\\nWhile lazy evaluation allows Spark to optimize your queries by peeking into your\\nchained transformations, lineage and data immutability provide fault tolerance.\\nBecause Spark records each transformation in its lineage and the DataFrames are\\nimmutable between transformations, it can reproduce its original state by simply\\nreplaying the recorded lineage, giving it resiliency in the event of failures.\\nTable 2-1  lists some examples of transformations and actions.\\nTable 2-1. Transformations and actions as Spark operations\\nTransformations Actions\\norderBy() show()\\ngroupBy() take()\\nfilter() count()\\nselect() collect()\\njoin() save()\\nThe actions and transformations contribute to a Spark query plan, which we will\\ncover in the next chapter. Nothing in a query plan is executed until an action is\\ninvoked. The following example, shown both in Python and Scala, has two transfor‐\\nmations— read()  and filter() —and one action— count() . The action is what\\nTransformations, Actions, and Lazy Evaluation | 29', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 52}), Document(page_content='triggers  the execution of all transformations recorded as part of the query execution\\nplan. In this example, nothing happens until filtered.count()  is executed in the\\nshell:\\n# In Python \\n>>> strings = spark.read.text(\"../README.md\" )\\n>>> filtered  = strings.filter(strings.value.contains (\"Spark\"))\\n>>> filtered .count()\\n20\\n// In Scala\\nscala> import org.apache.spark.sql.functions._\\nscala> val strings = spark.read.text(\"../README.md\" )\\nscala> val filtered  = strings.filter(col(\"value\").contains (\"Spark\"))\\nscala> filtered .count()\\nres5: Long = 20\\nNarrow and Wide Transformations\\nAs noted, transformations are operations that Spark evaluates lazily. A huge advan‐\\ntage of the lazy evaluation scheme is that Spark can inspect your computational query\\nand ascertain how it can optimize it. This optimization can be done by either joining\\nor pipelining some operations and assigning them to a stage, or breaking them into\\nstages by determining which operations require a shuffle or exchange of data across\\nclusters.\\nTransformations can be classified as having either narrow dependencies  or wide\\ndependencies . Any transformation where a single output partition can be computed\\nfrom a single input partition is a narrow  transformation. For example, in the previous\\ncode snippet, filter()  and contains()  represent narrow transformations because\\nthey can operate on a single partition and produce the resulting output partition\\nwithout any exchange of data.\\nHowever, groupBy()  or orderBy()  instruct Spark to perform wide  transformations,\\nwhere data from other partitions is read in, combined, and written to disk. Since each\\npartition will have its own count of the word that contains the “Spark” word in its row\\nof data, a count ( groupBy() ) will force a shuffle of data from each of the executor’s\\npartitions across the cluster. In this transformation, orderBy()  requires output from\\nother partitions to compute the final aggregation.\\nFigure 2-7  illustrates the two types of dependencies.\\n30 | Chapter 2: Downloading Apache Spark and Getting Started', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 53}), Document(page_content='Figure 2-7. Narrow versus wide transformations\\nThe Spark UI\\nSpark includes a graphical user interface  that you can use to inspect or monitor Spark\\napplications in their various stages of decomposition—that is jobs, stages, and tasks.\\nDepending on how Spark is deployed, the driver launches a web UI, running by\\ndefault on port 4040, where you can view metrics and details such as:\\n•A list of scheduler stages and tasks\\n•A summary of RDD sizes and memory usage\\n•Information about the environment\\n•Information about the running executors\\n•All the Spark SQL queries\\nIn local mode, you can access this interface at http://<localhost>:4040  in a web\\nbrowser.\\nWhen you launch spark-shell , part of the output shows the local‐\\nhost URL to access at port 4040.\\nLet’s inspect how the Python example from the previous section translates into jobs,\\nstages, and tasks. To view what the DAG looks like, click on “DAG Visualization” in\\nthe web UI. As Figure 2-8  shows, the driver created a single job and a single stage.\\nThe Spark UI | 31', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 54}), Document(page_content='Figure 2-8. The DAG for our simple Python example\\nNotice that there is no Exchange , where data is exchanged between executors,\\nrequired because there is only a single stage. The individual operations of the stage\\nare shown in blue boxes.\\nStage 0 is comprised of one task. If you have multiple tasks, they will be executed in\\nparallel. Y ou can view the details of each stage in the Stages tab, as shown in\\nFigure 2-9 .\\n32 | Chapter 2: Downloading Apache Spark and Getting Started', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 55}), Document(page_content='Figure 2-9. Details of stage 0\\nWe will cover the Spark UI in more detail in Chapter 7 . For now, just note that the UI\\nprovides a microscopic lens into Spark’s internal workings as a tool for debugging\\nand inspecting.\\nThe Spark UI | 33', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 56}), Document(page_content='Databricks Community Edition\\nDatabricks is a company that offers a managed Apache Spark platform in the cloud.\\nAside from using your local machine to run Spark in local mode, you can try some of\\nthe examples in this and other chapters using the free Databricks Community Edition\\n(Figure 2-10 ). As a learning tool for Apache Spark, the Community Edition has many\\ntutorials and examples worthy of note. As well as writing your own notebooks in\\nPython, R, Scala, or SQL, you can also import other notebooks, including Jupyter\\nnotebooks.\\nFigure 2-10. Databricks Community Edition\\nTo get an account, go to https://databricks.com/try  and follow the instructions to try\\nthe Community Edition for free. Once registered, you can import the notebooks for\\nthis book from its GitHub repo .\\nYour First Standalone Application\\nTo facilitate learning and exploring, the Spark distribution comes with a set of sample\\napplications for each of Spark’s components. Y ou are welcome to peruse the examples\\ndirectory in your installation location to get an idea of what’s available.\\nFrom the installation directory on your local machine, you can run one of the several\\nJava or Scala sample programs that are provided using the command bin/run-\\nexample <class> [params] . For example:\\n$ ./bin/run-example JavaWordCount README.md\\nThis will spew out INFO  messages on your console along with a list of each word in\\nthe README.md  file and its count (counting words is the “Hello, World” of dis‐\\ntributed computing).\\n34 | Chapter 2: Downloading Apache Spark and Getting Started', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 57}), Document(page_content='Counting M&Ms for the Cookie Monster\\nIn the previous example, we counted words in a file. If the file were huge, it would be\\ndistributed across a cluster partitioned into small chunks of data, and our Spark pro‐\\ngram would distribute the task of counting each word in each partition and return us\\nthe final aggregated count. But that example has become a bit of a cliche.\\nLet’s solve a similar problem, but with a larger data set and using more of Spark’s dis‐\\ntribution functionality and DataFrame APIs. We will cover the APIs used in this pro‐\\ngram in later chapters, but for now bear with us.\\nAmong the authors of this book is a data scientist who loves to bake cookies with\\nM&Ms in them, and she rewards her students in the US states where she frequently\\nteaches machine learning and data science courses with batches of those cookies. But\\nshe’s data-driven, obviously, and wants to ensure that she gets the right colors of\\nM&Ms in the cookies for students in the different states ( Figure 2-11 ).\\nFigure 2-11. Distribution of M&Ms by color (source: https://oreil.ly/mhWIT )\\nLet’s write a Spark program that reads a file with over 100,000 entries (where each\\nrow or line has a <state, mnm_color , count>) and computes and aggregates the\\ncounts for each color and state. These aggregated counts tell us the colors of M&Ms\\nfavored by students in each state. The complete Python listing is provided in\\nExample 2-1 .\\nYour First Standalone Application | 35', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 58}), Document(page_content='Example 2-1. Counting and aggregating M&Ms (Python version)\\n# Import the necessary libraries.\\n# Since we are using Python, import the SparkSession and related functions\\n# from the PySpark module.\\nimport sys\\nfrom pyspark.sql  import SparkSession\\nfrom pyspark.sql.functions  import count\\nif __name__  == \"__main__\" :\\n   if len(sys.argv) != 2:\\n       print(\"Usage: mnmcount <file>\" , file=sys.stderr)\\n       sys.exit(-1)\\n   # Build a SparkSession using the SparkSession APIs.\\n   # If one does not exist, then create an instance. There\\n   # can only be one SparkSession per JVM.\\n   spark = (SparkSession\\n     .builder\\n     .appName(\"PythonMnMCount\" )\\n     .getOrCreate ())\\n   # Get the M&M data set filename from the command-line arguments\\n   mnm_file  = sys.argv[1]\\n   # Read the file into a Spark DataFrame using the CSV\\n   # format by inferring the schema and specifying that the\\n   # file contains a header, which provides column names for comma-\\n   # separated fields.\\n   mnm_df = (spark.read.format(\"csv\") \\n     .option(\"header\" , \"true\") \\n     .option(\"inferSchema\" , \"true\") \\n     .load(mnm_file ))\\n   # We use the DataFrame high-level APIs. Note\\n   # that we don\\'t use RDDs at all. Because some of Spark\\'s \\n   # functions return the same object, we can chain function calls.\\n   # 1. Select from the DataFrame the fields \"State\", \"Color\", and \"Count\"\\n   # 2. Since we want to group each state and its M&M color count,\\n   #    we use groupBy()\\n   # 3. Aggregate counts of all colors and groupBy() State and Color\\n   # 4  orderBy() in descending order\\n   count_mnm_df  = (mnm_df\\n     .select(\"State\", \"Color\", \"Count\") \\n     .groupBy(\"State\", \"Color\") \\n     .agg(count(\"Count\").alias(\"Total\")) \\n     .orderBy(\"Total\", ascending =False))\\n   # Show the resulting aggregations for all the states and colors;\\n   # a total count of each color per state.\\n   # Note show() is an action, which will trigger the above\\n   # query to be executed.\\n   count_mnm_df .show(n=60, truncate =False)\\n   print(\"Total Rows = %d\" % (count_mnm_df .count()))\\n36 | Chapter 2: Downloading Apache Spark and Getting Started', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 59}), Document(page_content='   # While the above code aggregated and counted for all \\n   # the states, what if we just want to see the data for \\n   # a single state, e.g., CA? \\n   # 1. Select from all rows in the DataFrame\\n   # 2. Filter only CA state\\n   # 3. groupBy() State and Color as we did above\\n   # 4. Aggregate the counts for each color\\n   # 5. orderBy() in descending order  \\n   # Find the aggregate count for California by filtering\\n   ca_count_mnm_df  = (mnm_df\\n     .select(\"State\", \"Color\", \"Count\") \\n     .where(mnm_df.State == \"CA\") \\n     .groupBy(\"State\", \"Color\") \\n     .agg(count(\"Count\").alias(\"Total\")) \\n     .orderBy(\"Total\", ascending =False))\\n   # Show the resulting aggregation for California.\\n   # As above, show() is an action that will trigger the execution of the\\n   # entire computation. \\n   ca_count_mnm_df .show(n=10, truncate =False)\\n   # Stop the SparkSession\\n   spark.stop()\\nY ou can enter this code into a Python file called mnmcount.py  using your favorite edi‐\\ntor, download the mnn_dataset.csv  file from this book’s GitHub repo , and submit it as\\na Spark job using the submit-spark  script in the installation’s bin directory. Set your\\nSPARK_HOME  environment variable to the root-level directory where you installed\\nSpark on your local machine.\\nThe preceding code uses the DataFrame API, which reads like\\nhigh-level DSL queries. We will cover this and the other APIs in the\\nnext chapter; for now, note the clarity and simplicity with which\\nyou can instruct Spark what to do, not how to do it, unlike with the\\nRDD API. Cool stuff!\\nTo avoid having verbose INFO  messages printed to the console, copy the log4j.proper‐\\nties.template  file to log4j.properties  and set log4j.rootCategory=WARN  in the\\nconf/log4j.properties  file.\\nLet’s submit our first Spark job using the Python APIs (for an explanation of what the\\ncode does, please read the inline comments in Example 2-1 ):\\n$SPARK_HOME/bin/spark-submit mnmcount.py data/mnm_dataset.csv\\n+-----+------+-----+\\n|State|Color |Total|\\n+-----+------+-----+\\n|CA   |Yellow|1807 |\\n|WA   |Green |1779 |\\n|OR   |Orange|1743 |\\nYour First Standalone Application | 37', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 60}), Document(page_content='|TX   |Green |1737 |\\n|TX   |Red   |1725 |\\n|CA   |Green |1723 |\\n|CO   |Yellow|1721 |\\n|CA   |Brown |1718 |\\n|CO   |Green |1713 |\\n|NV   |Orange|1712 |\\n|TX   |Yellow|1703 |\\n|NV   |Green |1698 |\\n|AZ   |Brown |1698 |\\n|CO   |Blue  |1695 |\\n|WY   |Green |1695 |\\n|NM   |Red   |1690 |\\n|AZ   |Orange|1689 |\\n|NM   |Yellow|1688 |\\n|NM   |Brown |1687 |\\n|UT   |Orange|1684 |\\n|NM   |Green |1682 |\\n|UT   |Red   |1680 |\\n|AZ   |Green |1676 |\\n|NV   |Yellow|1675 |\\n|NV   |Blue  |1673 |\\n|WA   |Red   |1671 |\\n|WY   |Red   |1670 |\\n|WA   |Brown |1669 |\\n|NM   |Orange|1665 |\\n|WY   |Blue  |1664 |\\n|WA   |Yellow|1663 |\\n|WA   |Orange|1658 |\\n|NV   |Brown |1657 |\\n|CA   |Orange|1657 |\\n|CA   |Red   |1656 |\\n|CO   |Brown |1656 |\\n|UT   |Blue  |1655 |\\n|AZ   |Yellow|1654 |\\n|TX   |Orange|1652 |\\n|AZ   |Red   |1648 |\\n|OR   |Blue  |1646 |\\n|UT   |Yellow|1645 |\\n|OR   |Red   |1645 |\\n|CO   |Orange|1642 |\\n|TX   |Brown |1641 |\\n|NM   |Blue  |1638 |\\n|AZ   |Blue  |1636 |\\n|OR   |Green |1634 |\\n|UT   |Brown |1631 |\\n|WY   |Yellow|1626 |\\n|WA   |Blue  |1625 |\\n|CO   |Red   |1624 |\\n|OR   |Brown |1621 |\\n|TX   |Blue  |1614 |\\n|OR   |Yellow|1614 |\\n38 | Chapter 2: Downloading Apache Spark and Getting Started', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 61}), Document(page_content='|NV   |Red   |1610 |\\n|CA   |Blue  |1603 |\\n|WY   |Orange|1595 |\\n|UT   |Green |1591 |\\n|WY   |Brown |1532 |\\n+-----+------+-----+\\nTotal Rows = 60\\n+-----+------+-----+\\n|State|Color |Total|\\n+-----+------+-----+\\n|CA   |Yellow|1807 |\\n|CA   |Green |1723 |\\n|CA   |Brown |1718 |\\n|CA   |Orange|1657 |\\n|CA   |Red   |1656 |\\n|CA   |Blue  |1603 |\\n+-----+------+-----+\\nFirst we see all the aggregations for each M&M color for each state, followed by those\\nonly for CA (where the preferred color is yellow).\\nWhat if you want to use a Scala version of this same Spark program? The APIs are\\nsimilar; in Spark, parity is well preserved across the supported languages, with minor\\nsyntax differences. Example 2-2  is the Scala version of the program. Take a look, and\\nin the next section we’ll show you how to build and run the application.\\nExample 2-2. Counting and aggregating M&Ms (Scala version)\\npackage main.scala.chapter2\\nimport org.apache.spark.sql.SparkSession\\nimport org.apache.spark.sql.functions._\\n/**\\n * Usage: MnMcount <mnm_file_dataset>\\n */\\nobject MnMcount  {\\n def main(args: Array[String]) {\\n   val spark = SparkSession\\n     .builder\\n     .appName(\"MnMCount\" )\\n     .getOrCreate ()\\n   if (args.length < 1) {\\n     print(\"Usage: MnMcount <mnm_file_dataset>\" )\\n     sys.exit(1)\\n   }\\n   // Get the M&M data set filename\\n   val mnmFile = args(0)\\nYour First Standalone Application | 39', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 62}), Document(page_content='   // Read the file into a Spark DataFrame\\n   val mnmDF = spark.read.format(\"csv\")\\n     .option(\"header\" , \"true\")\\n     .option(\"inferSchema\" , \"true\")\\n     .load(mnmFile)\\n   // Aggregate counts of all colors and groupBy() State and Color\\n   // orderBy() in descending order\\n   val countMnMDF  = mnmDF\\n     .select(\"State\", \"Color\", \"Count\")\\n     .groupBy(\"State\", \"Color\")\\n     .agg(count(\"Count\").alias(\"Total\"))\\n     .orderBy(desc(\"Total\"))\\n   // Show the resulting aggregations for all the states and colors\\n   countMnMDF .show(60)\\n   println(s\"Total Rows = ${countMnMDF .count()}\")\\n   println()\\n   // Find the aggregate counts for California by filtering\\n   val caCountMnMDF  = mnmDF\\n     .select(\"State\", \"Color\", \"Count\")\\n     .where(col(\"State\") === \"CA\")\\n     .groupBy(\"State\", \"Color\")\\n     .agg(count(\"Count\").alias(\"Total\"))\\n     .orderBy(desc(\"Total\"))\\n   // Show the resulting aggregations for California\\n   caCountMnMDF .show(10)\\n   // Stop the SparkSession\\n   spark.stop()\\n }\\n}\\nBuilding Standalone Applications in Scala\\nWe will now show you how to build your first Scala Spark program, using the Scala\\nBuild Tool (sbt) .\\nBecause Python is an interpreted language and there is no such\\nstep as compiling first (though it’s possible to compile your Python\\ncode into bytecode in .pyc), we will not go into this step here. For\\ndetails on how to use Maven to build Java Spark programs, we refer\\nyou to the guide  on the Apache Spark website. For brevity in this\\nbook, we cover examples mainly in Python and Scala.\\nbuild.sbt  is the specification file that, like a makefile, describes and instructs the Scala\\ncompiler to build your Scala-related tasks, such as jars, packages, what dependencies\\nto resolve, and where to look for them. In our case, we have a simple sbt file for our\\nM&M code ( Example 2-3 ).\\n40 | Chapter 2: Downloading Apache Spark and Getting Started', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 63}), Document(page_content='Example 2-3. sbt build file\\n// Name of the package\\nname := \"main/scala/chapter2\"\\n// Version of our package\\nversion := \"1.0\"\\n// Version of Scala\\nscalaVersion  := \"2.12.10\"\\n// Spark library dependencies\\nlibraryDependencies  ++= Seq(\\n  \"org.apache.spark\"  %% \"spark-core\"  % \"3.0.0-preview2\" ,\\n  \"org.apache.spark\"  %% \"spark-sql\"   % \"3.0.0-preview2\"\\n)\\nAssuming that you have the Java Development Kit (JDK)  and sbt installed and\\nJAVA_HOME  and SPARK_HOME  set, with a single command, you can build your Spark\\napplication:\\n$ sbt clean package\\n[info] Updated file /Users/julesdamji/gits/LearningSparkV2/chapter2/scala/\\nproject/build.properties: set sbt.version to 1.2.8\\n[info] Loading project definition from /Users/julesdamji/gits/LearningSparkV2/\\nchapter2/scala/project\\n[info] Updating \\n[info] Done updating.\\n...\\n[info] Compiling 1 Scala source to /Users/julesdamji/gits/LearningSparkV2/\\nchapter2/scala/target/scala-2.12/classes ...\\n[info] Done compiling.\\n[info] Packaging /Users/julesdamji/gits/LearningSparkV2/chapter2/scala/target/\\nscala-2.12/main-scala-chapter2_2.12-1.0.jar ...\\n[info] Done packaging.\\n[success] Total time: 6 s, completed Jan 11, 2020, 4:11:02 PM\\nAfter a successful build, you can run the Scala version of the M&M count example as\\nfollows:\\n$SPARK_HOME/bin/spark-submit --class main.scala.chapter2.MnMcount \\\\ \\njars/main-scala-chapter2_2.12-1.0.jar data/mnm_dataset.csv\\n...\\n...\\n20/01/11 16:00:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: \\nStage finished\\n20/01/11 16:00:48 INFO DAGScheduler: Job 4 finished: show at MnMcount.scala:49, \\ntook 0.264579 s\\n+-----+------+-----+\\n|State| Color|Total|\\n+-----+------+-----+\\n|   CA|Yellow| 1807|\\n|   CA| Green| 1723|\\n|   CA| Brown| 1718|\\n|   CA|Orange| 1657|\\nYour First Standalone Application | 41', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 64}), Document(page_content='|   CA|   Red| 1656|\\n|   CA|  Blue| 1603|\\n+-----+------+-----+\\nThe output is the same as for the Python run. Try it!\\nThere you have it—our data scientist author will be more than happy to use this data\\nto decide what colors of M&Ms to use in the cookies she bakes for her classes in any\\nof the states she teaches in.\\nSummary\\nIn this chapter, we covered the three simple steps you need to take to get started with\\nApache Spark: downloading the framework, familiarizing yourself with the Scala or\\nPySpark interactive shell, and getting to grips with high-level Spark application con‐\\ncepts and terms. We gave a quick overview of the process by which you can use trans‐\\nformations and actions to write a Spark application, and we briefly introduced using\\nthe Spark UI to examine the jobs, stages, and tasks created.\\nFinally, through a short example, we showed you how you can use the high-level\\nStructured APIs to tell Spark what to do—which brings us to the next chapter, where\\nwe examine those APIs in more detail.\\n42 | Chapter 2: Downloading Apache Spark and Getting Started', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 65}), Document(page_content='CHAPTER 3\\nApache Spark’s Structured APIs\\nIn this chapter, we will explore the principal motivations behind adding structure to\\nApache Spark, how those motivations led to the creation of high-level APIs (Data‐\\nFrames and Datasets), and their unification in Spark 2.x across its components. We’ll\\nalso look at the Spark SQL engine that underpins these structured high-level APIs.\\nWhen Spark SQL  was first introduced in the early Spark 1.x releases, followed by\\nDataFrames  as a successor to SchemaRDDs  in Spark 1.3, we got our first glimpse of\\nstructure in Spark. Spark SQL introduced high-level expressive operational functions,\\nmimicking SQL-like syntax, and DataFrames, which laid the foundation for more\\nstructure in subsequent releases, paved the path to performant operations in Spark’s\\ncomputational queries.\\nBut before we talk about the newer Structured APIs, let’s get a brief glimpse of what\\nit’s like to not have structure in Spark by taking a peek at the simple RDD program‐\\nming API model.\\nSpark: What’s Underneath an RDD?\\nThe RDD  is the most basic abstraction in Spark. There are three vital characteristics\\nassociated with an RDD:\\n•Dependencies\\n•Partitions (with some locality information)\\n•Compute function: Partition => Iterator[T]\\n43', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 66}), Document(page_content='All three are integral to the simple RDD programming API model upon which all\\nhigher-level functionality is constructed. First, a list of dependencies  that instructs\\nSpark how an RDD is constructed with its inputs is required. When necessary to\\nreproduce results, Spark can recreate an RDD from these dependencies and replicate\\noperations on it. This characteristic gives RDDs resiliency.\\nSecond, partitions  provide Spark the ability to split the work to parallelize computa‐\\ntion on partitions across executors. In some cases—for example, reading from\\nHDFS—Spark  will use locality information to send work to executors close to the\\ndata. That way less data is transmitted over the network.\\nAnd finally, an RDD has a compute function  that produces an Iterator[T]  for the\\ndata that will be stored in the RDD.\\nSimple and elegant! Y et there are a couple of problems with this original model. For\\none, the compute function (or computation) is opaque to Spark. That is, Spark does\\nnot know what you are doing in the compute function. Whether you are performing\\na join, filter, select, or aggregation, Spark only sees it as a lambda expression. Another\\nproblem is that the Iterator[T]  data type is also opaque for Python RDDs; Spark\\nonly knows that it’s a generic object in Python.\\nFurthermore, because it’s unable to inspect the computation or expression in the\\nfunction, Spark has no way to optimize the expression—it has no comprehension of\\nits intention. And finally, Spark has no knowledge of the specific data type in T. To\\nSpark it’s an opaque object; it has no idea if you are accessing a column of a certain\\ntype within an object. Therefore, all Spark can do is serialize the opaque object as a\\nseries of bytes, without using any data compression techniques.\\nThis opacity clearly hampers Spark’s ability to rearrange your computation into an\\nefficient query plan. So what’s the solution?\\nStructuring Spark\\nSpark 2.x introduced a few key schemes for structuring Spark. One is to express com‐\\nputations by using common patterns found in data analysis. These patterns are\\nexpressed as high-level operations such as filtering, selecting, counting, aggregating,\\naveraging, and grouping. This provides added clarity and simplicity.\\nThis specificity is further narrowed through the use of a set of common operators in a\\nDSL. Through a set of operations in DSL, available as APIs in Spark’s supported lan‐\\nguages (Java, Python, Spark, R, and SQL), these operators let you tell Spark what you\\nwish to compute with your data, and as a result, it can construct an efficient query\\nplan for execution.\\n44 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 67}), Document(page_content='And the final scheme of order and structure is to allow you to arrange your data in a\\ntabular format, like a SQL table or spreadsheet, with supported structured data types\\n(which we will cover shortly).\\nBut what’s all this structure good for?\\nKey Merits and Benefits\\nStructure yields a number of benefits, including better performance and space effi‐\\nciency across Spark components. We will explore these benefits further when we talk\\nabout the use of the DataFrame and Dataset APIs shortly, but for now we’ll concen‐\\ntrate on the other advantages: expressivity, simplicity, composability, and uniformity.\\nLet’s demonstrate expressivity and composability first, with a simple code snippet. In\\nthe following example, we want to aggregate all the ages for each name, group by\\nname, and then average the ages—a common pattern in data analysis and discovery.\\nIf we were to use the low-level RDD API for this, the code would look as follows:\\n# In Python\\n# Create an RDD of tuples (name, age)\\ndataRDD = sc.parallelize ([(\"Brooke\" , 20), (\"Denny\", 31), (\"Jules\", 30), \\n  (\"TD\", 35), (\"Brooke\" , 25)])\\n# Use map and reduceByKey transformations with their lambda \\n# expressions to aggregate and then compute average\\nagesRDD = (dataRDD\\n  .map(lambda x: (x[0], (x[1], 1)))\\n  .reduceByKey (lambda x, y: (x[0] + y[0], x[1] + y[1]))\\n  .map(lambda x: (x[0], x[1][0]/x[1][1])))\\nNo one would dispute that this code, which tells Spark how to  aggregate keys and\\ncompute averages with a string of lambda functions, is cryptic and hard to read. In\\nother words, the code is instructing Spark how to compute the query. It’s completely\\nopaque to Spark, because it doesn’t communicate the intention. Furthermore, the\\nequivalent RDD code in Scala would look very different from the Python code shown\\nhere.\\nBy contrast, what if we were to express the same query with high-level DSL operators\\nand the DataFrame API, thereby instructing Spark what to do ? Have a look:\\n# In Python \\nfrom pyspark.sql  import SparkSession\\nfrom pyspark.sql.functions  import avg\\n# Create a DataFrame using SparkSession\\nspark = (SparkSession\\n  .builder\\n  .appName(\"AuthorsAges\" )\\n  .getOrCreate ())\\n# Create a DataFrame \\ndata_df = spark.createDataFrame ([(\"Brooke\" , 20), (\"Denny\", 31), (\"Jules\", 30), \\nStructuring Spark | 45', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 68}), Document(page_content='  (\"TD\", 35), (\"Brooke\" , 25)], [\"name\", \"age\"])\\n# Group the same names together, aggregate their ages, and compute an average\\navg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\\n# Show the results of the final execution\\navg_df.show()\\n+------+--------+\\n|  name|avg(age)|\\n+------+--------+\\n|Brooke|    22.5|\\n| Jules|    30.0|\\n|    TD|    35.0|\\n| Denny|    31.0|\\n+------+--------+\\nThis version of the code is far more expressive as well as simpler than the earlier ver‐\\nsion, because we are using high-level DSL operators and APIs to tell Spark what to\\ndo. In effect, we have employed these operators to compose our query. And because\\nSpark can inspect or parse this query and understand our intention, it can optimize\\nor arrange the operations for efficient execution. Spark knows exactly what  we wish\\nto do: group people by their names, aggregate their ages, and then compute the aver‐\\nage age of all people with the same name. We’ve composed an entire computation\\nusing high-level operators as a single simple query—how expressive is that?\\nSome would contend that by using only high-level, expressive DSL operators mapped\\nto common or recurring data analysis patterns to introduce order and structure, we\\nare limiting the scope of the developers’ ability to instruct the compiler or control\\nhow their queries should be computed. Rest assured that you are not confined to\\nthese structured patterns; you can switch back at any time to the unstructured low-\\nlevel RDD API, although we hardly ever find a need to do so.\\nAs well as being simpler to read, the structure of Spark’s high-level APIs also introdu‐\\nces uniformity across its components and languages. For example, the Scala code\\nshown here does the same thing as the previous Python code—and the API looks\\nnearly identical:\\n// In Scala\\nimport org.apache.spark.sql.functions.avg\\nimport org.apache.spark.sql.SparkSession\\n// Create a DataFrame using SparkSession\\nval spark = SparkSession\\n  .builder\\n  .appName(\"AuthorsAges\" )\\n  .getOrCreate ()\\n// Create a DataFrame of names and ages\\nval dataDF = spark.createDataFrame (Seq((\"Brooke\" , 20), (\"Brooke\" , 25), \\n  (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35))).toDF(\"name\", \"age\")\\n// Group the same names together, aggregate their ages, and compute an average\\nval avgDF = dataDF.groupBy(\"name\").agg(avg(\"age\"))\\n// Show the results of the final execution\\n46 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 69}), Document(page_content='avgDF.show()\\n+------+--------+\\n|  name|avg(age)|\\n+------+--------+\\n|Brooke|    22.5|\\n| Jules|    30.0|\\n|    TD|    35.0|\\n| Denny|    31.0|\\n+------+--------+\\nSome of these DSL operators perform relational-like operations\\nthat you’ll be familiar with if you know SQL, such as selecting, fil‐\\ntering, grouping, and aggregation.\\nAll of this simplicity and expressivity that we developers cherish is possible because of\\nthe Spark SQL engine upon which the high-level Structured APIs are built. It is\\nbecause of this engine, which underpins all the Spark components, that we get uni‐\\nform APIs. Whether you express a query against a DataFrame in Structured Stream‐\\ning or MLlib, you are always transforming and operating on DataFrames as\\nstructured data. We’ll take a closer look at the Spark SQL engine later in this chapter,\\nbut for now let’s explore those APIs and DSLs for common operations and how to use\\nthem for data analytics.\\nThe DataFrame API\\nInspired by pandas DataFrames  in structure, format, and a few specific operations,\\nSpark DataFrames are like distributed in-memory tables with named columns and\\nschemas, where each column has a specific data type: integer, string, array, map, real,\\ndate, timestamp, etc. To a human’s eye, a Spark DataFrame is like a table. An example\\nis shown in Table 3-1 .\\nTable 3-1. The table-like format of a DataFrame\\nId\\n(Int)First\\n(String)Last\\n(String)Url\\n(String)Published\\n(Date)Hits\\n(Int)Campaigns\\n(List[Strings])\\n1 Jules Damji https://\\ntinyurl.11/4/2016 4535 [twitter, LinkedIn]\\n2 Brooke Wenig https://\\ntinyurl.25/5/2018 8908 [twitter, LinkedIn]\\n3 Denny Lee https://\\ntinyurl.36/7/2019 7659 [web, twitter, FB, \\nLinkedIn]\\n4 Tathagata Das https://\\ntinyurl.45/12/2018 10568 [twitter, FB]\\nThe DataFrame API | 47', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 70}), Document(page_content='Id\\n(Int)First\\n(String)Last\\n(String)Url\\n(String)Published\\n(Date)Hits\\n(Int)Campaigns\\n(List[Strings])\\n5 Matei Zaharia https://\\ntinyurl.55/14/2014 40578 [web, twitter, FB, \\nLinkedIn]\\n6 Reynold Xin https://\\ntinyurl.63/2/2015 25568 [twitter, LinkedIn]\\nWhen data is visualized as a structured table, it’s not only easy to digest but also easy\\nto work with when it comes to common operations you might want to execute on\\nrows and columns. Also recall that, as you learned in Chapter 2 , DataFrames are\\nimmutable and Spark keeps a lineage of all transformations. Y ou can add or change\\nthe names and data types of the columns, creating new DataFrames while the previ‐\\nous versions are preserved. A named column in a DataFrame and its associated Spark\\ndata type can be declared in the schema.\\nLet’s examine the generic and structured data types available in Spark before we use\\nthem to define a schema. Then we’ll illustrate how to create a DataFrame with a\\nschema, capturing the data in Table 3-1 .\\nSpark’s Basic Data Types\\nMatching its supported programming languages, Spark supports basic internal data\\ntypes. These data types can be declared in your Spark application or defined in your\\nschema. For example, in Scala, you can define or declare a particular column name to\\nbe of type String , Byte , Long , or Map, etc. Here, we define variable names tied to a\\nSpark data type:\\n$SPARK_HOME /bin/spark-shell\\nscala> import org.apache.spark.sql.types._\\nimport org.apache.spark.sql.types._\\nscala> val nameTypes  = StringType\\nnameTypes : org.apache.spark.sql. types.StringType. type = StringType\\nscala> val firstName  = nameTypes\\nfirstName : org.apache.spark.sql. types.StringType. type = StringType\\nscala> val lastName  = nameTypes\\nlastName : org.apache.spark.sql. types.StringType. type = StringType\\nTable 3-2  lists the basic Scala data types supported in Spark. They all are subtypes of\\nthe class DataTypes , except for DecimalType .\\n48 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 71}), Document(page_content='Table 3-2. Basic Scala data types in Spark\\nData type Value assigned in Scala API to instantiate\\nByteType Byte DataTypes.ByteType\\nShortType Short DataTypes.ShortType\\nIntegerType Int DataTypes.IntegerType\\nLongType Long DataTypes.LongType\\nFloatType Float DataTypes.FloatType\\nDoubleType Double DataTypes.DoubleType\\nStringType String DataTypes.StringType\\nBooleanType Boolean DataTypes.BooleanType\\nDecimalType java.math.BigDecimal DecimalType\\nSpark supports similar basic Python data types , as enumerated in Table 3-3 .\\nTable 3-3. Basic Python data types in Spark\\nData type Value assigned in Python API to instantiate\\nByteType int DataTypes.ByteType\\nShortType int DataTypes.ShortType\\nIntegerType int DataTypes.IntegerType\\nLongType int DataTypes.LongType\\nFloatType float DataTypes.FloatType\\nDoubleType float DataTypes.DoubleType\\nStringType str DataTypes.StringType\\nBooleanType bool DataTypes.BooleanType\\nDecimalType decimal.Decimal DecimalType\\nSpark’s Structured and Complex Data Types\\nFor complex data analytics, you won’t deal only with simple or basic data types. Y our\\ndata will be complex, often structured or nested, and you’ll need Spark to handle\\nthese complex data types. They come in many forms: maps, arrays, structs, dates,\\ntimestamps, fields, etc. Table 3-4  lists the Scala structured data types that Spark\\nsupports.\\nThe DataFrame API | 49', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 72}), Document(page_content='Table 3-4. Scala structured data types in Spark\\nData type Value assigned in Scala API to instantiate\\nBinaryType Array[Byte] DataTypes.BinaryType\\nTimestamp\\nTypejava.sql.Timestamp DataTypes.TimestampType\\nDateType java.sql.Date DataTypes.DateType\\nArrayType scala.collection.Seq DataTypes.createArrayType(Element\\nType)\\nMapType scala.collection.Map DataTypes.createMapType(keyType, \\nvalueType)\\nStructType org.apache.spark.sql.Row StructType(ArrayType[fieldTypes])\\nStructField A value type corresponding to the type of this fieldStructField(name, dataType, [nulla\\nble])\\nThe equivalent structured data types in Python that Spark supports are enumerated\\nin Table 3-5 .\\nTable 3-5. Python structured data types in Spark\\nData type Value assigned in Python API to instantiate\\nBinaryType bytearray BinaryType()\\nTimestampType datetime.datetime TimestampType()\\nDateType datetime.date DateType()\\nArrayType List, tuple, or array ArrayType(dataType, [nullable])\\nMapType dict MapType(keyType, valueType, [nul\\nlable])\\nStructType List or tuple StructType([fields])\\nStructField A value type corresponding to the type of this\\nfieldStructField(name, dataType, [nul\\nlable])\\nWhile these tables showcase the myriad types supported, it’s far more important to\\nsee how these types come together when you define a schema for your data.\\nSchemas and Creating DataFrames\\nA schema  in Spark defines the column names and associated data types for a Data‐\\nFrame. Most often, schemas come into play when you are reading structured data\\nfrom an external data source (more on this in the next chapter). Defining a schema\\nup front as opposed to taking a schema-on-read approach offers three benefits:\\n50 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 73}), Document(page_content='•Y ou relieve Spark from the onus of inferring data types.\\n•Y ou prevent Spark from creating a separate job just to read a large portion of\\nyour file to ascertain the schema, which for a large data file can be expensive and\\ntime-consuming.\\n•Y ou can detect errors early if data doesn’t match the schema.\\nSo, we encourage you to always define your schema up front whenever you want to\\nread a large file from a data source. For a short illustration, let’s define a schema for\\nthe data in Table 3-1  and use that schema to create a DataFrame.\\nTwo ways to define  a schema\\nSpark allows you to define a schema in two ways. One is to define it programmati‐\\ncally, and the other is to employ a Data Definition Language (DDL) string, which is\\nmuch simpler and easier to read.\\nTo define a schema programmatically for a DataFrame with three named columns,\\nauthor , title , and pages , you can use the Spark DataFrame API. For example:\\n// In Scala\\nimport org.apache.spark.sql.types._\\nval schema = StructType (Array(StructField (\"author\" , StringType , false),\\n  StructField (\"title\", StringType , false),\\n  StructField (\"pages\", IntegerType , false)))\\n# In Python\\nfrom pyspark.sql.types  import *\\nschema = StructType ([StructField (\"author\" , StringType (), False),\\n  StructField (\"title\", StringType (), False),\\n  StructField (\"pages\", IntegerType (), False)])\\nDefining the same schema using DDL is much simpler:\\n// In Scala\\nval schema = \"author STRING, title STRING, pages INT\"\\n# In Python\\nschema = \"author STRING, title STRING, pages INT\"\\nY ou can choose whichever way you like to define a schema. For many examples, we\\nwill use both:\\n# In Python \\nfrom pyspark.sql  import SparkSession\\n# Define schema for our data using DDL \\nschema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, \\n  `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\\n# Create our static data\\ndata = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\" , \"1/4/2016\" , 4535, [\"twitter\" ,\\nThe DataFrame API | 51', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 74}), Document(page_content='\"LinkedIn\" ]],\\n       [2, \"Brooke\" ,\"Wenig\", \"https://tinyurl.2\" , \"5/5/2018\" , 8908, [\"twitter\" ,\\n\"LinkedIn\" ]],\\n       [3, \"Denny\", \"Lee\", \"https://tinyurl.3\" , \"6/7/2019\" , 7659, [\"web\",\\n\"twitter\" , \"FB\", \"LinkedIn\" ]],\\n       [4, \"Tathagata\" , \"Das\", \"https://tinyurl.4\" , \"5/12/2018\" , 10568, \\n[\"twitter\" , \"FB\"]],\\n       [5, \"Matei\",\"Zaharia\" , \"https://tinyurl.5\" , \"5/14/2014\" , 40578, [\"web\",\\n\"twitter\" , \"FB\", \"LinkedIn\" ]],\\n       [6, \"Reynold\" , \"Xin\", \"https://tinyurl.6\" , \"3/2/2015\" , 25568, \\n[\"twitter\" , \"LinkedIn\" ]]\\n      ]\\n# Main program\\nif __name__  == \"__main__\" :\\n   # Create a SparkSession\\n   spark = (SparkSession\\n     .builder\\n     .appName(\"Example-3_6\" )\\n     .getOrCreate ())\\n   # Create a DataFrame using the schema defined above\\n   blogs_df  = spark.createDataFrame (data, schema)\\n   # Show the DataFrame; it should reflect our table above\\n   blogs_df .show()\\n   # Print the schema used by Spark to process the DataFrame\\n   print(blogs_df .printSchema ())\\nRunning this program from the console will produce the following output:\\n$ spark-submit Example-3_6.py\\n...\\n+-------+---------+-------+-----------------+---------+-----+------------------+\\n|Id     |First    |Last   |Url              |Published |Hits |Campaigns          |\\n+-------+---------+-------+-----------------+---------+-----+------------------+\\n|1      |Jules    |Damji  |https://tinyurl.1|1/4/2016 |4535 |[twitter,...]     |\\n|2      |Brooke   |Wenig  |https://tinyurl.2|5/5/2018 |8908 |[twitter,...]     |\\n|3      |Denny    |Lee    |https://tinyurl.3|6/7/2019 |7659 |[web, twitter...] |\\n|4      |Tathagata |Das    |https://tinyurl.4|5/12/2018|10568|[twitter, FB]     |\\n|5      |Matei    |Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter,...]|\\n|6      |Reynold  |Xin    |https://tinyurl.6|3/2/2015 |25568|[twitter,...]     |\\n+-------+---------+-------+-----------------+---------+-----+------------------+\\nroot\\n |-- Id: integer (nullable  = false)\\n |-- First: string (nullable  = false)\\n |-- Last: string (nullable  = false)\\n |-- Url: string (nullable  = false)\\n |-- Published : string (nullable  = false)\\n |-- Hits: integer (nullable  = false)\\n |-- Campaigns : array (nullable  = false)\\n |    |-- element: string (containsNull  = false)\\n52 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 75}), Document(page_content='If you want to use this schema elsewhere in your code, simply execute\\nblogs_df.schema  and it will return the schema definition:\\nStructType(List(StructField(\"Id\",IntegerType,false),\\nStructField(\"First\",StringType,false),\\nStructField(\"Last\",StringType,false),\\nStructField(\"Url\",StringType,false),\\nStructField(\"Published\",StringType,false),\\nStructField(\"Hits\",IntegerType,false),\\nStructField(\"Campaigns\",ArrayType(StringType,true),false)))\\nAs you can observe, the DataFrame layout matches that of Table 3-1  along with the\\nrespective data types and schema output.\\nIf you were to read the data from a JSON file instead of creating static data, the\\nschema definition would be identical. Let’s illustrate the same code with a Scala exam‐\\nple, this time reading from a JSON file:\\n// In Scala\\npackage main.scala.chapter3\\nimport org.apache.spark.sql.SparkSession\\nimport org.apache.spark.sql.types._\\nobject Example3_7  {\\n def main(args: Array[String]) {\\n   val spark = SparkSession\\n     .builder\\n     .appName(\"Example-3_7\" )\\n     .getOrCreate ()\\n   if (args.length <= 0) {\\n     println(\"usage Example3_7 <file path to blogs.json>\" )\\n     System.exit(1)\\n   }\\n   // Get the path to the JSON file\\n   val jsonFile  = args(0)\\n   // Define our schema programmatically\\n   val schema = StructType (Array(StructField (\"Id\", IntegerType , false),\\n     StructField (\"First\", StringType , false),\\n     StructField (\"Last\", StringType , false),\\n     StructField (\"Url\", StringType , false),\\n     StructField (\"Published\" , StringType , false),\\n     StructField (\"Hits\", IntegerType , false),\\n     StructField (\"Campaigns\" , ArrayType (StringType ), false)))\\n   // Create a DataFrame by reading from the JSON file \\n   // with a predefined schema\\n   val blogsDF = spark.read.schema(schema).json(jsonFile )\\n   // Show the DataFrame schema as output\\n   blogsDF.show(false)\\nThe DataFrame API | 53', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 76}), Document(page_content='   // Print the schema\\n   println(blogsDF.printSchema )\\n   println(blogsDF.schema)\\n }\\n}\\nNot surprisingly, the output from the Scala program is no different than that from the\\nPython program:\\n+---+---------+-------+-----------------+---------+-----+----------------------+\\n|Id |First    |Last   |Url              |Published |Hits |Campaigns              |\\n+---+---------+-------+-----------------+---------+-----+----------------------+\\n|1  |Jules    |Damji  |https://tinyurl.1|1/4/2016 |4535 |[twitter, LinkedIn ]   |\\n|2  |Brooke   |Wenig  |https://tinyurl.2|5/5/2018 |8908 |[twitter, LinkedIn ]   |\\n|3  |Denny    |Lee    |https://tinyurl.3|6/7/2019 |7659 |[web, twitter,...]    |\\n|4  |Tathagata |Das    |https://tinyurl.4|5/12/2018|10568|[twitter, FB]         |\\n|5  |Matei    |Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB,...]|\\n|6  |Reynold  |Xin    |https://tinyurl.6|3/2/2015 |25568|[twitter, LinkedIn ]   |\\n+---+---------+-------+-----------------+---------+-----+----------------------+\\nroot\\n |-- Id: integer (nullable  = true)\\n |-- First: string (nullable  = true)\\n |-- Last: string (nullable  = true)\\n |-- Url: string (nullable  = true)\\n |-- Published : string (nullable  = true)\\n |-- Hits: integer (nullable  = true)\\n |-- Campaigns : array (nullable  = true)\\n |    |-- element: string (containsNull  = true)\\nStructType (StructField (\"Id\",IntegerType ,true), \\n    StructField (\"First\",StringType ,true), \\n    StructField (\"Last\",StringType ,true), \\n    StructField (\"Url\",StringType ,true),\\n    StructField (\"Published\" ,StringType ,true), \\n    StructField (\"Hits\",IntegerType ,true),\\n    StructField (\"Campaigns\" ,ArrayType (StringType ,true),true))\\nNow that you have an idea of how to use structured data and schemas in DataFrames,\\nlet’s focus on DataFrame columns and rows and what it means to operate on them\\nwith the DataFrame API.\\nColumns and Expressions\\nAs mentioned previously, named columns in DataFrames are conceptually similar to\\nnamed columns in pandas or R DataFrames or in an RDBMS table: they describe a\\ntype of field. Y ou can list all the columns by their names, and you can perform opera‐\\ntions on their values using relational or computational expressions. In Spark’s sup‐\\nported languages, columns are objects with public methods (represented by the\\nColumn  type).\\n54 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 77}), Document(page_content='Y ou can also use logical or mathematical expressions on columns. For example, you\\ncould create a simple expression using expr(\"columnName * 5\")  or (expr(\"colum\\nnName - 5\") > col(anothercolumnName)) , where columnName  is a Spark type (inte‐\\nger, string, etc.). expr()  is part of the pyspark.sql.functions  (Python) and\\norg.apache.spark.sql.functions  (Scala) packages. Like any other function in those\\npackages, expr()  takes arguments that Spark will parse as an expression, computing\\nthe result.\\nScala, Java, and Python all have public methods associated with col‐\\numns . Y ou’ll note that the Spark documentation refers to both col\\nand Column . Column  is the name of the object, while col()  is a stan‐\\ndard built-in function that returns a Column .\\nLet’s take a look at some examples of what we can do with columns in Spark. Each\\nexample is followed by its output:\\n// In Scala \\nscala> import org.apache.spark.sql.functions._\\nscala> blogsDF.columns\\nres2: Array[String] = Array(Campaigns , First, Hits, Id, Last, Published , Url)\\n// Access a particular column with col and it returns a Column type\\nscala> blogsDF.col(\"Id\")\\nres3: org.apache.spark.sql.Column  = id\\n// Use an expression to compute a value\\nscala> blogsDF.select(expr(\"Hits * 2\" )).show(2)\\n// or use col to compute value\\nscala> blogsDF.select(col(\"Hits\") * 2).show(2)\\n+----------+\\n|(Hits * 2)|\\n+----------+\\n|      9070|\\n|     17816|\\n+----------+\\n// Use an expression to compute big hitters for blogs\\n// This adds a new column, Big Hitters, based on the conditional expression\\nblogsDF.withColumn (\"Big Hitters\" , (expr(\"Hits > 10000\" ))).show()\\n+---+---------+-------+---+---------+-----+--------------------+-----------+\\n| Id|    First|   Last|Url|Published | Hits|           Campaigns |Big Hitters|\\n+---+---------+-------+---+---------+-----+--------------------+-----------+\\n|  1|    Jules|  Damji|...| 1/4/2016| 4535| [twitter, LinkedIn ]|      false|\\n|  2|   Brooke|  Wenig|...| 5/5/2018| 8908| [twitter, LinkedIn ]|      false|\\n|  3|    Denny|    Lee|...| 6/7/2019| 7659|[web, twitter, FB...|      false|\\n|  4|Tathagata|     Das|...| 5/12/2018|10568|       [twitter, FB]|       true|\\nThe DataFrame API | 55', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 78}), Document(page_content='|  5|    Matei|Zaharia|...| 5/14/2014|40578|[web, twitter, FB...|       true|\\n|  6|  Reynold|     Xin|...|  3/2/2015|25568| [twitter, LinkedIn ]|       true|\\n+---+---------+-------+---+---------+-----+--------------------+-----------+\\n// Concatenate  three columns, create a new column, and show the \\n// newly created concatenated  column\\nblogsDF\\n  .withColumn (\"AuthorsId \", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\"))))\\n  .select(col(\"AuthorsId \"))\\n  .show(4)\\n+-------------+\\n|    AuthorsId|\\n+-------------+\\n|  JulesDamji1|\\n| BrookeWenig2|\\n|    DennyLee3|\\n|TathagataDas4|\\n+-------------+\\n// These statements  return the same value, showing that \\n// expr is the same as a col method call\\nblogsDF.select (expr(\"Hits\")).show(2)\\nblogsDF.select (col(\"Hits\")).show(2)\\nblogsDF.select (\"Hits\").show(2)\\n+-----+\\n| Hits|\\n+-----+\\n| 4535|\\n| 8908|\\n+-----+\\n// Sort by column \"Id\" in descending  order\\nblogsDF.sort (col(\"Id\").desc).show()\\nblogsDF.sort ($\"Id\".desc).show()\\n+--------------------+---------+-----+---+-------+---------+-----------------+\\n|           Campaigns|     First| Hits| Id|   Last|Published|               Url|\\n+--------------------+---------+-----+---+-------+---------+-----------------+\\n| [twitter, LinkedIn ]|  Reynold| 25568|  6|    Xin| 3/2/2015|https://tinyurl. 6|\\n|[web, twitter, FB...|    Matei|40578|  5|Zaharia| 5/14/2014|https://tinyurl. 5|\\n|       [twitter, FB]|Tathagata| 10568|  4|    Das|5/12/2018|https://tinyurl. 4|\\n|[web, twitter, FB...|    Denny| 7659|  3|    Lee| 6/7/2019|https://tinyurl. 3|\\n| [twitter, LinkedIn ]|   Brooke| 8908|  2|  Wenig| 5/5/2018|https://tinyurl. 2|\\n| [twitter, LinkedIn ]|    Jules| 4535|  1|  Damji| 1/4/2016|https://tinyurl. 1|\\n+--------------------+---------+-----+---+-------+---------+-----------------+\\nIn this last example, the expressions blogs_df.sort(col(\"Id\").desc)  and\\nblogsDF.sort($\"Id\".desc)  are identical. They both sort the DataFrame column\\nnamed Id in descending order: one uses an explicit function, col(\"Id\") , to return a\\n56 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 79}), Document(page_content='Column  object, while the other uses $ before the name of the column, which is a func‐\\ntion in Spark that converts column named Id to a Column .\\nWe have only scratched the surface here, and employed just a cou‐\\nple of methods on Column  objects. For a complete list of all public\\nmethods for Column  objects, we refer you to the Spark documenta‐\\ntion.\\nColumn  objects in a DataFrame can’t exist in isolation; each column is part of a row in\\na record and all the rows together constitute a DataFrame, which as we will see later\\nin the chapter is really a Dataset[Row]  in Scala.\\nRows\\nA row in Spark is a generic Row object , containing one or more columns. Each col‐\\numn may be of the same data type (e.g., integer or string), or they can have different\\ntypes (integer, string, map, array, etc.). Because Row is an object in Spark and an\\nordered collection of fields, you can instantiate a Row in each of Spark’s supported lan‐\\nguages and access its fields by an index starting at 0:\\n// In Scala\\nimport org.apache.spark.sql.Row\\n// Create a Row\\nval blogRow = Row(6, \"Reynold\" , \"Xin\", \"https://tinyurl.6\" , 255568, \"3/2/2015\" , \\n  Array(\"twitter\" , \"LinkedIn\" ))\\n// Access using index for individual items\\nblogRow(1)\\nres62: Any = Reynold\\n# In Python\\nfrom pyspark.sql  import Row\\nblog_row  = Row(6, \"Reynold\" , \"Xin\", \"https://tinyurl.6\" , 255568, \"3/2/2015\" , \\n  [\"twitter\" , \"LinkedIn\" ])\\n# access using index for individual items\\nblog_row [1]\\n\\'Reynold\\'\\nRow objects can be used to create DataFrames if you need them for quick interactivity\\nand exploration:\\n# In Python    \\nrows = [Row(\"Matei Zaharia\" , \"CA\"), Row(\"Reynold Xin\" , \"CA\")]\\nauthors_df  = spark.createDataFrame (rows, [\"Authors\" , \"State\"])\\nauthors_df .show()\\n// In Scala\\nval rows = Seq((\"Matei Zaharia\" , \"CA\"), (\"Reynold Xin\" , \"CA\"))\\nval authorsDF  = rows.toDF(\"Author\" , \"State\") \\nauthorsDF .show()\\nThe DataFrame API | 57', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 80}), Document(page_content='1This public data is available at https://oreil.ly/iDzQK .\\n2The original data set has over 60 columns. We dropped a few unnecessary columns, removed records with\\nnull or invalid values, and added an extra Delay column.    \\n+-------------+-----+\\n|       Author|State|\\n+-------------+-----+\\n|Matei Zaharia|   CA|\\n|  Reynold Xin|   CA|\\n+-------------+-----+\\nIn practice, though, you will usually want to read DataFrames from a file as illustrated\\nearlier. In most cases, because your files are going to be huge, defining a schema and\\nusing it is a quicker and more efficient way to create DataFrames.\\nAfter you have created a large distributed DataFrame, you are going to want to per‐\\nform some common data operations on it. Let’s examine some of the Spark opera‐\\ntions you can perform with high-level relational operators in the Structured APIs.\\nCommon DataFrame Operations\\nTo perform common data operations on DataFrames, you’ll first need to load a Data‐\\nFrame from a data source that holds your structured data. Spark provides an inter‐\\nface, DataFrameReader , that enables you to read data into a DataFrame from myriad\\ndata sources in formats such as JSON, CSV , Parquet, Text, Avro, ORC, etc. Likewise,\\nto write a DataFrame back to a data source in a particular format, Spark uses\\nDataFrameWriter .\\nUsing DataFrameReader and DataFrameWriter\\nReading and writing are simple in Spark because of these high-level abstractions and\\ncontributions from the community to connect to a wide variety of data sources,\\nincluding common NoSQL stores, RDBMSs, streaming engines such as Apache Kafka\\nand Kinesis, and more.\\nTo get started, let’s read a large CSV file containing data on San Francisco Fire\\nDepartment calls.1 As noted previously, we will define a schema for this file and use\\nthe DataFrameReader  class and its methods to tell Spark what to do. Because this file\\ncontains 28 columns and over 4,380,660 records,2 it’s more efficient to define a\\nschema than have Spark infer it.\\n58 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 81}), Document(page_content='If you don’t want to specify the schema, Spark can infer schema\\nfrom a sample at a lesser cost. For example, you can use the\\nsamplingRatio  option:\\n// In Scala\\nval sampleDF  = spark\\n  .read\\n  .option(\"samplingRatio\" , 0.001)\\n  .option(\"header\" , true)\\n  .csv(\"\"\"/databricks-datasets/learning-spark-v2/\\n  sf-fire/sf-fire-calls.csv\"\"\" )\\nLet’s take a look at how to do this:\\n# In Python, define a schema \\nfrom pyspark.sql.types  import *\\n# Programmatic way to define a schema \\nfire_schema  = StructType ([StructField (\\'CallNumber\\' , IntegerType (), True),\\n                StructField (\\'UnitID\\' , StringType (), True),\\n                StructField (\\'IncidentNumber\\' , IntegerType (), True),\\n                StructField (\\'CallType\\' , StringType (), True),                  \\n                StructField (\\'CallDate\\' , StringType (), True),      \\n                StructField (\\'WatchDate\\' , StringType (), True),\\n                StructField (\\'CallFinalDisposition\\' , StringType (), True),\\n                StructField (\\'AvailableDtTm\\' , StringType (), True),\\n                StructField (\\'Address\\' , StringType (), True),       \\n                StructField (\\'City\\', StringType (), True),       \\n                StructField (\\'Zipcode\\' , IntegerType (), True),       \\n                StructField (\\'Battalion\\' , StringType (), True),                 \\n                StructField (\\'StationArea\\' , StringType (), True),       \\n                StructField (\\'Box\\', StringType (), True),       \\n                StructField (\\'OriginalPriority\\' , StringType (), True),       \\n                StructField (\\'Priority\\' , StringType (), True),       \\n                StructField (\\'FinalPriority\\' , IntegerType (), True),       \\n                StructField (\\'ALSUnit\\' , BooleanType (), True),       \\n                StructField (\\'CallTypeGroup\\' , StringType (), True),\\n                StructField (\\'NumAlarms\\' , IntegerType (), True),\\n                StructField (\\'UnitType\\' , StringType (), True),\\n                StructField (\\'UnitSequenceInCallDispatch\\' , IntegerType (), True),\\n                StructField (\\'FirePreventionDistrict\\' , StringType (), True),\\n                StructField (\\'SupervisorDistrict\\' , StringType (), True),\\n                StructField (\\'Neighborhood\\' , StringType (), True),\\n                StructField (\\'Location\\' , StringType (), True),\\n                StructField (\\'RowID\\', StringType (), True),\\n                StructField (\\'Delay\\', FloatType (), True)])\\n# Use the DataFrameReader interface to read a CSV file\\nsf_fire_file  = \"/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\"\\nfire_df = spark.read.csv(sf_fire_file , header=True, schema=fire_schema )\\n// In Scala it would be similar\\nval fireSchema  = StructType (Array(StructField (\"CallNumber\" , IntegerType , true),\\nThe DataFrame API | 59', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 82}), Document(page_content='                   StructField (\"UnitID\" , StringType , true),\\n                   StructField (\"IncidentNumber\" , IntegerType , true),\\n                   StructField (\"CallType\" , StringType , true), \\n                   StructField (\"Location\" , StringType , true),\\n                   ...\\n                   ...\\n                   StructField (\"Delay\", FloatType , true)))\\n// Read the file using the CSV DataFrameReader\\nval sfFireFile =\"/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv\"\\nval fireDF = spark.read.schema(fireSchema )\\n  .option(\"header\" , \"true\")\\n  .csv(sfFireFile )\\nThe spark.read.csv()  function reads in the CSV file and returns a DataFrame of\\nrows and named columns with the types dictated in the schema.\\nTo write the DataFrame into an external data source in your format of choice, you\\ncan use the DataFrameWriter  interface. Like DataFrameReader , it supports multiple\\ndata sources . Parquet, a popular columnar format, is the default format; it uses\\nsnappy compression to compress the data. If the DataFrame is written as Parquet, the\\nschema is preserved as part of the Parquet metadata. In this case, subsequent reads\\nback into a DataFrame do not require you to manually supply a schema.\\nSaving a DataFrame as a Parquet file or SQL table.    A common data operation is to explore\\nand transform your data, and then persist the DataFrame in Parquet format or save it\\nas a SQL table. Persisting a transformed DataFrame is as easy as reading it. For exam‐\\nple, to persist the DataFrame we were just working with as a file after reading it you\\nwould do the following:\\n// In Scala to save as a Parquet file\\nval parquetPath  = ...\\nfireDF.write.format(\"parquet\" ).save(parquetPath )\\n# In Python to save as a Parquet file\\nparquet_path  = ...\\nfire_df.write.format(\"parquet\" ).save(parquet_path )\\nAlternatively, you can save it as a table, which registers metadata with the Hive meta‐\\nstore (we will cover SQL managed and unmanaged tables, metastores, and Data‐\\nFrames in the next chapter):\\n// In Scala to save as a table \\nval parquetTable  = ... // name of the table\\nfireDF.write.format(\"parquet\" ).saveAsTable (parquetTable )\\n# In Python\\nparquet_table  = ... # name of the table\\nfire_df.write.format(\"parquet\" ).saveAsTable (parquet_table )\\n60 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 83}), Document(page_content='Let’s walk through some common operations to perform on DataFrames after you\\nhave read the data.\\nTransformations and actions\\nNow that you have a distributed DataFrame composed of San Francisco Fire Depart‐\\nment calls in memory, the first thing you as a developer will want to do is examine\\nyour data to see what the columns look like. Are they of the correct types? Do any of\\nthem need to be converted to different types? Do they have null  values?\\nIn “Transformations, Actions, and Lazy Evaluation”  on page 28 in Chapter 2 , you got\\na glimpse of how transformations and actions are used to operate on DataFrames,\\nand saw some common examples of each. What can we find out from our San Fran‐\\ncisco Fire Department calls using these?\\nProjections and filters .    A projection  in relational parlance is a way to return only the\\nrows matching a certain relational condition by using filters. In Spark, projections are\\ndone with the select()  method, while filters can be expressed using the filter()  or\\nwhere()  method. We can use this technique to examine specific aspects of our SF Fire\\nDepartment data set:\\n# In Python\\nfew_fire_df  = (fire_df\\n  .select(\"IncidentNumber\" , \"AvailableDtTm\" , \"CallType\" ) \\n  .where(col(\"CallType\" ) != \"Medical Incident\" ))\\nfew_fire_df .show(5, truncate =False)\\n// In Scala\\nval fewFireDF  = fireDF\\n  .select(\"IncidentNumber\" , \"AvailableDtTm\" , \"CallType\" )\\n  .where($\"CallType\"  =!= \"Medical Incident\" )    \\nfewFireDF .show(5, false)\\n+--------------+----------------------+--------------+\\n|IncidentNumber |AvailableDtTm          |CallType       |\\n+--------------+----------------------+--------------+\\n|2003235       |01/11/2002 01:47:00 AM|Structure  Fire|\\n|2003235       |01/11/2002 01:51:54 AM|Structure  Fire|\\n|2003235       |01/11/2002 01:47:00 AM|Structure  Fire|\\n|2003235       |01/11/2002 01:47:00 AM|Structure  Fire|\\n|2003235       |01/11/2002 01:51:17 AM|Structure  Fire|\\n+--------------+----------------------+--------------+\\nonly showing top 5 rows\\nWhat if we want to know how many distinct CallType s were recorded as the causes\\nof the fire calls? These simple and expressive queries do the job:\\n# In Python, return number of distinct types of calls using countDistinct()\\nfrom pyspark.sql.functions  import *\\n(fire_df\\nThe DataFrame API | 61', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 84}), Document(page_content='  .select(\"CallType\" )\\n  .where(col(\"CallType\" ).isNotNull ())\\n  .agg(countDistinct (\"CallType\" ).alias(\"DistinctCallTypes\" ))\\n  .show())\\n// In Scala\\nimport org.apache.spark.sql.functions._\\nfireDF\\n  .select(\"CallType\" )\\n  .where(col(\"CallType\" ).isNotNull )\\n  .agg(countDistinct (\\'CallType ) as \\'DistinctCallTypes )\\n  .show()\\n+-----------------+\\n|DistinctCallTypes |\\n+-----------------+\\n|               32|\\n+-----------------+\\nWe can list the distinct call types in the data set using these queries:\\n# In Python, filter for only distinct non-null CallTypes from all the rows\\n(fire_df\\n  .select(\"CallType\" )\\n  .where(col(\"CallType\" ).isNotNull ())\\n  .distinct ()\\n  .show(10, False))\\n// In Scala\\nfireDF\\n  .select(\"CallType\" )\\n  .where($\"CallType\" .isNotNull ())\\n  .distinct ()\\n  .show(10, false)\\nOut[20]: 32\\n+-----------------------------------+\\n|CallType                            |\\n+-----------------------------------+\\n|Elevator  / Escalator  Rescue        |\\n|Marine Fire                        |\\n|Aircraft  Emergency                  |\\n|Confined  Space / Structure  Collapse |\\n|Administrative                      |\\n|Alarms                             |\\n|Odor (Strange / Unknown)           |\\n|Lightning  Strike (Investigation )   |\\n|Citizen Assist / Service Call      |\\n|HazMat                             |\\n+-----------------------------------+\\nonly showing top 10 rows\\n62 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 85}), Document(page_content='Renaming, adding, and dropping columns.    Sometimes you want to rename particular\\ncolumns for reasons of style or convention, and at other times for readability or brev‐\\nity. The original column names in the SF Fire Department data set had spaces in\\nthem. For example, the column name IncidentNumber  was Incident Number . Spaces\\nin column names can be problematic, especially when you want to write or save a\\nDataFrame as a Parquet file (which prohibits this).\\nBy specifying the desired column names in the schema with StructField , as we did,\\nwe effectively changed all names in the resulting DataFrame.\\nAlternatively, you could selectively rename columns with the withColumnRenamed()\\nmethod. For instance, let’s change the name of our Delay  column to ResponseDe\\nlayedinMins  and take a look at the response times that were longer than five\\nminutes:\\n# In Python\\nnew_fire_df  = fire_df.withColumnRenamed (\"Delay\", \"ResponseDelayedinMins\" )\\n(new_fire_df\\n  .select(\"ResponseDelayedinMins\" )\\n  .where(col(\"ResponseDelayedinMins\" ) > 5)\\n  .show(5, False))\\n// In Scala\\nval newFireDF  = fireDF.withColumnRenamed (\"Delay\", \"ResponseDelayedinMins\" )\\nnewFireDF\\n  .select(\"ResponseDelayedinMins\" )\\n  .where($\"ResponseDelayedinMins\"  > 5)\\n  .show(5, false)\\nThis gives us a new renamed column:\\n+---------------------+\\n|ResponseDelayedinMins |\\n+---------------------+\\n|5.233333              |\\n|6.9333334             |\\n|6.116667              |\\n|7.85                 |\\n|77.333336             |\\n+---------------------+\\nonly showing top 5 rows\\nBecause DataFrame transformations are immutable, when we\\nrename a column using withColumnRenamed()  we get a new Data‐\\nFrame while retaining the original with the old column name.\\nModifying the contents of a column or its type are common operations during data\\nexploration. In some cases the data is raw or dirty, or its types are not amenable to\\nThe DataFrame API | 63', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 86}), Document(page_content='being supplied as arguments to relational operators. For example, in our SF Fire\\nDepartment data set, the columns CallDate , WatchDate , and AlarmDtTm  are strings\\nrather than either Unix timestamps or SQL dates, both of which Spark supports and\\ncan easily manipulate during transformations or actions (e.g., during a date- or time-\\nbased analysis of the data).\\nSo how do we convert them into a more usable format? It’s quite simple, thanks to\\nsome high-level API methods. spark.sql.functions  has a set of to/from date/time‐\\nstamp functions such as to_timestamp()  and to_date()  that we can use for just this\\npurpose:\\n# In Python\\nfire_ts_df  = (new_fire_df\\n  .withColumn (\"IncidentDate\" , to_timestamp (col(\"CallDate\" ), \"MM/dd/yyyy\" ))\\n  .drop(\"CallDate\" ) \\n  .withColumn (\"OnWatchDate\" , to_timestamp (col(\"WatchDate\" ), \"MM/dd/yyyy\" ))\\n  .drop(\"WatchDate\" ) \\n  .withColumn (\"AvailableDtTS\" , to_timestamp (col(\"AvailableDtTm\" ), \\n  \"MM/dd/yyyy hh:mm:ss a\" ))\\n  .drop(\"AvailableDtTm\" ))\\n# Select the converted columns\\n(fire_ts_df\\n  .select(\"IncidentDate\" , \"OnWatchDate\" , \"AvailableDtTS\" )\\n  .show(5, False))\\n// In Scala\\nval fireTsDF  = newFireDF\\n  .withColumn (\"IncidentDate\" , to_timestamp (col(\"CallDate\" ), \"MM/dd/yyyy\" ))\\n  .drop(\"CallDate\" )\\n  .withColumn (\"OnWatchDate\" , to_timestamp (col(\"WatchDate\" ), \"MM/dd/yyyy\" ))\\n  .drop(\"WatchDate\" ) \\n  .withColumn (\"AvailableDtTS\" , to_timestamp (col(\"AvailableDtTm\" ), \\n  \"MM/dd/yyyy hh:mm:ss a\" ))\\n  .drop(\"AvailableDtTm\" ) \\n// Select the converted columns\\nfireTsDF\\n  .select(\"IncidentDate\" , \"OnWatchDate\" , \"AvailableDtTS\" )\\n  .show(5, false)\\nThose queries pack quite a punch—a number of things are happening. Let’s unpack\\nwhat they do:\\n64 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 87}), Document(page_content='1.Convert the existing column’s data type from string to a Spark-supported\\ntimestamp.\\n2.Use the new format specified in the format string \"MM/dd/yyyy\"  or \"MM/dd/yyyy\\nhh:mm:ss a\"  where appropriate.\\n3.After converting to the new data type, drop()  the old column and append the\\nnew one specified in the first argument to the withColumn()  method.\\n4.Assign the new modified DataFrame to fire_ts_df .\\nThe queries result in three new columns:\\n+-------------------+-------------------+-------------------+\\n|IncidentDate       |OnWatchDate        |AvailableDtTS      |\\n+-------------------+-------------------+-------------------+\\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:58:43|\\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:10:17|\\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:47:00|\\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:54|\\n|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:47:00|\\n+-------------------+-------------------+-------------------+\\nonly showing top 5 rows\\nNow that we have modified the dates, we can query using functions from\\nspark.sql.functions  like month() , year() , and day()  to explore our data further.\\nWe could find out how many calls were logged in the last seven days, or we could see\\nhow many years’ worth of Fire Department calls are included in the data set with this\\nquery:\\n# In Python\\n(fire_ts_df\\n  .select(year(\\'IncidentDate\\' ))\\n  .distinct ()\\n  .orderBy(year(\\'IncidentDate\\' ))\\n  .show())\\n// In Scala\\nfireTsDF\\n  .select(year($\"IncidentDate\" ))\\n  .distinct ()\\n  .orderBy(year($\"IncidentDate\" ))\\n  .show()\\n+------------------+\\n|year(IncidentDate )|\\n+------------------+\\n|              2000|\\n|              2001|\\n|              2002|\\n|              2003|\\n|              2004|\\n|              2005|\\nThe DataFrame API | 65', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 88}), Document(page_content='|              2006|\\n|              2007|\\n|              2008|\\n|              2009|\\n|              2010|\\n|              2011|\\n|              2012|\\n|              2013|\\n|              2014|\\n|              2015|\\n|              2016|\\n|              2017|\\n|              2018|\\n+------------------+\\nSo far in this section, we have explored a number of common data operations: read‐\\ning and writing DataFrames; defining a schema and using it when reading in a Data‐\\nFrame; saving a DataFrame as a Parquet file or table; projecting and filtering selected\\ncolumns from an existing DataFrame; and modifying, renaming, and dropping\\ncolumns.\\nOne final common operation is grouping data by values in a column and aggregating\\nthe data in some way, like simply counting it. This pattern of grouping and counting\\nis as common as projecting and filtering. Let’s have a go at it.\\nAggregations.    What if we want to know what the most common types of fire calls\\nwere, or what zip codes accounted for the most calls? These kinds of questions are\\ncommon in data analysis and exploration.\\nA handful of transformations and actions on DataFrames, such as groupBy() ,\\norderBy() , and count() , offer the ability to aggregate by column names and then\\naggregate counts across them.\\nFor larger DataFrames on which you plan to conduct frequent or\\nrepeated queries, you could benefit from caching. We will cover\\nDataFrame caching strategies and their benefits in later chapters.\\nLet’s take our first question: what were the most common types of fire calls?\\n# In Python\\n(fire_ts_df\\n  .select(\"CallType\" )\\n  .where(col(\"CallType\" ).isNotNull ())\\n  .groupBy(\"CallType\" )\\n  .count()\\n  .orderBy(\"count\", ascending =False)\\n  .show(n=10, truncate =False))\\n66 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 89}), Document(page_content='// In Scala \\nfireTsDF\\n  .select(\"CallType\" )\\n  .where(col(\"CallType\" ).isNotNull )\\n  .groupBy(\"CallType\" )\\n  .count()\\n  .orderBy(desc(\"count\"))\\n  .show(10, false)\\n+-------------------------------+-------+\\n|CallType                        |count  |\\n+-------------------------------+-------+\\n|Medical Incident                |2843475|\\n|Structure  Fire                 |578998 |\\n|Alarms                         |483518 |\\n|Traffic Collision               |175507 |\\n|Citizen Assist / Service Call  |65360  |\\n|Other                          |56961  |\\n|Outside Fire                   |51603  |\\n|Vehicle Fire                   |20939  |\\n|Water Rescue                   |20037  |\\n|Gas Leak (Natural and LP Gases)|17284  |\\n+-------------------------------+-------+\\nFrom this output we can conclude that the most common call type is Medical\\nIncident.\\nThe DataFrame API also offers the collect()  method, but for\\nextremely large DataFrames this is resource-heavy (expensive) and\\ndangerous, as it can cause out-of-memory (OOM) exceptions.\\nUnlike count() , which returns a single number to the driver, col\\nlect()  returns a collection of all the Row objects in the entire Data‐\\nFrame or Dataset. If you want to take a peek at some Row records\\nyou’re better off with take(n), which will return only the first n\\nRow objects of the DataFrame.\\nOther common DataFrame operations.    Along with all the others we’ve seen, the Data‐\\nFrame API provides descriptive statistical methods like min() , max() , sum() , and\\navg() . Let’s take a look at some examples showing how to compute them with our SF\\nFire Department data set.\\nHere we compute the sum of alarms, the average response time, and the minimum\\nand maximum response times to all fire calls in our data set, importing the PySpark\\nfunctions in a Pythonic way so as not to conflict with the built-in Python functions:\\n# In Python\\nimport pyspark.sql.functions  as F\\n(fire_ts_df\\n  .select(F.sum(\"NumAlarms\" ), F.avg(\"ResponseDelayedinMins\" ),\\nThe DataFrame API | 67', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 90}), Document(page_content='    F.min(\"ResponseDelayedinMins\" ), F.max(\"ResponseDelayedinMins\" ))\\n  .show())\\n// In Scala\\nimport org.apache.spark.sql. {functions  => F}\\nfireTsDF\\n  .select(F.sum(\"NumAlarms\" ), F.avg(\"ResponseDelayedinMins\" ), \\n  F.min(\"ResponseDelayedinMins\" ), F.max(\"ResponseDelayedinMins\" ))\\n  .show()\\n+--------------+--------------------------+--------------------------+---------+\\n|sum(NumAlarms )|avg(ResponseDelayedinMins )|min(ResponseDelayedinMins )|max(...) |\\n+--------------+--------------------------+--------------------------+---------+\\n|       4403441|         3.902170335891614 |               0.016666668 |1879.6167 |\\n+--------------+--------------------------+--------------------------+---------+\\nFor more advanced statistical needs common with data science workloads, read the\\nAPI documentation for methods like stat() , describe() , correlation() ,\\ncovariance() , sampleBy() , approxQuantile() , frequentItems() , and so on.\\nAs you can see, it’s easy to compose and chain expressive queries with DataFrames’\\nhigh-level API and DSL operators. We can’t imagine the opacity and comparative\\nunreadability of the code if we were to try to do the same with RDDs!\\nEnd-to-End DataFrame Example\\nThere are many possibilities for exploratory data analysis, ETL, and common data\\noperations on the San Francisco Fire Department public data set, above and beyond\\nwhat we’ve shown here.\\nFor brevity we won’t include all the example code here, but the book’s GitHub repo\\nprovides Python and Scala notebooks for you to try to complete an end-to-end Data‐\\nFrame example using this data set. The notebooks explore and answer the following\\ncommon questions that you might ask, using the DataFrame API and DSL relational\\noperators:\\n•What were all the different types of fire calls in 2018?\\n•What months within the year 2018 saw the highest number of fire calls?\\n•Which neighborhood in San Francisco generated the most fire calls in 2018?\\n•Which neighborhoods had the worst response times to fire calls in 2018?\\n•Which week in the year in 2018 had the most fire calls?\\n•Is there a correlation between neighborhood, zip code, and number of fire calls?\\n•How can we use Parquet files or SQL tables to store this data and read it back?\\n68 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 91}), Document(page_content='So far we have extensively discussed the DataFrame API, one of the Structured APIs\\nthat span Spark’s MLlib and Structured Streaming components, which we cover later\\nin the book.\\nNext, we’ll shift our focus to the Dataset API and explore how the two APIs provide a\\nunified, structured interface to developers for programming Spark. We’ll then exam‐\\nine the relationship between the RDD, DataFrame, and Dataset APIs, and help you\\ndetermine when to use which API and why.\\nThe Dataset API\\nAs stated earlier in this chapter, Spark 2.0 unified  the DataFrame and Dataset APIs as\\nStructured APIs with similar interfaces so that developers would only have to learn a\\nsingle set of APIs. Datasets take on two characteristics: typed  and untyped  APIs , as\\nshown in Figure 3-1 .\\nFigure 3-1. Structured APIs in Apache Spark\\nConceptually, you can think of a DataFrame in Scala as an alias for a collection of\\ngeneric objects, Dataset[Row] , where a Row is a generic untyped JVM object that may\\nhold different types of fields. A Dataset, by contrast, is a collection of strongly typed\\nJVM objects in Scala or a class in Java. Or, as the Dataset documentation  puts it, a\\nDataset is:\\na strongly typed collection of domain-specific objects that can be transformed in paral‐\\nlel using functional or relational operations. Each Dataset [in Scala] also has an unty‐\\nped view called a DataFrame, which is a Dataset of Row.\\nTyped Objects, Untyped Objects, and Generic Rows\\nIn Spark’s supported languages, Datasets make sense only in Java and Scala, whereas\\nin Python and R only DataFrames make sense. This is because Python and R are not\\ncompile-time type-safe; types are dynamically inferred or assigned during execution,\\nnot during compile time. The reverse is true in Scala and Java: types are bound to\\nThe Dataset API | 69', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 92}), Document(page_content='variables and objects at compile time. In Scala, however, a DataFrame is just an alias\\nfor untyped Dataset[Row] . Table 3-6  distills it in a nutshell.\\nTable 3-6. Typed and untyped objects in Spark\\nLanguage Typed and untyped main abstraction Typed or untyped\\nScala Dataset[T]  and DataFrame (alias for Dataset[Row] )Both typed and untyped\\nJava Dataset<T> Typed\\nPython DataFrame Generic Row  untyped\\nR DataFrame Generic Row  untyped\\nRow is a generic object type in Spark, holding a collection of mixed types that can be\\naccessed using an index. Internally, Spark manipulates Row objects, converting them\\nto the equivalent types covered in Table 3-2  and Table 3-3 . For example, an Int as one\\nof your fields in a Row will be mapped or converted to IntegerType  or IntegerType()\\nrespectively for Scala or Java and Python:\\n// In Scala\\nimport org.apache.spark.sql.Row  \\nval row = Row(350, true, \"Learning Spark 2E\" , null)\\n# In Python\\nfrom pyspark.sql  import Row\\nrow = Row(350, True, \"Learning Spark 2E\" , None)\\nUsing an index into the Row object, you can access individual fields with its public\\ngetter  methods:\\n// In Scala\\nrow.getInt(0)\\nres23: Int = 350\\nrow.getBoolean (1)\\nres24: Boolean = true\\nrow.getString (2)\\nres25: String = Learning  Spark 2E\\n# In Python\\nrow[0]\\nOut[13]: 350\\nrow[1]\\nOut[14]: True\\nrow[2]\\nOut[15]: \\'Learning Spark 2E\\'\\nBy contrast, typed objects are actual Java or Scala class objects in the JVM. Each ele‐\\nment in a Dataset maps to a JVM object.\\n70 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 93}), Document(page_content='Creating Datasets\\nAs with creating DataFrames from data sources, when creating a Dataset you have to\\nknow the schema. In other words, you need to know the data types. Although with\\nJSON and CSV data it’s possible to infer the schema, for large data sets this is\\nresource-intensive (expensive). When creating a Dataset in Scala, the easiest way to\\nspecify the schema for the resulting Dataset is to use a case class. In Java, JavaBean\\nclasses are used (we further discuss JavaBean and Scala case class in Chapter 6 ).\\nScala: Case classes\\nWhen you wish to instantiate your own domain-specific object as a Dataset, you can\\ndo so by defining a case class in Scala. As an example, let’s look at a collection of read‐\\nings from Internet of Things (IoT) devices in a JSON file (we use this file in the end-\\nto-end example later in this section).\\nOur file has rows of JSON strings that look as follows:\\n{\"device_id\": 198164, \"device_name\": \"sensor-pad-198164owomcJZ\", \"ip\": \\n\"80.55.20.25\", \"cca2\": \"PL\", \"cca3\": \"POL\", \"cn\": \"Poland\", \"latitude\":\\n53.080000, \"longitude\": 18.620000, \"scale\": \"Celsius\", \"temp\": 21, \\n\"humidity\": 65, \"battery_level\": 8, \"c02_level\": 1408,\"lcd\": \"red\", \\n\"timestamp\" :1458081226051}\\nTo express each JSON entry as DeviceIoTData , a domain-specific object, we can\\ndefine a Scala case class:\\ncase class DeviceIoTData  (battery_level : Long, c02_level : Long, \\ncca2: String, cca3: String, cn: String, device_id : Long, \\ndevice_name : String, humidity : Long, ip: String, latitude : Double,\\nlcd: String, longitude : Double, scale:String, temp: Long, \\ntimestamp : Long)\\nOnce defined, we can use it to read our file and convert the returned Dataset[Row]\\ninto Dataset[DeviceIoTData]  (output truncated to fit on the page):\\n// In Scala\\nval ds = spark.read\\n .json(\"/databricks-datasets/learning-spark-v2/iot-devices/iot_devices.json\" )\\n .as[DeviceIoTData ]\\nds: org.apache.spark.sql.Dataset [DeviceIoTData ] = [battery_level... ]\\nds.show(5, false)\\n+-------------|---------|----|----|-------------|---------|---+\\n|battery_level |c02_level |cca2|cca3|cn           |device_id |...|\\n+-------------|---------|----|----|-------------|---------|---+\\n|8            |868      |US  |USA |United States|1        |...|\\n|7            |1473     |NO  |NOR |Norway       |2        |...|\\n|2            |1556     |IT  |ITA |Italy        |3        |...|\\nThe Dataset API | 71', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 94}), Document(page_content='|6            |1080     |US  |USA |United States|4        |...|\\n|4            |931      |PH  |PHL |Philippines   |5        |...|\\n+-------------|---------|----|----|-------------|---------|---+\\nonly showing top 5 rows\\nDataset Operations\\nJust as you can perform transformations and actions on DataFrames, so you can with\\nDatasets. Depending on the kind of operation, the results will vary:\\n// In Scala\\nval filterTempDS  = ds.filter({d => {d.temp > 30 && d.humidity  > 70})\\nfilterTempDS : org.apache.spark.sql.Dataset [DeviceIoTData ] = [battery_level... ]\\nfilterTempDS .show(5, false)\\n+-------------|---------|----|----|-------------|---------|---+\\n|battery_level |c02_level |cca2|cca3|cn           |device_id |...|\\n+-------------|---------|----|----|-------------|---------|---+\\n|0            |1466     |US  |USA |United States|17       |...|\\n|9            |986      |FR  |FRA |France       |48       |...|\\n|8            |1436     |US  |USA |United States|54       |...|\\n|4            |1090     |US  |USA |United States|63       |...|\\n|4            |1072     |PH  |PHL |Philippines   |81       |...|\\n+-------------|---------|----|----|-------------|---------|---+\\nonly showing top 5 rows\\nIn this query, we used a function as an argument to the Dataset method filter() .\\nThis is an overloaded method with many signatures. The version we used, fil\\nter(func: (T) > Boolean): Dataset[T] , takes a lambda function, func: (T) >\\nBoolean , as its argument.\\nThe argument to the lambda function is a JVM object of type DeviceIoTData . As\\nsuch, we can access its individual data fields using the dot ( .) notation, like you\\nwould in a Scala class or JavaBean.\\nAnother thing to note is that with DataFrames, you express your filter()  condi‐\\ntions as SQL-like DSL operations, which are language-agnostic (as we saw earlier in\\nthe fire calls examples). With Datasets, we use language-native expressions as Scala or\\nJava code.\\nHere’s another example that results in another, smaller Dataset:\\n// In Scala\\ncase class DeviceTempByCountry (temp: Long, device_name : String, device_id : Long, \\n  cca3: String)\\nval dsTemp = ds\\n  .filter(d => {d.temp > 25})\\n  .map(d => (d.temp, d.device_name , d.device_id , d.cca3))\\n  .toDF(\"temp\", \"device_name\" , \"device_id\" , \"cca3\")\\n72 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 95}), Document(page_content='  .as[DeviceTempByCountry ]\\ndsTemp.show(5, false)\\n+----+---------------------+---------+----+\\n|temp|device_name           |device_id |cca3|\\n+----+---------------------+---------+----+\\n|34  |meter-gauge-1xbYRYcj |1        |USA |\\n|28  |sensor-pad-4mzWkz    |4        |USA |\\n|27  |sensor-pad-6al7RTAobR |6        |USA |\\n|27  |sensor-pad-8xUD6pzsQI |8        |JPN |\\n|26  |sensor-pad-10BsywSYUF |10       |USA |\\n+----+---------------------+---------+----+\\nonly showing top 5 rows\\nOr you can inspect only the first row of your Dataset:\\nval device = dsTemp.first()\\nprintln(device)\\ndevice: DeviceTempByCountry  =\\nDeviceTempByCountry (34,meter-gauge-1xbYRYcj,1,USA)\\nAlternatively, you could express the same query using column names and then cast to\\na Dataset[DeviceTempByCountry] :\\n// In Scala\\nval dsTemp2 = ds\\n  .select($\"temp\", $\"device_name\" , $\"device_id\" , $\"device_id\" , $\"cca3\")\\n  .where(\"temp > 25\" )\\n  .as[DeviceTempByCountry ]\\nSemantically, select()  is like map()  in the previous query, in that\\nboth of these queries select fields and generate equivalent results.\\nTo recap, the operations we can perform on Datasets— filter() , map() , groupBy() ,\\nselect() , take() , etc.—are similar to the ones on DataFrames. In a way, Datasets are\\nsimilar to RDDs in that they provide a similar interface to its aforementioned meth‐\\nods and compile-time safety but with a much easier to read and an object-oriented\\nprogramming interface.\\nWhen we use Datasets, the underlying Spark SQL engine handles the creation, con‐\\nversion, serialization, and deserialization of the JVM objects. It also takes care of off-\\nJava heap memory management with the help of Dataset encoders. (We will talk more\\nabout Datasets and memory management in Chapter 6 .)\\nThe Dataset API | 73', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 96}), Document(page_content='End-to-End Dataset Example\\nIn this end-to-end Dataset example you’ll conduct similar exploratory data analysis,\\nETL (extract, transform, and load), and data operations as in the DataFrame example,\\nusing the IoT data set. This data set is small and fake, but our main goal here is to\\nillustrate the clarity with which you can express queries with Datasets and the read‐\\nability of those queries, just as we did with DataFrames.\\nAgain, for brevity, we won’t include all the example code here; however, we have fur‐\\nnished the notebook in the GitHub repo . The notebook explores common operations\\nyou might conduct with this data set. Using the Dataset API, we attempt to do the\\nfollowing:\\n1.Detect failing devices with battery levels below a threshold.\\n2.Identify offending countries with high levels of CO2 emissions.\\n3.Compute the min and max values for temperature, battery level, CO2, and\\nhumidity.\\n4.Sort and group by average temperature, CO2, humidity, and country.\\nDataFrames Versus Datasets\\nBy now you may be wondering why and when you should use DataFrames or Data‐\\nsets. In many cases either will do, depending on the languages you are working in, but\\nthere are some situations where one is preferable to the other. Here are a few\\nexamples:\\n•If you want to tell Spark what to do , not how to do it , use DataFrames or Datasets.\\n•If you want rich semantics, high-level abstractions, and DSL operators, use Data‐\\nFrames or Datasets.\\n•If you want strict compile-time type safety and don’t mind creating multiple case\\nclasses for a specific Dataset[T] , use Datasets.\\n•If your processing demands high-level expressions, filters, maps, aggregations,\\ncomputing averages or sums, SQL queries, columnar access, or use of relational\\noperators on semi-structured data, use DataFrames or Datasets.\\n•If your processing dictates relational transformations similar to SQL-like queries,\\nuse DataFrames.\\n•If you want to take advantage of and benefit from Tungsten’s efficient serializa‐\\ntion with Encoders, , use Datasets .\\n•If you want unification, code optimization, and simplification of APIs across\\nSpark components, use DataFrames.\\n74 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 97}), Document(page_content='•If you are an R user, use DataFrames.\\n•If you are a Python user, use DataFrames and drop down to RDDs if you need\\nmore control.\\n•If you want space and speed efficiency, use DataFrames.\\n•If you want errors caught during compilation rather than at runtime, choose the\\nappropriate API as depicted in Figure 3-2 .\\nFigure 3-2. When errors are detected using the Structured APIs\\nWhen to Use RDDs\\nY ou may ask: Are RDDs being relegated to second-class citizens? Are they being dep‐\\nrecated? The answer is a resounding no! The RDD API will continue to be supported,\\nalthough all future development work in Spark 2.x and Spark 3.0 will continue to\\nhave a DataFrame interface and semantics rather than using RDDs.\\nThere are some scenarios where you’ll want to consider using RDDs, such as when\\nyou:\\n•Are using a third-party package that’s written using RDDs\\n•Can forgo the code optimization, efficient space utilization, and performance\\nbenefits available with DataFrames and Datasets\\n•Want to precisely instruct Spark how to do  a query\\nWhat’s more, you can seamlessly move between DataFrames or Datasets and RDDs at\\nwill using a simple API method call, df.rdd . (Note, however, that this does have a\\ncost and should be avoided unless necessary.) After all, DataFrames and Datasets are\\nbuilt on top of RDDs, and they get decomposed to compact RDD code during whole-\\nstage code generation, which we discuss in the next section.\\nFinally, the preceding sections provided some intuition on how Structured APIs in\\nSpark enable developers to use easy and friendly APIs to compose expressive queries\\non structured data. In other words, you tell Spark what to do , not how to do it , using\\nDataFrames Versus Datasets | 75', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 98}), Document(page_content='high-level operations, and it ascertains the most efficient way to build a query and\\ngenerates compact code for you.\\nThis process of building efficient queries and generating compact code is the job of\\nthe Spark SQL engine. It’s the substrate upon which the Structured APIs we’ve been\\nlooking at are built. Let’s peek under the hood at that engine now.\\nSpark SQL and the Underlying Engine\\nAt a programmatic level, Spark SQL allows developers to issue ANSI SQL:2003–com‐\\npatible queries on structured data with a schema. Since its introduction in Spark 1.3,\\nSpark SQL has evolved into a substantial engine upon which many high-level struc‐\\ntured functionalities have been built. Apart from allowing you to issue SQL-like quer‐\\nies on your data, the Spark SQL engine:\\n•Unifies Spark components and permits abstraction to DataFrames/Datasets in\\nJava, Scala, Python, and R, which simplifies working with structured data sets.\\n•Connects to the Apache Hive metastore and tables.\\n•Reads and writes structured data with a specific schema from structured file for‐\\nmats (JSON, CSV , Text, Avro, Parquet, ORC, etc.) and converts data into tempo‐\\nrary tables.\\n•Offers an interactive Spark SQL shell for quick data exploration.\\n•Provides a bridge to (and from) external tools via standard database JDBC/\\nODBC connectors.\\n•Generates optimized query plans and compact code for the JVM, for final\\nexecution.\\nFigure 3-3  shows the components that Spark SQL interacts with to achieve all of this.\\n76 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 99}), Document(page_content='Figure 3-3. Spark SQL and its stack\\nAt the core of the Spark SQL engine are the Catalyst optimizer and Project Tungsten.\\nTogether, these support the high-level DataFrame and Dataset APIs and SQL queries.\\nWe’ll talk more about Tungsten in Chapter 6 ; for now, let’s take a closer look at the\\noptimizer.\\nThe Catalyst Optimizer\\nThe Catalyst optimizer takes a computational query and converts it into an execution\\nplan. It goes through four transformational phases , as shown in Figure 3-4 :\\n1.Analysis\\n2.Logical optimization\\n3.Physical planning\\n4.Code generation\\nSpark SQL and the Underlying Engine | 77', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 100}), Document(page_content='Figure 3-4. A Spark computation’s four-phase journey\\nFor example, consider one of the queries from our M&Ms example in Chapter 2 .\\nBoth of the following sample code blocks will go through the same process, eventu‐\\nally ending up with a similar query plan and identical bytecode for execution. That is,\\nregardless of the language you use, your computation undergoes the same journey\\nand the resulting bytecode is likely the same:\\n# In Python\\ncount_mnm_df  = (mnm_df\\n  .select(\"State\", \"Color\", \"Count\") \\n  .groupBy(\"State\", \"Color\") \\n  .agg(count(\"Count\") \\n  .alias(\"Total\")) \\n  .orderBy(\"Total\", ascending =False))\\n78 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 101}), Document(page_content=\"-- In SQL\\nSELECT State, Color, Count, sum(Count) AS Total\\nFROM MNM_TABLE_NAME\\nGROUP BY State, Color, Count\\nORDER BY Total DESC\\nTo see the different stages the Python code goes through , you can use the\\ncount_mnm_df.explain(True)  method on the DataFrame. Or, to get a look at the dif‐\\nferent logical and physical plans, in Scala you can call df.queryExecution.logical\\nor df.queryExecution.optimizedPlan . (In Chapter 7 , we will discuss more about\\ntuning and debugging Spark and how to read query plans.) This gives us the follow‐\\ning output:\\ncount_mnm_df .explain(True)\\n== Parsed Logical Plan ==\\n'Sort ['Total DESC NULLS LAST], true\\n+- Aggregate  [State#10, Color#11], [State#10, Color#11, count(Count#12) AS...]\\n   +- Project [State#10, Color#11, Count#12]\\n      +- Relation [State#10,Color#11,Count#12] csv\\n== Analyzed  Logical Plan ==\\nState: string, Color: string, Total: bigint\\nSort [Total#24L DESC NULLS LAST], true\\n+- Aggregate  [State#10, Color#11], [State#10, Color#11, count(Count#12) AS...]\\n   +- Project [State#10, Color#11, Count#12]\\n      +- Relation [State#10,Color#11,Count#12] csv\\n== Optimized  Logical Plan ==\\nSort [Total#24L DESC NULLS LAST], true\\n+- Aggregate  [State#10, Color#11], [State#10, Color#11, count(Count#12) AS...]\\n   +- Relation [State#10,Color#11,Count#12] csv\\n== Physical  Plan ==\\n*(3) Sort [Total#24L DESC NULLS LAST], true, 0\\n+- Exchange  rangepartitioning (Total#24L DESC NULLS LAST, 200)\\n   +- *(2) HashAggregate (keys=[State#10, Color#11], functions =[count(Count#12)],\\noutput=[State#10, Color#11, Total#24L])\\n      +- Exchange  hashpartitioning (State#10, Color#11, 200)\\n         +- *(1) HashAggregate (keys=[State#10, Color#11],\\nfunctions =[partial_count (Count#12)], output=[State#10, Color#11, count#29L])\\n            +- *(1) FileScan  csv [State#10,Color#11,Count#12] Batched: false,\\nFormat: CSV, Location :\\nInMemoryFileIndex [file:/Users/jules/gits/LearningSpark2. 0/chapter2/py/src/...\\ndataset.csv ], PartitionFilters : [], PushedFilters : [], ReadSchema :\\nstruct<State:string ,Color:string,Count:int>\\nLet’s consider another DataFrame computation example. The following Scala code\\nundergoes a similar journey as the underlying engine optimizes its logical and physi‐\\ncal plans:\\nSpark SQL and the Underlying Engine | 79\", metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 102}), Document(page_content='// In Scala\\n// Users DataFrame read from a Parquet table\\nval usersDF  = ...\\n// Events DataFrame read from a Parquet table\\nval eventsDF  = ...\\n// Join two DataFrames\\nval joinedDF  = users\\n  .join(events, users(\"id\") === events(\"uid\"))\\n  .filter(events(\"date\") > \"2015-01-01\" )\\nAfter going through an initial analysis phase, the query plan is transformed and rear‐\\nranged by the Catalyst optimizer as shown in Figure 3-5 .\\nFigure 3-5. An example of a specific  query transformation\\n80 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 103}), Document(page_content='Let’s go through each of the four query optimization phases..\\nPhase 1: Analysis\\nThe Spark SQL engine begins by generating an abstract syntax tree (AST)  for the SQL\\nor DataFrame query. In this initial phase, any columns or table names will be resolved\\nby consulting an internal Catalog , a programmatic interface to Spark SQL that holds\\na list of names of columns, data types, functions, tables, databases, etc. Once they’ve\\nall been successfully resolved, the query proceeds to the next phase.\\nPhase 2: Logical optimization\\nAs Figure 3-4  shows, this phase comprises two internal stages. Applying a standard-\\nrule based optimization approach, the Catalyst optimizer will first construct a set of\\nmultiple plans and then, using its cost-based optimizer (CBO) , assign costs to each\\nplan. These plans are laid out as operator trees (like in Figure 3-5 ); they may include,\\nfor example, the process of constant folding, predicate pushdown, projection prun‐\\ning, Boolean expression simplification, etc. This logical plan is the input into the\\nphysical plan.\\nPhase 3: Physical planning\\nIn this phase, Spark SQL generates an optimal physical plan for the selected logical\\nplan, using physical operators that match those available in the Spark execution\\nengine.\\nPhase 4: Code generation\\nThe final phase of query optimization involves generating efficient Java bytecode to\\nrun on each machine. Because Spark SQL can operate on data sets loaded in memory,\\nSpark can use state-of-the-art compiler technology for code generation to speed up\\nexecution. In other words, it acts as a compiler . Project Tungsten, which facilitates\\nwhole-stage code generation, plays a role here.\\nJust what is whole-stage code generation? It’s a physical query optimization phase that\\ncollapses the whole query into a single function, getting rid of virtual function calls\\nand employing CPU registers for intermediate data. The second-generation Tungsten\\nengine, introduced in Spark 2.0, uses this approach to generate compact RDD code\\nfor final execution. This streamlined strategy significantly improves CPU efficiency\\nand performance .\\nSpark SQL and the Underlying Engine | 81', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 104}), Document(page_content='We have talked at a conceptual level about the workings of the\\nSpark SQL engine, with its two principal components: the Catalyst\\noptimizer and Project Tungsten. The internal technical workings\\nare beyond the scope of this book; however, for the curious, we\\nencourage you to check out the references in the text for in-depth\\ntechnical discussions.\\nSummary\\nIn this chapter, we took a deep dive into Spark’s Structured APIs, beginning with a\\nlook at the history and merits of structure in Spark.\\nThrough illustrative common data operations and code examples, we demonstrated\\nthat the high-level DataFrame and Dataset APIs are far more expressive and intuitive\\nthan the low-level RDD API. Designed to make processing of large data sets easier,\\nthe Structured APIs provide domain-specific operators for common data operations,\\nincreasing the clarity and expressiveness of your code.\\nWe explored when to use RDDs, DataFrames, and Datasets, depending on your use\\ncase scenarios.\\nAnd finally, we took a look under the hood to see how the Spark SQL engine’s main\\ncomponents—the Catalyst optimizer and Project Tungsten—support structured high-\\nlevel APIs and DSL operators. As you saw, no matter which of the Spark-supported\\nlanguages you use, a Spark query undergoes the same optimization journey, from log‐\\nical and physical plan construction to final compact code generation.\\nThe concepts and code examples in this chapter have laid the groundwork for the\\nnext two chapters, in which we will further illustrate the seamless interoperability\\nbetween DataFrames, Datasets, and Spark SQL.\\n82 | Chapter 3: Apache Spark’s Structured APIs', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 105}), Document(page_content='CHAPTER 4\\nSpark SQL and DataFrames:\\nIntroduction to Built-in Data Sources\\nIn the previous chapter, we explained the evolution of and justification for structure\\nin Spark. In particular, we discussed how the Spark SQL engine provides a unified\\nfoundation for the high-level DataFrame and Dataset APIs. Now, we’ll continue our\\ndiscussion of the DataFrame and explore its interoperability with Spark SQL.\\nThis chapter and the next also explore how Spark SQL interfaces with some of the\\nexternal components shown in Figure 4-1 .\\nIn particular, Spark SQL:\\n•Provides the engine upon which the high-level Structured APIs we explored in\\nChapter 3  are built.\\n•Can read and write data in a variety of structured formats (e.g., JSON, Hive\\ntables, Parquet, Avro, ORC, CSV).\\n•Lets you query data using JDBC/ODBC connectors from external business intel‐\\nligence (BI) data sources such as Tableau, Power BI, Talend, or from RDBMSs\\nsuch as MySQL and PostgreSQL.\\n•Provides a programmatic interface to interact with structured data stored as\\ntables or views in a database from a Spark application\\n•Offers an interactive shell to issue SQL queries on your structured data.\\n•Supports ANSI SQL:2003 -compliant commands and HiveQL .\\n83', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 106}), Document(page_content='Figure 4-1. Spark SQL connectors and data sources\\nLet’s begin with how you can use Spark SQL in a Spark application.\\nUsing Spark SQL in Spark Applications\\nThe SparkSession , introduced in Spark 2.0, provides a unified entry point  for pro‐\\ngramming Spark with the Structured APIs. Y ou can use a SparkSession  to access\\nSpark functionality: just import the class and create an instance in your code.\\nTo issue any SQL query, use the sql()  method on the SparkSession  instance, spark ,\\nsuch as spark.sql(\"SELECT * FROM myTableName\") . All spark.sql  queries executed\\nin this manner return a DataFrame on which you may perform further Spark opera‐\\ntions if you desire—the kind we explored in Chapter 3  and the ones you will learn\\nabout in this chapter and the next.\\n84 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 107}), Document(page_content='Basic Query Examples\\nIn this section we’ll walk through a few examples of queries on the Airline On-Time\\nPerformance and Causes of Flight Delays data set , which contains data on US flights\\nincluding date, delay, distance, origin, and destination. It’s available as a CSV file with\\nover a million records. Using a schema, we’ll read the data into a DataFrame and reg‐\\nister the DataFrame as a temporary view (more on temporary views shortly) so we\\ncan query it with SQL.\\nQuery examples are provided in code snippets, and Python and Scala notebooks\\ncontaining all of the code presented here are available in the book’s GitHub repo .\\nThese examples will offer you a taste of how to use SQL in your Spark applications via\\nthe spark.sql  programmatic interface . Similar to the DataFrame API in its declara‐\\ntive flavor, this interface allows you to query structured data in your Spark\\napplications.\\nNormally, in a standalone Spark application, you will create a SparkSession  instance\\nmanually, as shown in the following example. However, in a Spark shell (or Data‐\\nbricks notebook), the SparkSession  is created for you and accessible via the appro‐\\npriately named variable spark .\\nLet’s get started by reading the data set into a temporary view:\\n// In Scala\\nimport org.apache.spark.sql.SparkSession             \\nval spark = SparkSession\\n  .builder\\n  .appName(\"SparkSQLExampleApp\" )\\n  .getOrCreate ()\\n// Path to data set \\nval csvFile=\"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\\n// Read and create a temporary view\\n// Infer schema (note that for larger files you may want to specify the schema)\\nval df = spark.read.format(\"csv\")\\n  .option(\"inferSchema\" , \"true\")\\n  .option(\"header\" , \"true\")\\n  .load(csvFile)\\n// Create a temporary view\\ndf.createOrReplaceTempView (\"us_delay_flights_tbl\" )\\n# In Python\\nfrom pyspark.sql  import SparkSession         \\n# Create a SparkSession\\nspark = (SparkSession\\n  .builder\\n  .appName(\"SparkSQLExampleApp\" )\\n  .getOrCreate ())\\nUsing Spark SQL in Spark Applications | 85', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 108}), Document(page_content='# Path to data set\\ncsv_file  = \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\\n# Read and create a temporary view\\n# Infer schema (note that for larger files you \\n# may want to specify the schema)\\ndf = (spark.read.format(\"csv\")\\n  .option(\"inferSchema\" , \"true\")\\n  .option(\"header\" , \"true\")\\n  .load(csv_file ))\\ndf.createOrReplaceTempView (\"us_delay_flights_tbl\" )\\nIf you want to specify a schema, you can use a DDL-formatted\\nstring. For example:\\n// In Scala\\nval schema = \"date STRING, delay INT, distance INT, \\n origin STRING, destination STRING\"\\n# In Python\\nschema = \"`date` STRING, `delay` INT, `distance` INT, \\n `origin`  STRING, `destination`  STRING\"\\nNow that we have a temporary view, we can issue SQL queries using Spark SQL.\\nThese queries are no different from those you might issue against a SQL table in, say,\\na MySQL or PostgreSQL database. The point here is to show that Spark SQL offers an\\nANSI:2003–compliant SQL interface, and to demonstrate the interoperability\\nbetween SQL and DataFrames.\\nThe US flight delays data set has five columns:\\n•The date  column contains a string like 02190925 . When converted, this maps to\\n02-19 09:25 am .\\n•The delay  column gives the delay in minutes between the scheduled and actual\\ndeparture times. Early departures show negative numbers.\\n•The distance  column gives the distance in miles from the origin airport to the\\ndestination airport.\\n•The origin  column contains the origin IATA airport code.\\n•The destination  column contains the destination IATA airport code.\\nWith that in mind, let’s try some example queries against this data set.\\nFirst, we’ll find all flights whose distance is greater than 1,000 miles:\\nspark.sql(\"\"\"SELECT distance, origin, destination \\nFROM us_delay_flights_tbl WHERE distance > 1000 \\nORDER BY distance DESC\"\"\").show(10)\\n86 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 109}), Document(page_content='+--------+------+-----------+\\n|distance|origin|destination|\\n+--------+------+-----------+\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n+--------+------+-----------+\\nonly showing top 10 rows\\nAs the results show, all of the longest flights were between Honolulu (HNL) and New\\nY ork (JFK). Next, we’ll find all flights between San Francisco (SFO) and Chicago\\n(ORD) with at least a two-hour delay:\\nspark.sql(\"\"\"SELECT date, delay, origin, destination \\nFROM us_delay_flights_tbl \\nWHERE delay > 120 AND ORIGIN = \\'SFO\\' AND DESTINATION = \\'ORD\\' \\nORDER by delay DESC\"\"\").show(10)\\n+--------+-----+------+-----------+\\n|date    |delay|origin|destination|\\n+--------+-----+------+-----------+\\n|02190925|1638 |SFO   |ORD        |\\n|01031755|396  |SFO   |ORD        |\\n|01022330|326  |SFO   |ORD        |\\n|01051205|320  |SFO   |ORD        |\\n|01190925|297  |SFO   |ORD        |\\n|02171115|296  |SFO   |ORD        |\\n|01071040|279  |SFO   |ORD        |\\n|01051550|274  |SFO   |ORD        |\\n|03120730|266  |SFO   |ORD        |\\n|01261104|258  |SFO   |ORD        |\\n+--------+-----+------+-----------+\\nonly showing top 10 rows\\nIt seems there were many significantly delayed flights between these two cities, on dif‐\\nferent dates. (As an exercise, convert the date  column into a readable format and find\\nthe days or months when these delays were most common. Were the delays related to\\nwinter months or holidays?)\\nLet’s try a more complicated query where we use the CASE  clause in SQL. In the fol‐\\nlowing example, we want to label all US flights, regardless of origin and destination,\\nwith an indication of the delays they experienced: Very Long Delays (> 6 hours),\\nLong Delays (2–6 hours), etc. We’ll add these human-readable labels in a new column\\ncalled Flight_Delays :\\nUsing Spark SQL in Spark Applications | 87', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 110}), Document(page_content='spark.sql(\"\"\"SELECT delay, origin, destination,\\n              CASE\\n                  WHEN delay > 360 THEN \\'Very Long Delays\\'\\n                  WHEN delay > 120 AND delay < 360 THEN \\'Long Delays\\'\\n                  WHEN delay > 60 AND delay < 120 THEN \\'Short Delays\\'\\n                  WHEN delay > 0 and delay < 60  THEN  \\'Tolerable Delays\\'\\n                  WHEN delay = 0 THEN \\'No Delays\\'\\n                  ELSE \\'Early\\'\\n               END AS Flight_Delays\\n               FROM us_delay_flights_tbl\\n               ORDER BY origin, delay DESC\"\"\").show(10)\\n+-----+------+-----------+-------------+\\n|delay|origin|destination|Flight_Delays|\\n+-----+------+-----------+-------------+\\n|333  |ABE   |ATL        |Long Delays  |\\n|305  |ABE   |ATL        |Long Delays  |\\n|275  |ABE   |ATL        |Long Delays  |\\n|257  |ABE   |ATL        |Long Delays  |\\n|247  |ABE   |DTW        |Long Delays  |\\n|247  |ABE   |ATL        |Long Delays  |\\n|219  |ABE   |ORD        |Long Delays  |\\n|211  |ABE   |ATL        |Long Delays  |\\n|197  |ABE   |DTW        |Long Delays  |\\n|192  |ABE   |ORD        |Long Delays  |\\n+-----+------+-----------+-------------+\\nonly showing top 10 rows\\nAs with the DataFrame and Dataset APIs, with the spark.sql  interface you can con‐\\nduct common data analysis operations like those we explored in the previous chapter.\\nThe computations undergo an identical journey in the Spark SQL engine (see “The\\nCatalyst Optimizer” on page 77  in Chapter 3  for details), giving you the same results.\\nAll three of the preceding SQL queries can be expressed with an equivalent Data‐\\nFrame API query. For example, the first query can be expressed in the Python Data‐\\nFrame API as:\\n# In Python\\nfrom pyspark.sql.functions  import col, desc\\n(df.select(\"distance\" , \"origin\" , \"destination\" )\\n  .where(col(\"distance\" ) > 1000)\\n  .orderBy(desc(\"distance\" ))).show(10)\\n# Or\\n(df.select(\"distance\" , \"origin\" , \"destination\" )\\n  .where(\"distance > 1000\" )\\n  .orderBy(\"distance\" , ascending =False).show(10))\\nThis produces the same results as the SQL query:\\n88 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 111}), Document(page_content='+--------+------+-----------+\\n|distance|origin|destination|\\n+--------+------+-----------+\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n|4330    |HNL   |JFK        |\\n+--------+------+-----------+\\nonly showing top 10 rows\\nAs an exercise, try converting the other two SQL queries to use the DataFrame API.\\nAs these examples show, using the Spark SQL interface to query data is similar to\\nwriting a regular SQL query to a relational database table. Although the queries are in\\nSQL, you can feel the similarity in readability and semantics to DataFrame API oper‐\\nations, which you encountered in Chapter 3  and will explore further in the next\\nchapter.\\nTo enable you to query structured data as shown in the preceding examples, Spark\\nmanages all the complexities of creating and managing views and tables, both in\\nmemory and on disk. That leads us to our next topic: how tables and views are cre‐\\nated and managed.\\nSQL Tables and Views\\nTables hold data. Associated with each table in Spark is its relevant metadata, which is\\ninformation about the table and its data: the schema, description, table name, data‐\\nbase name, column names, partitions, physical location where the actual data resides,\\netc. All of this is stored in a central metastore.\\nInstead of having a separate metastore for Spark tables, Spark by default uses the\\nApache Hive metastore, located at /user/hive/warehouse, to persist all the metadata\\nabout your tables. However, you may change the default location by setting the Spark\\nconfig variable spark.sql.warehouse.dir  to another location, which can be set to a\\nlocal or external distributed storage.\\nManaged Versus UnmanagedTables\\nSpark allows you to create two types of tables: managed and unmanaged. For a man‐\\naged  table, Spark manages both the metadata and the data in the file store. This could\\nbe a local filesystem, HDFS, or an object store such as Amazon S3 or Azure Blob. For\\nSQL Tables and Views | 89', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 112}), Document(page_content='an unmanaged  table, Spark only manages the metadata, while you manage the data\\nyourself in an external data source  such as Cassandra.\\nWith a managed table, because Spark manages everything, a SQL command such as\\nDROP TABLE table_name  deletes both the metadata and the data. With an unmanaged\\ntable, the same command will delete only the metadata, not the actual data. We will\\nlook at some examples of how to create managed and unmanaged tables in the next\\nsection.\\nCreating SQL Databases and Tables\\nTables reside within a database. By default, Spark creates tables under the default\\ndatabase. To create your own database name, you can issue a SQL command from\\nyour Spark application or notebook. Using the US flight delays data set, let’s create\\nboth a managed and an unmanaged table. To begin, we’ll create a database called\\nlearn_spark_db  and tell Spark we want to use that database:\\n// In Scala/Python\\nspark.sql(\"CREATE DATABASE learn_spark_db\" )\\nspark.sql(\"USE learn_spark_db\" )\\nFrom this point, any commands we issue in our application to create tables will result\\nin the tables being created in this database and residing under the database name\\nlearn_spark_db .\\nCreating a managed table\\nTo create a managed table within the database learn_spark_db , you can issue a SQL\\nquery like the following:\\n// In Scala/Python\\nspark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT,  \\n  distance INT, origin STRING, destination STRING)\" )\\nY ou can do the same thing using the DataFrame API like this:\\n# In Python\\n# Path to our US flight delays CSV file \\ncsv_file  = \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\\n# Schema as defined in the preceding example\\nschema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\\nflights_df  = spark.read.csv(csv_file , schema=schema)\\nflights_df .write.saveAsTable (\"managed_us_delay_flights_tbl\" )\\nBoth of these statements will create the managed table us_delay_flights_tbl  in the\\nlearn_spark_db  database.\\n90 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 113}), Document(page_content='Creating an unmanaged table\\nBy contrast, you can create unmanaged tables from your own data sources—say, Par‐\\nquet, CSV , or JSON files stored in a file store accessible to your Spark application.\\nTo create an unmanaged table from a data source such as a CSV file, in SQL use:\\nspark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, \\n  distance INT, origin STRING, destination STRING) \\n  USING csv OPTIONS (PATH \\n  \\'/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\\')\"\"\")\\nAnd within the DataFrame API use:\\n(flights_df\\n  .write\\n  .option(\"path\", \"/tmp/data/us_flights_delay\")\\n  .saveAsTable(\"us_delay_flights_tbl\"))\\nTo enable you to explore these examples, we have created Python\\nand Scala example notebooks that you can find in the book’s Git‐\\nHub repo .\\nCreating Views\\nIn addition to creating tables, Spark can create views on top of existing tables. Views\\ncan be global (visible across all SparkSession s on a given cluster) or session-scoped\\n(visible only to a single SparkSession ), and they are temporary: they disappear after\\nyour Spark application terminates.\\nCreating views  has a similar syntax to creating tables within a database. Once you cre‐\\nate a view, you can query it as you would a table. The difference between a view and a\\ntable is that views don’t actually hold the data; tables persist after your Spark applica‐\\ntion terminates, but views disappear.\\nY ou can create a view from an existing table using SQL. For example, if you wish to\\nwork on only the subset of the US flight delays data set with origin airports of New\\nY ork (JFK) and San Francisco (SFO), the following queries will create global tempo‐\\nrary and temporary views consisting of just that slice of the table:\\n-- In SQL\\nCREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view  AS\\n  SELECT date, delay, origin, destination  from us_delay_flights_tbl  WHERE \\n  origin = \\'SFO\\';\\nCREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view  AS\\n  SELECT date, delay, origin, destination  from us_delay_flights_tbl  WHERE \\n  origin = \\'JFK\\'\\nSQL Tables and Views | 91', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 114}), Document(page_content='Y ou can accomplish the same thing with the DataFrame API as follows:\\n# In Python\\ndf_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM \\n  us_delay_flights_tbl  WHERE origin = \\'SFO\\'\")\\ndf_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM \\n  us_delay_flights_tbl  WHERE origin = \\'JFK\\'\")\\n# Create a temporary and global temporary view\\ndf_sfo.createOrReplaceGlobalTempView (\"us_origin_airport_SFO_global_tmp_view\" )\\ndf_jfk.createOrReplaceTempView (\"us_origin_airport_JFK_tmp_view\" )\\nOnce you’ve created these views, you can issue queries against them just as you would\\nagainst a table. Keep in mind that when accessing a global temporary view you must\\nuse the prefix global_temp .<view_name> , because Spark creates global temporary\\nviews in a global temporary database called global_temp . For example:\\n-- In SQL \\nSELECT * FROM global_temp .us_origin_airport_SFO_global_tmp_view\\nBy contrast, you can access the normal temporary view without the global_temp\\nprefix:\\n-- In SQL \\nSELECT * FROM us_origin_airport_JFK_tmp_view\\n// In Scala/Python\\nspark.read.table(\"us_origin_airport_JFK_tmp_view\" )\\n// Or\\nspark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\" )\\nY ou can also drop a view just like you would a table:\\n-- In SQL\\nDROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view ;\\nDROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view\\n// In Scala/Python\\nspark.catalog.dropGlobalTempView (\"us_origin_airport_SFO_global_tmp_view\" )\\nspark.catalog.dropTempView (\"us_origin_airport_JFK_tmp_view\" )\\nTemporary views versus global temporary views\\nThe difference between temporary  and global temporary  views being subtle, it can be a\\nsource of mild confusion among developers new to Spark. A temporary view is tied\\nto a single SparkSession  within a Spark application. In contrast, a global temporary\\nview is visible across multiple SparkSession s within a Spark application. Y es, you can\\ncreate multiple SparkSession s within a single Spark application—this can be handy,\\nfor example, in cases where you want to access (and combine) data from two different\\nSparkSession s that don’t share the same Hive metastore configurations.\\n92 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 115}), Document(page_content='Viewing the Metadata\\nAs mentioned previously, Spark manages the metadata associated with each managed\\nor unmanaged table. This is captured in the Catalog , a high-level abstraction in\\nSpark SQL for storing metadata. The Catalog ’s functionality was expanded in Spark\\n2.x with new public methods enabling you to examine the metadata associated with\\nyour databases, tables, and views. Spark 3.0 extends it to use external catalog  (which\\nwe briefly discuss in Chapter 12 ).\\nFor example, within a Spark application, after creating the SparkSession  variable\\nspark , you can access all the stored metadata through methods like these:\\n// In Scala/Python\\nspark.catalog.listDatabases ()\\nspark.catalog.listTables ()\\nspark.catalog.listColumns (\"us_delay_flights_tbl\" )\\nImport the notebook from the book’s GitHub repo  and give it a try.\\nCaching SQL Tables\\nAlthough we will discuss table caching strategies in the next chapter, it’s worth men‐\\ntioning here that, like DataFrames, you can cache and uncache SQL tables and views.\\nIn Spark 3.0 , in addition to other options, you can specify a table as LAZY , meaning\\nthat it should only be cached when it is first used instead of immediately:\\n-- In SQL\\nCACHE [LAZY] TABLE <table-name>\\nUNCACHE TABLE <table-name>\\nReading Tables into DataFrames\\nOften, data engineers build data pipelines as part of their regular data ingestion and\\nETL processes. They populate Spark SQL databases and tables with cleansed data for\\nconsumption by applications downstream.\\nLet’s assume you have an existing database, learn_spark_db , and table,\\nus_delay_flights_tbl , ready for use. Instead of reading from an external JSON file,\\nyou can simply use SQL to query the table and assign the returned result to a\\nDataFrame:\\n// In Scala\\nval usFlightsDF  = spark.sql(\"SELECT * FROM us_delay_flights_tbl\" )\\nval usFlightsDF2  = spark.table(\"us_delay_flights_tbl\" )\\nSQL Tables and Views | 93', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 116}), Document(page_content='# In Python\\nus_flights_df  = spark.sql(\"SELECT * FROM us_delay_flights_tbl\" )\\nus_flights_df2  = spark.table(\"us_delay_flights_tbl\" )\\nNow you have a cleansed DataFrame read from an existing Spark SQL table. Y ou can\\nalso read data in other formats using Spark’s built-in data sources, giving you the flex‐\\nibility to interact with various common file formats.\\nData Sources for DataFrames and SQL Tables\\nAs shown in Figure 4-1 , Spark SQL provides an interface to a variety of data sources.\\nIt also provides a set of common methods for reading and writing data to and from\\nthese data sources using the Data Sources API .\\nIn this section we will cover some of the built-in data sources , available file formats,\\nand ways to load and write data, along with specific options pertaining to these data\\nsources. But first, let’s take a closer look at two high-level Data Source API constructs\\nthat dictate the manner in which you interact with different data sources: DataFrameR\\neader  and DataFrameWriter .\\nDataFrameReader\\nDataFrameReader  is the core construct for reading data from a data source into a\\nDataFrame. It has a defined format and a recommended pattern for usage:\\nDataFrameReader.format(args).option(\"key\", \"value\").schema(args).load()\\nThis pattern of stringing methods together is common in Spark, and easy to read. We\\nsaw it in Chapter 3  when exploring common data analysis patterns.\\nNote that you can only access a DataFrameReader  through a SparkSession  instance.\\nThat is, you cannot create an instance of DataFrameReader . To get an instance handle\\nto it, use:\\nSparkSession.read \\n// or \\nSparkSession.readStream\\nWhile read  returns a handle to DataFrameReader  to read into a DataFrame from a\\nstatic data source, readStream  returns an instance to read from a streaming source.\\n(We will cover Structured Streaming later in the book.)\\nArguments to each of the public methods to DataFrameReader  take different values.\\nTable 4-1  enumerates these, with a subset of the supported arguments.\\n94 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 117}), Document(page_content='Table 4-1. DataFrameReader methods, arguments, and options\\nMethod Arguments Description\\nformat() \"parquet\" , \"csv\" , \"txt\" , \"json\" ,\\n\"jdbc\" , \"orc\" , \"avro\" , etc.If you don’t specify this method, then the default is\\nParquet or whatever is set in spark.sql.sour\\nces.default .\\noption() (\"mode\", {PERMISSIVE | FAILFAST \\n| DROPMALFORMED } )\\n(\"inferSchema\", {true | false})\\n(\"path\", \\n\"path_file_data_source\")A series of key/value pairs and options.\\nThe Spark documentation  shows some examples and\\nexplains the different  modes and their actions. The default\\nmode is PERMISSIVE . The \"inferSchema\"  and\\n\"mode\"  options are specific  to the JSON and CSV file\\nformats.\\nschema() DDL String  or StructType , e.g., \\'A \\nINT, B STRING\\'  or\\nStructType(...)For JSON or CSV format, you can specify to infer the\\nschema in the option()  method. Generally, providing a\\nschema for any format makes loading faster and ensures\\nyour data conforms to the expected schema.\\nload() \"/path/to/data/source\" The path to the data source. This can be empty if specified\\nin option(\"path\", \"...\") .\\nWhile we won’t comprehensively enumerate all the different combinations of argu‐\\nments and options, the documentation for Python, Scala, R, and Java  offers sugges‐\\ntions and guidance. It’s worthwhile to show a couple of examples, though:\\n// In Scala\\n// Use Parquet \\nval file = \"\"\"/databricks-datasets/learning-spark-v2/flights/summary-\\n  data/parquet/2010-summary.parquet\"\"\"\\nval df = spark.read.format(\"parquet\" ).load(file) \\n// Use Parquet; you can omit format(\"parquet\") if you wish as it\\'s the default\\nval df2 = spark.read.load(file)\\n// Use CSV\\nval df3 = spark.read.format(\"csv\")\\n  .option(\"inferSchema\" , \"true\")\\n  .option(\"header\" , \"true\")\\n  .option(\"mode\", \"PERMISSIVE\" )\\n  .load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\" )\\n// Use JSON\\nval df4 = spark.read.format(\"json\")\\n  .load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\" )\\nData Sources for DataFrames and SQL Tables | 95', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 118}), Document(page_content='In general, no schema is needed when reading from a static Parquet\\ndata source—the Parquet metadata usually contains the schema, so\\nit’s inferred. However, for streaming data sources you will have to\\nprovide a schema. (We will cover reading from streaming data\\nsources in Chapter 8 .)\\nParquet is the default and preferred data source for Spark because\\nit’s efficient, uses columnar storage, and employs a fast compres‐\\nsion algorithm. Y ou will see additional benefits later (such as col‐\\numnar pushdown), when we cover the Catalyst optimizer in\\ngreater depth.\\nDataFrameWriter\\nDataFrameWriter  does the reverse of its counterpart: it saves or writes data to a speci‐\\nfied built-in data source. Unlike with DataFrameReader , you access its instance not\\nfrom a SparkSession  but from the DataFrame you wish to save. It has a few recom‐\\nmended usage patterns:\\nDataFrameWriter.format(args)\\n  .option(args)\\n  .bucketBy(args)\\n  .partitionBy(args)\\n  .save(path)\\nDataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)\\nTo get an instance handle, use:\\nDataFrame.write\\n// or \\nDataFrame.writeStream\\nArguments to each of the methods to DataFrameWriter  also take different values. We\\nlist these in Table 4-2 , with a subset of the supported arguments.\\nTable 4-2. DataFrameWriter methods, arguments, and options\\nMethod Arguments Description\\nformat() \"parquet\" , \"csv\" , \"txt\" , \"json\" ,\\n\"jdbc\" , \"orc\" , \"avro\" , etc.If you don’t specify this method, then the default is Parquet\\nor whatever is set in spark.sql.sources.default .\\noption() (\"mode\", {append | overwrite \\n| ignore | error or errorifex\\nists} )\\n(\"mode\", {SaveMode.Overwrite \\n| SaveMode.Append, Save\\nMode.Ignore, SaveMode.ErrorI\\nfExists})\\n(\"path\", \"path_to_write_to\")A series of key/value pairs and options. The Spark\\ndocumentation  shows some examples. This is an overloaded\\nmethod. The default mode options are error or error\\nifexists  and SaveMode.ErrorIfExists ; they\\nthrow an exception at runtime if the data already exists.\\n96 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 119}), Document(page_content='Method Arguments Description\\nbuck\\netBy()(numBuckets, col, col..., \\ncoln)The number of buckets and names of columns to bucket by.\\nUses Hive’s bucketing scheme on a filesystem.\\nsave() \"/path/to/data/source\" The path to save to. This can be empty if specified  in\\noption(\"path\", \"...\") .\\nsaveAsTa\\nble()\"table_name\" The table to save to.\\nHere’s a short example snippet to illustrate the use of methods and arguments:\\n// In Scala\\n// Use JSON\\nval location  = ... \\ndf.write.format(\"json\").mode(\"overwrite\" ).save(location )\\nParquet\\nWe’ll start our exploration of data sources with Parquet , because it’s the default data\\nsource in Spark. Supported and widely used by many big data processing frameworks\\nand platforms, Parquet is an open source columnar file format that offers many I/O\\noptimizations (such as compression, which saves storage space and allows for quick\\naccess to data columns).\\nBecause of its efficiency and these optimizations, we recommend that after you have\\ntransformed and cleansed your data, you save your DataFrames in the Parquet format\\nfor downstream consumption. (Parquet is also the default table open format for Delta\\nLake, which we will cover in Chapter 9 .)\\nReading Parquet files  into a DataFrame\\nParquet files  are stored in a directory structure that contains the data files, metadata,\\na number of compressed files, and some status files. Metadata in the footer contains\\nthe version of the file format, the schema, and column data such as the path, etc.\\nFor example, a directory in a Parquet file might contain a set of files like this:\\n_SUCCESS\\n_committed_1799640464332036264\\n_started_1799640464332036264\\npart-00000-tid-1799640464332036264-91273258-d7ef-4dc7-<...>-c000.snappy.parquet\\nThere may be a number of part-XXXX  compressed files in a directory (the names\\nshown here have been shortened to fit on the page).\\nTo read Parquet files into a DataFrame, you simply specify the format and path:\\nData Sources for DataFrames and SQL Tables | 97', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 120}), Document(page_content='// In Scala\\nval file = \"\"\"/databricks-datasets/learning-spark-v2/flights/summary-data/\\n  parquet/2010-summary.parquet/\"\"\"\\nval df = spark.read.format(\"parquet\" ).load(file)\\n# In Python\\nfile = \"\"\"/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/\\n  2010-summary.parquet/\"\"\"\\ndf = spark.read.format(\"parquet\" ).load(file)\\nUnless you are reading from a streaming data source there’s no need to supply the\\nschema, because Parquet saves it as part of its metadata.\\nReading Parquet files  into a Spark SQL table\\nAs well as reading Parquet files into a Spark DataFrame, you can also create a Spark\\nSQL unmanaged table or view directly using SQL:\\n-- In SQL\\nCREATE OR REPLACE TEMPORARY  VIEW us_delay_flights_tbl\\n    USING parquet\\n    OPTIONS (\\n      path \"/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/\\n      2010-summary.parquet/\"  )\\nOnce you’ve created the table or view, you can read data into a DataFrame using SQL,\\nas we saw in some earlier examples:\\n// In Scala\\nspark.sql(\"SELECT * FROM us_delay_flights_tbl\" ).show()\\n# In Python\\nspark.sql(\"SELECT * FROM us_delay_flights_tbl\" ).show()\\nBoth of these operations return the same results:\\n+-----------------+-------------------+-----+\\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\\n+-----------------+-------------------+-----+\\n|United States    |Romania            |1    |\\n|United States    |Ireland            |264  |\\n|United States    |India              |69   |\\n|Egypt            |United States      |24   |\\n|Equatorial Guinea|United States      |1    |\\n|United States    |Singapore          |25   |\\n|United States    |Grenada            |54   |\\n|Costa Rica       |United States      |477  |\\n|Senegal          |United States      |29   |\\n|United States    |Marshall Islands   |44   |\\n+-----------------+-------------------+-----+\\nonly showing top 10 rows\\n98 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 121}), Document(page_content='Writing DataFrames to Parquet files\\nWriting or saving a DataFrame as a table or file is a common operation in Spark. To\\nwrite a DataFrame you simply use the methods and arguments to the DataFrame\\nWriter  outlined earlier in this chapter, supplying the location to save the Parquet files\\nto. For example:\\n// In Scala\\ndf.write.format(\"parquet\" )\\n  .mode(\"overwrite\" )\\n  .option(\"compression\" , \"snappy\" )\\n  .save(\"/tmp/data/parquet/df_parquet\" )\\n# In Python\\n(df.write.format(\"parquet\" )\\n  .mode(\"overwrite\" )\\n  .option(\"compression\" , \"snappy\" )\\n  .save(\"/tmp/data/parquet/df_parquet\" ))\\nRecall that Parquet is the default file format. If you don’t include\\nthe format()  method, the DataFrame will still be saved as a Par‐\\nquet file.\\nThis will create a set of compact and compressed Parquet files at the specified path.\\nSince we used snappy as our compression choice here, we’ll have snappy compressed\\nfiles. For brevity, this example generated only one file; normally, there may be a dozen\\nor so files created:\\n-rw-r--r--  1 jules  wheel    0 May 19 10:58 _SUCCESS\\n-rw-r--r--  1 jules  wheel  966 May 19 10:58 part-00000-<...>-c000.snappy.parquet\\nWriting DataFrames to Spark SQL tables\\nWriting a DataFrame to a SQL table is as easy as writing to a file—just use saveAsTa\\nble()  instead of save() . This will create a managed table called\\nus_delay_flights_tbl :\\n// In Scala\\ndf.write\\n  .mode(\"overwrite\" )\\n  .saveAsTable (\"us_delay_flights_tbl\" )\\n# In Python\\n(df.write\\n  .mode(\"overwrite\" )\\n  .saveAsTable (\"us_delay_flights_tbl\" ))\\nData Sources for DataFrames and SQL Tables | 99', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 122}), Document(page_content='To sum up, Parquet is the preferred and default built-in data source file format in\\nSpark, and it has been adopted by many other frameworks. We recommend that you\\nuse this format in your ETL and data ingestion processes.\\nJSON\\nJavaScript Object Notation (JSON) is also a popular data format. It came to promi‐\\nnence as an easy-to-read and easy-to-parse format compared to XML. It has two rep‐\\nresentational formats: single-line mode and multiline mode . Both modes are\\nsupported in Spark.\\nIn single-line mode each line denotes a single JSON object , whereas in multiline\\nmode the entire multiline object constitutes a single JSON object. To read in this\\nmode, set multiLine  to true in the option()  method.\\nReading a JSON file into a DataFrame\\nY ou can read a JSON file into a DataFrame the same way you did with Parquet—just\\nspecify \"json\"  in the format()  method:\\n// In Scala\\nval file = \"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\\nval df = spark.read.format(\"json\").load(file)\\n# In Python\\nfile = \"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\\ndf = spark.read.format(\"json\").load(file)\\nReading a JSON file into a Spark SQL table\\nY ou can also create a SQL table from a JSON file just like you did with Parquet:\\n-- In SQL\\nCREATE OR REPLACE TEMPORARY  VIEW us_delay_flights_tbl\\n    USING json\\n    OPTIONS (\\n      path  \"/databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\\n    )\\nOnce the table is created, you can read data into a DataFrame using SQL:\\n// In Scala/Python\\nspark.sql(\"SELECT * FROM us_delay_flights_tbl\" ).show()\\n+-----------------+-------------------+-----+\\n|DEST_COUNTRY_NAME |ORIGIN_COUNTRY_NAME |count|\\n+-----------------+-------------------+-----+\\n|United States    |Romania            |15   |\\n|United States    |Croatia            |1    |\\n|United States    |Ireland            |344  |\\n|Egypt            |United States      |15   |\\n100 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 123}), Document(page_content='|United States    |India              |62   |\\n|United States    |Singapore           |1    |\\n|United States    |Grenada            |62   |\\n|Costa Rica       |United States      |588  |\\n|Senegal          |United States      |40   |\\n|Moldova          |United States      |1    |\\n+-----------------+-------------------+-----+\\nonly showing top 10 rows\\nWriting DataFrames to JSON files\\nSaving a DataFrame as a JSON file is simple. Specify the appropriate\\nDataFrameWriter  methods and arguments, and supply the location to save the JSON\\nfiles to:\\n// In Scala\\ndf.write.format(\"json\")\\n  .mode(\"overwrite\" )\\n  .option(\"compression\" , \"snappy\" )\\n  .save(\"/tmp/data/json/df_json\" )\\n# In Python\\n(df.write.format(\"json\")\\n  .mode(\"overwrite\" )\\n  .option(\"compression\" , \"snappy\" )\\n  .save(\"/tmp/data/json/df_json\" ))\\nThis creates a directory at the specified path populated with a set of compact JSON\\nfiles:\\n-rw-r--r--  1 jules  wheel   0 May 16 14:44 _SUCCESS\\n-rw-r--r--  1 jules  wheel  71 May 16 14:44 part-00000-<...>-c000.json\\nJSON data source options\\nTable 4-3  describes common JSON options for DataFrameReader  and DataFrame\\nWriter . For a comprehensive list, we refer you to the documentation.\\nTable 4-3. JSON options for DataFrameReader and DataFrameWriter\\nProperty name Values Meaning Scope\\ncompression none , uncompressed ,\\nbzip2 , deflate , gzip ,\\nlz4 , or snappyUse this compression codec for writing. Note that read\\nwill only detect the compression or codec from the file\\nextension.Write\\ndateFormat yyyy-MM-dd  or DateTi\\nmeFormatterUse this format or any format from Java’s DateTime\\nFormatter .Read/\\nwrite\\nmultiLine true , false Use multiline mode. Default is false  (single-line\\nmode).Read\\nallowUnquoted\\nFieldNamestrue , false Allow unquoted JSON field  names. Default is false . Read\\nData Sources for DataFrames and SQL Tables | 101', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 124}), Document(page_content='CSV\\nAs widely used as plain text files, this common text file format captures each datum\\nor field delimited by a comma; each line with comma-separated fields represents a\\nrecord. Even though a comma is the default separator, you may use other delimiters\\nto separate fields in cases where commas are part of your data. Popular spreadsheets\\ncan generate CSV files, so it’s a popular format among data and business analysts.\\nReading a CSV file into a DataFrame\\nAs with the other built-in data sources, you can use the DataFrameReader  methods\\nand arguments to read a CSV file into a DataFrame:\\n// In Scala\\nval file = \"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\\nval schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\\nval df = spark.read.format(\"csv\")\\n  .schema(schema)\\n  .option(\"header\" , \"true\")\\n  .option(\"mode\", \"FAILFAST\" )     // Exit if any errors\\n  .option(\"nullValue\" , \"\")        // Replace any null data with quotes\\n  .load(file)\\n# In Python\\nfile = \"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\\nschema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\"\\ndf = (spark.read.format(\"csv\")\\n  .option(\"header\" , \"true\")\\n  .schema(schema)\\n  .option(\"mode\", \"FAILFAST\" )  # Exit if any errors\\n  .option(\"nullValue\" , \"\")     # Replace any null data field with quotes\\n  .load(file))\\nReading a CSV file into a Spark SQL table\\nCreating a SQL table from a CSV data source is no different from using Parquet or\\nJSON:\\n-- In SQL\\nCREATE OR REPLACE TEMPORARY  VIEW us_delay_flights_tbl\\n    USING csv\\n    OPTIONS (\\n      path \"/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\" ,\\n      header \"true\",\\n      inferSchema  \"true\",\\n      mode \"FAILFAST\"\\n    )\\nOnce you’ve created the table, you can read data into a DataFrame using SQL as\\nbefore:\\n102 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 125}), Document(page_content='// In Scala/Python\\nspark.sql(\"SELECT * FROM us_delay_flights_tbl\" ).show(10)\\n+-----------------+-------------------+-----+\\n|DEST_COUNTRY_NAME |ORIGIN_COUNTRY_NAME |count|\\n+-----------------+-------------------+-----+\\n|United States    |Romania            |1    |\\n|United States    |Ireland            |264  |\\n|United States    |India              |69   |\\n|Egypt            |United States      |24   |\\n|Equatorial  Guinea|United States      |1    |\\n|United States    |Singapore           |25   |\\n|United States    |Grenada            |54   |\\n|Costa Rica       |United States      |477  |\\n|Senegal          |United States      |29   |\\n|United States    |Marshall  Islands   |44   |\\n+-----------------+-------------------+-----+\\nonly showing top 10 rows\\nWriting DataFrames to CSV files\\nSaving a DataFrame as a CSV file is simple. Specify the appropriate DataFrameWriter\\nmethods and arguments, and supply the location to save the CSV files to:\\n// In Scala\\ndf.write.format(\"csv\").mode(\"overwrite\" ).save(\"/tmp/data/csv/df_csv\" )\\n# In Python\\ndf.write.format(\"csv\").mode(\"overwrite\" ).save(\"/tmp/data/csv/df_csv\" )\\nThis generates a folder at the specified location, populated with a bunch of com‐\\npressed and compact files:\\n-rw-r--r--  1 jules  wheel   0 May 16 12:17 _SUCCESS\\n-rw-r--r--  1 jules  wheel  36 May 16 12:17 part-00000-251690eb-<...>-c000.csv\\nCSV data source options\\nTable 4-4  describes some of the common CSV options for DataFrameReader  and Data\\nFrameWriter . Because CSV files can be complex, many options are available; for a\\ncomprehensive list we refer you to the documentation.\\nData Sources for DataFrames and SQL Tables | 103', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 126}), Document(page_content='Table 4-4. CSV options for DataFrameReader and DataFrameWriter\\nProperty name Values Meaning Scope\\ncompression none , bzip2 , deflate ,\\ngzip , lz4 , or snappyUse this compression codec for writing. Write\\ndateFormat yyyy-MM-dd  or DateTime\\nFormatterUse this format or any format from Java’s Date\\nTimeFormatter .Read/\\nwrite\\nmultiLine true , false Use multiline mode. Default is false  (single-\\nline mode).Read\\ninferSchema true , false If true , Spark will determine the column data\\ntypes. Default is false .Read\\nsep Any character Use this character to separate column values in a\\nrow. Default delimiter is a comma ( ,).Read/\\nwrite\\nescape Any character Use this character to escape quotes. Default is \\\\. Read/\\nwrite\\nheader true , false Indicates whether the first  line is a header\\ndenoting each column name. Default is false .Read/\\nwrite\\nAvro\\nIntroduced in Spark 2.4  as a built-in data source, the Avro format  is used, for exam‐\\nple, by Apache Kafka  for message serializing and deserializing. It offers many bene‐\\nfits, including direct mapping to JSON, speed and efficiency, and bindings available\\nfor many programming languages.\\nReading an Avro file into a DataFrame\\nReading an Avro file into a DataFrame using DataFrameReader  is consistent in usage\\nwith the other data sources we have discussed in this section:\\n// In Scala\\nval df = spark.read.format(\"avro\")\\n .load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\" )\\ndf.show(false)\\n# In Python\\ndf = (spark.read.format(\"avro\")\\n  .load(\"/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\" ))\\ndf.show(truncate =False)\\n+-----------------+-------------------+-----+\\n|DEST_COUNTRY_NAME |ORIGIN_COUNTRY_NAME |count|\\n+-----------------+-------------------+-----+\\n|United States    |Romania            |1    |\\n|United States    |Ireland            |264  |\\n|United States    |India              |69   |\\n|Egypt            |United States      |24   |\\n|Equatorial  Guinea|United States      |1    |\\n104 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 127}), Document(page_content='|United States    |Singapore           |25   |\\n|United States    |Grenada            |54   |\\n|Costa Rica       |United States      |477  |\\n|Senegal          |United States      |29   |\\n|United States    |Marshall  Islands   |44   |\\n+-----------------+-------------------+-----+\\nonly showing top 10 rows\\nReading an Avro file into a Spark SQL table\\nAgain, creating SQL tables using an Avro data source is no different from using Par‐\\nquet, JSON, or CSV:\\n-- In SQL \\nCREATE OR REPLACE TEMPORARY  VIEW episode_tbl\\n    USING avro\\n    OPTIONS (\\n      path \"/databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"\\n    )\\nOnce you’ve created a table, you can read data into a DataFrame using SQL:\\n// In Scala\\nspark.sql(\"SELECT * FROM episode_tbl\" ).show(false)\\n# In Python\\nspark.sql(\"SELECT * FROM episode_tbl\" ).show(truncate =False)\\n+-----------------+-------------------+-----+\\n|DEST_COUNTRY_NAME |ORIGIN_COUNTRY_NAME |count|\\n+-----------------+-------------------+-----+\\n|United States    |Romania            |1    |\\n|United States    |Ireland            |264  |\\n|United States    |India              |69   |\\n|Egypt            |United States      |24   |\\n|Equatorial  Guinea|United States      |1    |\\n|United States    |Singapore           |25   |\\n|United States    |Grenada            |54   |\\n|Costa Rica       |United States      |477  |\\n|Senegal          |United States      |29   |\\n|United States    |Marshall  Islands   |44   |\\n+-----------------+-------------------+-----+\\nonly showing top 10 rows\\nWriting DataFrames to Avro files\\nWriting a DataFrame as an Avro file is simple. As usual, specify the appropriate Data\\nFrameWriter  methods and arguments, and supply the location to save the Avro files\\nto:\\nData Sources for DataFrames and SQL Tables | 105', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 128}), Document(page_content='// In Scala\\ndf.write\\n  .format(\"avro\")\\n  .mode(\"overwrite\" )\\n  .save(\"/tmp/data/avro/df_avro\" )\\n# In Python\\n(df.write\\n  .format(\"avro\")\\n  .mode(\"overwrite\" )\\n  .save(\"/tmp/data/avro/df_avro\" ))\\nThis generates a folder at the specified location, populated with a bunch of com‐\\npressed and compact files:\\n-rw-r--r--  1 jules  wheel    0 May 17 11:54 _SUCCESS\\n-rw-r--r--  1 jules  wheel  526 May 17 11:54 part-00000-ffdf70f4-<...>-c000.avro\\nAvro data source options\\nTable 4-5  describes common options for DataFrameReader  and DataFrameWriter . A\\ncomprehensive list of options is in the documentation .\\nTable 4-5. Avro options for DataFrameReader and DataFrameWriter\\nProperty name Default\\nvalueMeaning Scope\\navroSchema None Optional Avro schema provided by a user in JSON format. The data\\ntype and naming of record fields  should match the input Avro data or\\nCatalyst data (Spark internal data type), otherwise the read/write\\naction will fail.Read/\\nwrite\\nrecordName topLevel\\nRecordTop-level record name in write result, which is required in the Avro\\nspec.Write\\nrecordNamespace \"\" Record namespace in write result. Write\\nignoreExtension true If this option is enabled, all files  (with and without the .avro\\nextension) are loaded. Otherwise, files  without the .avro  extension are\\nignored.Read\\ncompression snappy Allows you to specify the compression codec to use in writing.\\nCurrently supported codecs are uncompressed , snappy ,\\ndeflate , bzip2 , and xz.\\nIf this option is not set, the value in spark.sql.avro.compres\\nsion.codec  is taken into account.Write\\nORC\\nAs an additional optimized columnar file format, Spark 2.x supports a vectorized\\nORC reader . Two Spark configurations dictate which ORC implementation to use.\\nWhen spark.sql.orc.impl  is set to native  and spark.sql.orc.enableVectorize\\ndReader  is set to true , Spark uses the vectorized ORC reader. A vectorized reader\\n106 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 129}), Document(page_content='reads blocks of rows (often 1,024 per block) instead of one row at a time, streamlining\\noperations and reducing CPU usage for intensive operations like scans, filters, aggre‐\\ngations, and joins.\\nFor Hive ORC SerDe (serialization and deserialization) tables created with the SQL\\ncommand USING HIVE OPTIONS (fileFormat \\'ORC\\') , the vectorized reader is used\\nwhen the Spark configuration parameter spark.sql.hive.convertMetastoreOrc  is\\nset to true .\\nReading an ORC file into a DataFrame\\nTo read in a DataFrame using the ORC vectorized reader, you can just use the normal\\nDataFrameReader  methods and options:\\n// In Scala \\nval file = \"/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\\nval df = spark.read.format(\"orc\").load(file)\\ndf.show(10, false)\\n# In Python\\nfile = \"/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\\ndf = spark.read.format(\"orc\").option(\"path\", file).load()\\ndf.show(10, False)\\n+-----------------+-------------------+-----+\\n|DEST_COUNTRY_NAME |ORIGIN_COUNTRY_NAME |count|\\n+-----------------+-------------------+-----+\\n|United States    |Romania            |1    |\\n|United States    |Ireland            |264  |\\n|United States    |India              |69   |\\n|Egypt            |United States      |24   |\\n|Equatorial  Guinea|United States      |1    |\\n|United States    |Singapore           |25   |\\n|United States    |Grenada            |54   |\\n|Costa Rica       |United States      |477  |\\n|Senegal          |United States      |29   |\\n|United States    |Marshall  Islands   |44   |\\n+-----------------+-------------------+-----+\\nonly showing top 10 rows\\nReading an ORC file into a Spark SQL table\\nThere is no difference from Parquet, JSON, CSV , or Avro when creating a SQL view\\nusing an ORC data source:\\n-- In SQL\\nCREATE OR REPLACE TEMPORARY  VIEW us_delay_flights_tbl\\n    USING orc\\n    OPTIONS (\\n      path \"/databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\\n    )\\nData Sources for DataFrames and SQL Tables | 107', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 130}), Document(page_content='Once a table is created, you can read data into a DataFrame using SQL as usual:\\n// In Scala/Python\\nspark.sql(\"SELECT * FROM us_delay_flights_tbl\" ).show()\\n+-----------------+-------------------+-----+\\n|DEST_COUNTRY_NAME |ORIGIN_COUNTRY_NAME |count|\\n+-----------------+-------------------+-----+\\n|United States    |Romania            |1    |\\n|United States    |Ireland            |264  |\\n|United States    |India              |69   |\\n|Egypt            |United States      |24   |\\n|Equatorial  Guinea|United States      |1    |\\n|United States    |Singapore           |25   |\\n|United States    |Grenada            |54   |\\n|Costa Rica       |United States      |477  |\\n|Senegal          |United States      |29   |\\n|United States    |Marshall  Islands   |44   |\\n+-----------------+-------------------+-----+\\nonly showing top 10 rows\\nWriting DataFrames to ORC files\\nWriting back a transformed DataFrame after reading is equally simple using the\\nDataFrameWriter  methods:\\n// In Scala\\ndf.write.format(\"orc\")\\n  .mode(\"overwrite\" )\\n  .option(\"compression\" , \"snappy\" )\\n  .save(\"/tmp/data/orc/df_orc\" )\\n# In Python\\n(df.write.format(\"orc\")\\n  .mode(\"overwrite\" )\\n  .option(\"compression\" , \"snappy\" )\\n  .save(\"/tmp/data/orc/flights_orc\" ))\\nThe result will be a folder at the specified location containing some compressed ORC\\nfiles:\\n-rw-r--r--  1 jules  wheel    0 May 16 17:23 _SUCCESS\\n-rw-r--r--  1 jules  wheel  547 May 16 17:23 part-00000-<...>-c000.snappy.orc\\nImages\\nIn Spark 2.4 the community introduced a new data source, image files , to support\\ndeep learning and machine learning frameworks such as TensorFlow and PyTorch.\\nFor computer vision–based machine learning applications, loading and processing\\nimage data sets is important.\\n108 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 131}), Document(page_content='Reading an image file into a DataFrame\\nAs with all of the previous file formats, you can use the DataFrameReader  methods\\nand options to read in an image file as shown here:\\n// In Scala\\nimport org.apache.spark.ml.source.image\\nval imageDir  = \"/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\\nval imagesDF  = spark.read.format(\"image\").load(imageDir )\\nimagesDF .printSchema\\nimagesDF .select(\"image.height\" , \"image.width\" , \"image.nChannels\" , \"image.mode\" , \\n  \"label\").show(5, false)\\n# In Python\\nfrom pyspark.ml  import image\\nimage_dir  = \"/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\\nimages_df  = spark.read.format(\"image\").load(image_dir )\\nimages_df .printSchema ()\\nroot\\n |-- image: struct (nullable  = true)\\n |    |-- origin: string (nullable  = true)\\n |    |-- height: integer (nullable  = true)\\n |    |-- width: integer (nullable  = true)\\n |    |-- nChannels : integer (nullable  = true)\\n |    |-- mode: integer (nullable  = true)\\n |    |-- data: binary (nullable  = true)\\n |-- label: integer (nullable  = true)\\nimages_df .select(\"image.height\" , \"image.width\" , \"image.nChannels\" , \"image.mode\" , \\n  \"label\").show(5, truncate =False)\\n+------+-----+---------+----+-----+\\n|height|width|nChannels |mode|label|\\n+------+-----+---------+----+-----+\\n|288   |384  |3        |16  |0    |\\n|288   |384  |3        |16  |1    |\\n|288   |384  |3        |16  |0    |\\n|288   |384  |3        |16  |0    |\\n|288   |384  |3        |16  |0    |\\n+------+-----+---------+----+-----+\\nonly showing top 5 rows\\nData Sources for DataFrames and SQL Tables | 109', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 132}), Document(page_content='Binary Files\\nSpark 3.0 adds support for binary files as a data source . The DataFrameReader  con‐\\nverts each binary file into a single DataFrame row (record) that contains the raw con‐\\ntent and metadata of the file. The binary file data source produces a DataFrame with\\nthe following columns:\\n•path: StringType\\n•modificationTime: TimestampType\\n•length: LongType\\n•content: BinaryType\\nReading a binary file into a DataFrame\\nTo read binary files, specify the data source format as a binaryFile . Y ou can load files\\nwith paths matching a given global pattern while preserving the behavior of partition\\ndiscovery with the data source option pathGlobFilter . For example, the following\\ncode reads all JPG files from the input directory with any partitioned directories:\\n// In Scala\\nval path = \"/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\\nval binaryFilesDF  = spark.read.format(\"binaryFile\" )\\n  .option(\"pathGlobFilter\" , \"*.jpg\")\\n  .load(path)\\nbinaryFilesDF .show(5)\\n# In Python\\npath = \"/databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\\nbinary_files_df  = (spark.read.format(\"binaryFile\" )\\n  .option(\"pathGlobFilter\" , \"*.jpg\")\\n  .load(path))\\nbinary_files_df .show(5)\\n+--------------------+-------------------+------+--------------------+-----+\\n|                path|   modificationTime |length|             content|label|\\n+--------------------+-------------------+------+--------------------+-----+\\n|file:/Users/jules...|2020-02-12 12:04:24| 55037|[FF D8 FF E0 00 1...|    0|\\n|file:/Users/jules...|2020-02-12 12:04:24| 54634|[FF D8 FF E0 00 1...|    1|\\n|file:/Users/jules...|2020-02-12 12:04:24| 54624|[FF D8 FF E0 00 1...|    0|\\n|file:/Users/jules...|2020-02-12 12:04:24| 54505|[FF D8 FF E0 00 1...|    0|\\n|file:/Users/jules...|2020-02-12 12:04:24| 54475|[FF D8 FF E0 00 1...|    0|\\n+--------------------+-------------------+------+--------------------+-----+\\nonly showing top 5 rows\\nTo ignore partitioning data discovery in a directory, you can set recursiveFile\\nLookup  to \"true\" :\\n110 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 133}), Document(page_content='// In Scala\\nval binaryFilesDF  = spark.read.format(\"binaryFile\" )\\n  .option(\"pathGlobFilter\" , \"*.jpg\")\\n  .option(\"recursiveFileLookup\" , \"true\")\\n  .load(path)\\nbinaryFilesDF .show(5)\\n# In Python\\nbinary_files_df  = (spark.read.format(\"binaryFile\" )\\n  .option(\"pathGlobFilter\" , \"*.jpg\")\\n  .option(\"recursiveFileLookup\" , \"true\")\\n  .load(path))\\nbinary_files_df .show(5)\\n+--------------------+-------------------+------+--------------------+\\n|                path|   modificationTime |length|             content|\\n+--------------------+-------------------+------+--------------------+\\n|file:/Users/jules...|2020-02-12 12:04:24| 55037|[FF D8 FF E0 00 1...|\\n|file:/Users/jules...|2020-02-12 12:04:24| 54634|[FF D8 FF E0 00 1...|\\n|file:/Users/jules...|2020-02-12 12:04:24| 54624|[FF D8 FF E0 00 1...|\\n|file:/Users/jules...|2020-02-12 12:04:24| 54505|[FF D8 FF E0 00 1...|\\n|file:/Users/jules...|2020-02-12 12:04:24| 54475|[FF D8 FF E0 00 1...|\\n+--------------------+-------------------+------+--------------------+\\nonly showing top 5 rows\\nNote that the label  column is absent when the recursiveFileLookup  option is set to\\n\"true\" .\\nCurrently, the binary file data source does not support writing a DataFrame back to\\nthe original file format.\\nIn this section, you got a tour of how to read data into a DataFrame from a range of\\nsupported file formats. We also showed you how to create temporary views and tables\\nfrom the existing built-in data sources. Whether you’re using the DataFrame API or\\nSQL, the queries produce identical outcomes. Y ou can examine some of these queries\\nin the notebook available in the GitHub repo  for this book.\\nSummary\\nTo recap, this chapter explored the interoperability between the DataFrame API and\\nSpark SQL. In particular, you got a flavor of how to use Spark SQL to:\\n•Create managed and unmanaged tables using Spark SQL and the DataFrame\\nAPI.\\n•Read from and write to various built-in data sources and file formats.\\n•Employ the spark.sql  programmatic interface to issue SQL queries on struc‐\\ntured data stored as Spark SQL tables or views.\\n•Peruse the Spark Catalog  to inspect metadata associated with tables and views.\\nSummary | 111', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 134}), Document(page_content='•Use the DataFrameWriter  and DataFrameReader  APIs.\\nThrough the code snippets in the chapter and the notebooks available in the book’s\\nGitHub repo , you got a feel for how to use DataFrames and Spark SQL. Continuing in\\nthis vein, the next chapter further explores how Spark interacts with the external data\\nsources shown in Figure 4-1 . Y ou’ll see some more in-depth examples of transforma‐\\ntions and the interoperability between the DataFrame API and Spark SQL.\\n112 | Chapter 4: Spark SQL and DataFrames: Introduction to Built-in Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 135}), Document(page_content='1The current Spark SQL engine no longer uses the Hive code in its implementation.CHAPTER 5\\nSpark SQL and DataFrames:\\nInteracting with External Data Sources\\nIn the previous chapter, we explored interacting with the built-in data sources in\\nSpark. We also took a closer look at the DataFrame API and its interoperability with\\nSpark SQL. In this chapter, we will focus on how Spark SQL interfaces with external\\ncomponents. Specifically, we discuss how Spark SQL allows you to:\\n•Use user-defined functions for both Apache Hive and Apache Spark.\\n•Connect with external data sources such as JDBC and SQL databases, Post‐\\ngreSQL, MySQL, Tableau, Azure Cosmos DB, and MS SQL Server.\\n•Work with simple and complex types, higher-order functions, and common rela‐\\ntional operators.\\nWe’ll also look at some different options for querying Spark using Spark SQL, such as\\nthe Spark SQL shell, Beeline, and Tableau.\\nSpark SQL and Apache Hive\\nSpark SQL is a foundational component of Apache Spark that integrates relational\\nprocessing with Spark’s functional programming API. Its genesis was in previous\\nwork on Shark . Shark was originally built on the Hive codebase on top of Apache\\nSpark1 and became one of the first interactive SQL query engines on Hadoop systems.\\nIt demonstrated that it was possible to have the best of both worlds ; as fast as an\\nenterprise data warehouse, and scaling as well as Hive/MapReduce.\\n113', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 136}), Document(page_content='Spark SQL lets Spark programmers leverage the benefits of faster performance and\\nrelational programming (e.g., declarative queries and optimized storage), as well as\\ncall complex analytics libraries (e.g., machine learning). As discussed in the previous\\nchapter, as of Apache Spark 2.x, the SparkSession  provides a single unified entry\\npoint to manipulate data in Spark.\\nUser-Defined  Functions\\nWhile Apache Spark has a plethora of built-in functions, the flexibility of Spark\\nallows for data engineers and data scientists to define their own functions too. These\\nare known as user-defined  functions  (UDFs).\\nSpark SQL UDFs\\nThe benefit of creating your own PySpark or Scala UDFs is that you (and others) will\\nbe able to make use of them within Spark SQL itself. For example, a data scientist can\\nwrap an ML model within a UDF so that a data analyst can query its predictions in\\nSpark SQL without necessarily understanding the internals of the model.\\nHere’s a simplified example of creating a Spark SQL UDF. Note that UDFs operate per\\nsession and they will not be persisted in the underlying metastore:\\n// In Scala\\n// Create cubed function\\nval cubed = (s: Long) => {\\n  s * s * s\\n}\\n// Register UDF\\nspark.udf.register (\"cubed\", cubed)\\n// Create temporary view\\nspark.range(1, 9).createOrReplaceTempView (\"udf_test\" )\\n# In Python\\nfrom pyspark.sql.types  import LongType\\n# Create cubed function\\ndef cubed(s):\\n  return s * s * s\\n# Register UDF\\nspark.udf.register (\"cubed\", cubed, LongType ())\\n# Generate temporary view\\nspark.range(1, 9).createOrReplaceTempView (\"udf_test\" )\\n114 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 137}), Document(page_content='Y ou can now use Spark SQL to execute either of these cubed()  functions:\\n// In Scala/Python\\n// Query the cubed UDF\\nspark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\" ).show()\\n+---+--------+\\n| id|id_cubed |\\n+---+--------+\\n|  1|       1|\\n|  2|       8|\\n|  3|      27|\\n|  4|      64|\\n|  5|     125|\\n|  6|     216|\\n|  7|     343|\\n|  8|     512|\\n+---+--------+\\nEvaluation order and null checking in Spark SQL\\nSpark SQL (this includes SQL, the DataFrame API, and the Dataset API) does not\\nguarantee the order of evaluation of subexpressions. For example, the following query\\ndoes not guarantee that the s is NOT NULL  clause is executed prior to the strlen(s)\\n> 1 clause:\\nspark.sql(\"SELECT s FROM test1 WHERE s IS NOT NULL AND strlen(s) > 1\" )\\nTherefore, to perform proper null  checking, it is recommended that you do the\\nfollowing:\\n1.Make the UDF itself null -aware and do null  checking inside the UDF.\\n2.Use IF or CASE WHEN  expressions to do the null  check and invoke the UDF in a\\nconditional branch.\\nSpeeding up and distributing PySpark UDFs with Pandas UDFs\\nOne of the previous prevailing issues with using PySpark UDFs was that they had\\nslower performance than Scala UDFs. This was because the PySpark UDFs required\\ndata movement between the JVM and Python, which was quite expensive. To resolve\\nthis problem, Pandas UDFs  (also known as vectorized UDFs) were introduced as part\\nof Apache Spark 2.3. A Pandas UDF uses Apache Arrow to transfer data and Pandas\\nto work with the data. Y ou define a Pandas UDF using the keyword pandas_udf  as\\nthe decorator, or to wrap the function itself. Once the data is in Apache Arrow for‐\\nmat, there is no longer the need to serialize/pickle the data as it is already in a format\\nconsumable by the Python process. Instead of operating on individual inputs row by\\nrow, you are operating on a Pandas Series or DataFrame (i.e., vectorized execution).\\nSpark SQL and Apache Hive | 115', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 138}), Document(page_content='2Note there are slight differences when working with Pandas UDFs between Spark 2.3, 2.4, and 3.0.From Apache Spark 3.0 with Python 3.6 and above, Pandas UDFs were split into two\\nAPI categories : Pandas UDFs and Pandas Function APIs.\\nPandas UDFs\\nWith Apache Spark 3.0, Pandas UDFs infer the Pandas UDF type from Python\\ntype hints in Pandas UDFs such as pandas.Series , pandas.DataFrame , Tuple ,\\nand Iterator . Previously you needed to manually define and specify each Pan‐\\ndas UDF type. Currently, the supported cases of Python type hints in Pandas\\nUDFs are Series to Series, Iterator of Series to Iterator of Series, Iterator of Multi‐\\nple Series to Iterator of Series, and Series to Scalar (a single value).\\nPandas Function APIs\\nPandas Function APIs allow you to directly apply a local Python function to a\\nPySpark DataFrame where both the input and output are Pandas instances. For\\nSpark 3.0, the supported Pandas Function APIs are grouped map, map, co-\\ngrouped map.\\nFor more information, refer to “Redesigned Pandas UDFs with Python Type Hints”\\non page 354  in Chapter 12 .\\nThe following is an example of a scalar Pandas UDF for Spark 3.0:2\\n# In Python\\n# Import pandas\\nimport pandas as pd\\n# Import various pyspark SQL functions including pandas_udf\\nfrom pyspark.sql.functions  import col, pandas_udf\\nfrom pyspark.sql.types  import LongType\\n# Declare the cubed function \\ndef cubed(a: pd.Series) -> pd.Series:\\n    return a * a * a\\n# Create the pandas UDF for the cubed function \\ncubed_udf  = pandas_udf (cubed, returnType =LongType ())\\nThe preceding code snippet declares a function called cubed()  that performs a cubed\\noperation. This is a regular Pandas function with the additional cubed_udf = pan\\ndas_udf()  call to create our Pandas UDF.\\n116 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 139}), Document(page_content='Let’s start with a simple Pandas Series (as defined for x) and then apply the local func‐\\ntion cubed()  for the cubed calculation:\\n# Create a Pandas Series\\nx = pd.Series([1, 2, 3])\\n# The function for a pandas_udf executed with local Pandas data\\nprint(cubed(x))\\nThe output is as follows:\\n0     1\\n1     8\\n2    27\\ndtype: int64\\nNow let’s switch to a Spark DataFrame. We can execute this function as a Spark vec‐\\ntorized UDF as follows:\\n# Create a Spark DataFrame, \\'spark\\' is an existing SparkSession\\ndf = spark.range(1, 4)\\n# Execute function as a Spark vectorized UDF\\ndf.select(\"id\", cubed_udf (col(\"id\"))).show()\\nHere’s the output:\\n+---+---------+\\n| id|cubed(id)|\\n+---+---------+\\n|  1|        1|\\n|  2|        8|\\n|  3|       27|\\n+---+---------+\\nAs opposed to a local function, using a vectorized UDF will result in the execution of\\nSpark jobs; the previous local function is a Pandas function executed only on the\\nSpark driver. This becomes more apparent when viewing the Spark UI for one of the\\nstages of this pandas_udf  function ( Figure 5-1 ).\\nFor a deeper dive into Pandas UDFs, refer to pandas user-defined\\nfunctions documentation .\\nSpark SQL and Apache Hive | 117', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 140}), Document(page_content='Figure 5-1. Spark UI stages for executing a Pandas UDF on a Spark DataFrame\\n118 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 141}), Document(page_content='Like many Spark jobs, the job starts with parallelize()  to send local data (Arrow\\nbinary batches) to executors and calls mapPartitions()  to convert the Arrow binary\\nbatches to Spark’s internal data format, which can be distributed to the Spark work‐\\ners. There are a number of WholeStageCodegen  steps, which represent a fundamental\\nstep up in performance (thanks to Project Tungsten’s whole-stage code generation ,\\nwhich significantly improves CPU efficiency and performance). But it is the ArrowE\\nvalPython  step that identifies that (in this case) a Pandas UDF is being executed.\\nQuerying with the Spark SQL Shell, Beeline, and Tableau\\nThere are various mechanisms to query Apache Spark, including the Spark SQL shell,\\nthe Beeline CLI utility, and reporting tools like Tableau and Power BI.\\nIn this section, we include instructions for Tableau; for Power BI, please refer to the\\ndocumentation .\\nUsing the Spark SQL Shell\\nA convenient tool for executing Spark SQL queries is the spark-sql  CLI. While this\\nutility communicates with the Hive metastore service in local mode, it does not talk\\nto the Thrift JDBC/ODBC server  (a.k.a. Spark Thrift  Server  or STS). The STS allows\\nJDBC/ODBC clients to execute SQL queries over JDBC and ODBC protocols on\\nApache Spark.\\nTo start the Spark SQL CLI, execute the following command in the $SPARK_HOME\\nfolder:\\n./bin/spark-sql\\nOnce you’ve started the shell, you can use it to interactively perform Spark SQL quer‐\\nies. Let’s take a look at a few examples.\\nCreate a table\\nTo create a new permanent Spark SQL table, execute the following statement:\\nspark-sql> CREATE TABLE people (name STRING, age int);\\nY our output should be similar to this, noting the creation of the Spark SQL table\\npeople  as well as its file location ( /user/hive/warehouse/people ):\\nQuerying with the Spark SQL Shell, Beeline, and Tableau | 119', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 142}), Document(page_content='20/01/11 22:42:16 WARN HiveMetaStore : Location : file:/user/hive/warehouse /people\\nspecified  for non-external  table:people\\nTime taken: 0.63 seconds\\nInsert data into the table\\nY ou can insert data into a Spark SQL table by executing a statement similar to:\\nINSERT INTO people SELECT name, age FROM ...\\nAs you’re not dependent on loading data from a preexisting table or file, you can\\ninsert data into the table using INSERT...VALUES  statements. These three statements\\ninsert three individuals (their names and ages, if known) into the people  table:\\nspark-sql> INSERT INTO people VALUES (\"Michael\", NULL);\\nTime taken: 1.696 seconds\\nspark-sql> INSERT INTO people VALUES (\"Andy\", 30);\\nTime taken: 0.744 seconds\\nspark-sql> INSERT INTO people VALUES (\"Samantha\", 19);\\nTime taken: 0.637 seconds\\nspark-sql>\\nRunning a Spark SQL query\\nNow that you have data in your table, you can run Spark SQL queries against it. Let’s\\nstart by viewing what tables exist in our metastore:\\nspark-sql> SHOW TABLES;\\ndefault   people     false\\nTime taken: 0.016 seconds, Fetched 1 row(s)\\nNext, let’s find out how many people in our table are younger than 20 years of age:\\nspark-sql> SELECT * FROM people WHERE age < 20;\\nSamantha  19\\nTime taken: 0.593 seconds, Fetched 1 row(s)\\nAs well, let’s see who the individuals are who did not specify their age:\\nspark-sql> SELECT name FROM people WHERE age IS NULL;\\nMichael\\nTime taken: 0.272 seconds, Fetched 1 row(s)\\nWorking with Beeline\\nIf you’ve worked with Apache Hive you may be familiar with the command-line tool\\nBeeline , a common utility for running HiveQL queries against HiveServer2. Beeline\\nis a JDBC client based on the SQLLine CLI . Y ou can use this same utility to execute\\nSpark SQL queries against the Spark Thrift server. Note that the currently imple‐\\nmented Thrift JDBC/ODBC server corresponds to HiveServer2 in Hive 1.2.1. Y ou\\ncan test the JDBC server with the following Beeline script that comes with either\\nSpark or Hive 1.2.1.\\n120 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 143}), Document(page_content='Start the Thrift server\\nTo start the Spark Thrift JDBC/ODBC server, execute the following command from\\nthe $SPARK_HOME  folder:\\n./sbin/start-thriftserver.sh\\nIf you have not already started your Spark driver and worker, exe‐\\ncute the following command prior to start-thriftserver.sh :\\n./sbin/start-all.sh\\nConnect to the Thrift server via Beeline\\nTo test the Thrift JDBC/ODBC server using Beeline, execute the following command:\\n./bin/beeline\\nThen configure Beeline to connect to the local Thrift server:\\n!connect jdbc:hive2://localhost:10000\\nBy default, Beeline is in non-secure mode . Thus, the username is\\nyour login (e.g., user@learningspark.org ) and the password is\\nblank.\\nExecute a Spark SQL query with Beeline\\nFrom here, you can run a Spark SQL query similar to how you would run a Hive\\nquery with Beeline. Here are a few sample queries and their output:\\n0: jdbc:hive2://localhost:10000> SHOW tables;\\n+-----------+------------+--------------+\\n| database  | tableName  | isTemporary  |\\n+-----------+------------+--------------+\\n| default   | people     | false        |\\n+-----------+------------+--------------+\\n1 row selected (0.417 seconds)\\n0: jdbc:hive2://localhost:10000> SELECT * FROM people;\\n+-----------+-------+\\n|   name    |  age  |\\n+-----------+-------+\\n| Samantha  | 19    |\\n| Andy      | 30    |\\n| Michael   | NULL  |\\n+-----------+-------+\\nQuerying with the Spark SQL Shell, Beeline, and Tableau | 121', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 144}), Document(page_content='3 rows selected (1.512 seconds)\\n0: jdbc:hive2://localhost:10000>\\nStop the Thrift server\\nOnce you’re done, you can stop the Thrift server with the following command:\\n./sbin/stop-thriftserver.sh\\nWorking with Tableau\\nSimilar to running queries through Beeline or the Spark SQL CLI, you can connect\\nyour favorite BI tool to Spark SQL via the Thrift JDBC/ODBC server. In this section,\\nwe will show you how to connect Tableau Desktop (version 2019.2) to your local\\nApache Spark instance.\\nY ou will need to have the Tableau’s Spark ODBC  driver version\\n1.2.0 or above already installed. If you have installed (or upgraded\\nto) Tableau 2018.1 or greater, this driver should already be\\npreinstalled.\\nStart the Thrift server\\nTo start the Spark Thrift JDBC/ODBC server, execute the following command from\\nthe $SPARK_HOME  folder:\\n./sbin/start-thriftserver.sh\\nIf you have not already started your Spark driver and worker, exe‐\\ncute the following command prior to start-thriftserver.sh :\\n./sbin/start-all.sh\\n122 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 145}), Document(page_content='Start Tableau\\nIf you are starting Tableau for the first time, you will be greeted with a Connect dialog\\nthat allows you to connect to a plethora of data sources. By default, the Spark SQL\\noption will not be included in the “To a Server” menu on the left (see Figure 5-2 ).\\nFigure 5-2. Tableau Connect dialog box\\nQuerying with the Spark SQL Shell, Beeline, and Tableau | 123', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 146}), Document(page_content='To access the Spark SQL option, click More… at the bottom of that list and then\\nchoose Spark SQL from the list that appears in the main panel, as shown in\\nFigure 5-3 .\\nFigure 5-3. Choose More… > Spark SQL to connect to Spark SQL\\n124 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 147}), Document(page_content='This will pop up the Spark SQL dialog ( Figure 5-4 ). As you’re connecting to a local\\nApache Spark instance, you can use the non-secure username authentication mode\\nwith the following parameters:\\n•Server: localhost\\n•Port: 10000 (default)\\n•Type: SparkThriftServer (default)\\n•Authentication: Username\\n•Username: Y our login, e.g., user@learningspark.org\\n•Require SSL: Not checked\\nFigure 5-4. The Spark SQL dialog box\\nQuerying with the Spark SQL Shell, Beeline, and Tableau | 125', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 148}), Document(page_content='Once you have successfully connected to the Spark SQL data source, you will see a\\nData Source Connections view similar to Figure 5-5 .\\nFigure 5-5. Tableau Data Source Connections view, connected to a local Spark instance\\n126 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 149}), Document(page_content='From the Select Schema drop-down menu on the left, choose “default. ” Then enter\\nthe name of the table you want to query (see Figure 5-6 ). Note that you can click the\\nmagnifying glass icon to get a full list of the tables that are available.\\nFigure 5-6. Select a schema and a table to query\\nFor more information on using Tableau to connect to a Spark SQL\\ndatabase, refer to Tableau’s Spark SQL documentation  and the\\nDatabricks Tableau documentation .\\nQuerying with the Spark SQL Shell, Beeline, and Tableau | 127', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 150}), Document(page_content='Enter people  as the table name, then drag and drop the table from the left side into\\nthe main dialog (in the space marked “Drag tables here”). Y ou should see something\\nlike Figure 5-7 .\\nFigure 5-7. Connecting to the people table in your local Spark instance\\nClick Update Now, and under the covers Tableau will query your Spark SQL data\\nsource ( Figure 5-8 ).\\nY ou can now execute queries against your Spark data source, join tables, and more,\\njust like with any other Tableau data source.\\n128 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 151}), Document(page_content='Figure 5-8. Tableau worksheet table view querying a local Spark data source\\nStop the Thrift server\\nOnce you’re done, you can stop the Thrift server with the following command:\\n./sbin/stop-thriftserver.sh\\nExternal Data Sources\\nIn this section, we will focus on how to use Spark SQL to connect to external data\\nsources, starting with JDBC and SQL databases.\\nJDBC and SQL Databases\\nSpark SQL includes a data source API that can read data from other databases using\\nJDBC . It simplifies querying these data sources as it returns the results as a Data‐\\nFrame, thus providing all of the benefits of Spark SQL (including performance and\\nthe ability to join with other data sources).\\nTo get started, you will need to specify the JDBC driver for your JDBC data source\\nand it will need to be on the Spark classpath. From the $SPARK_HOME  folder, you’ll\\nissue a command like the following:\\nExternal Data Sources | 129', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 152}), Document(page_content='./bin/spark-shell --driver-class-path $database.jar --jars $database.jar\\nUsing the data source API, the tables from the remote database can be loaded as a\\nDataFrame or Spark SQL temporary view. Users can specify the JDBC connection\\nproperties in the data source options. Table 5-1  contains some of the more common\\nconnection properties (case-insensitive) that Spark supports.\\nTable 5-1. Common connection properties\\nProperty\\nnameDescription\\nuser , pass\\nwordThese are normally provided as connection properties for logging into the data sources.\\nurl JDBC connection URL, e.g., jdbc:postgresql://localhost/test?user=fred&pass\\nword=secret .\\ndbtable JDBC table to read from or write to. You can’t specify the dbtable  and query  options at the same time.\\nquery Query to be used to read data from Apache Spark, e.g., SELECT column1, column2, ..., col\\numnN FROM [table|subquery] . You can’t specify the query  and dbtable  options at the same\\ntime.\\ndriver Class name of the JDBC driver to use to connect to the specified  URL.\\nFor the full list of connection properties, see the Spark SQL documentation .\\nThe importance of partitioning\\nWhen transferring large amounts of data between Spark SQL and a JDBC external\\nsource, it is important to partition your data source. All of your data is going through\\none driver connection, which can saturate and significantly slow down the perfor‐\\nmance of your extraction, as well as potentially saturate the resources of your source\\nsystem. While these JDBC properties are optional, for any large-scale operations it is\\nhighly recommended to use the properties shown in Table 5-2 .\\nTable 5-2. Partitioning connection properties\\nProperty name Description\\nnumPartitions The maximum number of partitions that can be used for parallelism in table reading and\\nwriting. This also determines the maximum number of concurrent JDBC connections.\\npartitionColumn When reading an external source, partitionColumn  is the column that is used to\\ndetermine the partitions; note, partitionColumn  must be a numeric, date, or\\ntimestamp column.\\nlowerBound Sets the minimum value of partitionColumn  for the partition stride.\\nupperBound Sets the maximum value of partitionColumn  for the partition stride.\\n130 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 153}), Document(page_content='Let’s take a look at an example  to help you understand how these properties work.\\nSuppose we use the following settings:\\n•numPartitions : 10\\n•lowerBound : 1000\\n•upperBound : 10000\\nThen the stride is equal to 1,000, and 10 partitions will be created. This is the equiva‐\\nlent of executing these 10 queries (one for each partition):\\n•SELECT * FROM table WHERE partitionColumn BETWEEN 1000 and 2000\\n•SELECT * FROM table WHERE partitionColumn BETWEEN 2000 and 3000\\n•...\\n•SELECT * FROM table WHERE partitionColumn BETWEEN 9000 and 10000\\nWhile not all-encompassing, the following are some hints to keep in mind when\\nusing these properties:\\n•A good starting point for numPartitions  is to use a multiple of the number of\\nSpark workers. For example, if you have four Spark worker nodes, then perhaps\\nstart with 4 or 8 partitions. But it is also important to note how well your source\\nsystem can handle the read requests. For systems that have processing windows,\\nyou can maximize the number of concurrent requests to the source system; for\\nsystems lacking processing windows (e.g., an OLTP system continuously process‐\\ning data), you should reduce the number of concurrent requests to prevent satu‐\\nration of the source system.\\n•Initially, calculate the lowerBound  and upperBound  based on the minimum and\\nmaximum partitionColumn  actual  values. For example, if you choose\\n{numPartitions:10, lowerBound: 1000, upperBound: 10000} , but all of the\\nvalues are between 2000  and 4000 , then only 2 of the 10 queries (one for each\\npartition) will be doing all of the work. In this scenario, a better configuration\\nwould be {numPartitions:10, lowerBound: 2000, upperBound: 4000} .\\n•Choose a partitionColumn  that can be uniformly distributed to avoid data skew.\\nFor example, if the majority of your partitionColumn  has the value 2500 , with\\n{numPartitions:10, lowerBound: 1000, upperBound: 10000}  most of the\\nwork will be performed by the task requesting the values between 2000  and 3000 .\\nInstead, choose a different partitionColumn , or if possible generate a new one\\n(perhaps a hash of multiple columns) to more evenly distribute your partitions.\\nExternal Data Sources | 131', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 154}), Document(page_content='PostgreSQL\\nTo connect to a PostgreSQL database, build or download the JDBC jar from Maven\\nand add it to your classpath. Then start a Spark shell ( spark-shell  or pyspark ), spec‐\\nifying that jar:\\nbin/spark-shell --jars postgresql-42.2.6.jar\\nThe following examples show how to load from and save to a PostgreSQL database\\nusing the Spark SQL data source API and JDBC in Scala:\\n// In Scala\\n// Read Option 1: Loading data from a JDBC source using load method\\nval jdbcDF1 = spark\\n  .read\\n  .format(\"jdbc\")\\n  .option(\"url\", \"jdbc:postgresql:[DBSERVER]\" )\\n  .option(\"dbtable\" , \"[SCHEMA].[TABLENAME]\" )\\n  .option(\"user\", \"[USERNAME]\" )\\n  .option(\"password\" , \"[PASSWORD]\" )\\n  .load()\\n// Read Option 2: Loading data from a JDBC source using jdbc method\\n// Create connection properties\\nimport java.util.Properties\\nval cxnProp = new Properties ()\\ncxnProp.put(\"user\", \"[USERNAME]\" ) \\ncxnProp.put(\"password\" , \"[PASSWORD]\" )\\n// Load data using the connection properties\\nval jdbcDF2 = spark\\n  .read\\n  .jdbc(\"jdbc:postgresql:[DBSERVER]\" , \"[SCHEMA].[TABLENAME]\" , cxnProp)\\n// Write Option 1: Saving data to a JDBC source using save method\\njdbcDF1\\n  .write\\n  .format(\"jdbc\")\\n  .option(\"url\", \"jdbc:postgresql:[DBSERVER]\" )\\n  .option(\"dbtable\" , \"[SCHEMA].[TABLENAME]\" )\\n  .option(\"user\", \"[USERNAME]\" )\\n  .option(\"password\" , \"[PASSWORD]\" )\\n  .save()\\n// Write Option 2: Saving data to a JDBC source using jdbc method\\njdbcDF2.write\\n  .jdbc(s\"jdbc:postgresql:[DBSERVER]\" , \"[SCHEMA].[TABLENAME]\" , cxnProp)\\nAnd here’s how to do it in PySpark:\\n# In Python\\n# Read Option 1: Loading data from a JDBC source using load method\\njdbcDF1 = (spark\\n132 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 155}), Document(page_content='  .read\\n  .format(\"jdbc\") \\n  .option(\"url\", \"jdbc:postgresql://[DBSERVER]\" )\\n  .option(\"dbtable\" , \"[SCHEMA].[TABLENAME]\" )\\n  .option(\"user\", \"[USERNAME]\" )\\n  .option(\"password\" , \"[PASSWORD]\" )\\n  .load())\\n# Read Option 2: Loading data from a JDBC source using jdbc method\\njdbcDF2 = (spark\\n  .read \\n  .jdbc(\"jdbc:postgresql://[DBSERVER]\" , \"[SCHEMA].[TABLENAME]\" ,\\n          properties ={\"user\": \"[USERNAME]\" , \"password\" : \"[PASSWORD]\" }))\\n# Write Option 1: Saving data to a JDBC source using save method\\n(jdbcDF1\\n  .write\\n  .format(\"jdbc\")\\n  .option(\"url\", \"jdbc:postgresql://[DBSERVER]\" )\\n  .option(\"dbtable\" , \"[SCHEMA].[TABLENAME]\" ) \\n  .option(\"user\", \"[USERNAME]\" )\\n  .option(\"password\" , \"[PASSWORD]\" )\\n  .save())\\n# Write Option 2: Saving data to a JDBC source using jdbc method\\n(jdbcDF2\\n  .write \\n  .jdbc(\"jdbc:postgresql:[DBSERVER]\" , \"[SCHEMA].[TABLENAME]\" ,\\n          properties ={\"user\": \"[USERNAME]\" , \"password\" : \"[PASSWORD]\" }))\\nMySQL\\nTo connect to a MySQL database, build or download the JDBC jar from Maven  or\\nMySQL  (the latter is easier!) and add it to your classpath. Then start a Spark shell\\n(spark-shell  or pyspark ), specifying that jar:\\nbin/spark-shell --jars mysql-connector-java_8.0.16-bin.jar\\nThe following examples show how to load data from and save it to a MySQL database\\nusing the Spark SQL data source API and JDBC in Scala:\\n// In Scala\\n// Loading data from a JDBC source using load \\nval jdbcDF = spark\\n  .read\\n  .format(\"jdbc\")\\n  .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\" )\\n  .option(\"driver\" , \"com.mysql.jdbc.Driver\" )\\n  .option(\"dbtable\" , \"[TABLENAME]\" )\\n  .option(\"user\", \"[USERNAME]\" )\\n  .option(\"password\" , \"[PASSWORD]\" )\\n  .load()\\nExternal Data Sources | 133', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 156}), Document(page_content='// Saving data to a JDBC source using save \\njdbcDF\\n  .write\\n  .format(\"jdbc\")\\n  .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\" )\\n  .option(\"driver\" , \"com.mysql.jdbc.Driver\" )\\n  .option(\"dbtable\" , \"[TABLENAME]\" )\\n  .option(\"user\", \"[USERNAME]\" )\\n  .option(\"password\" , \"[PASSWORD]\" )\\n  .save()\\nAnd here’s how to do it in Python:\\n# In Python\\n# Loading data from a JDBC source using load \\njdbcDF = (spark\\n  .read\\n  .format(\"jdbc\")\\n  .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\" )\\n  .option(\"driver\" , \"com.mysql.jdbc.Driver\" ) \\n  .option(\"dbtable\" , \"[TABLENAME]\" )\\n  .option(\"user\", \"[USERNAME]\" )\\n  .option(\"password\" , \"[PASSWORD]\" )\\n  .load())\\n# Saving data to a JDBC source using save \\n(jdbcDF\\n  .write \\n  .format(\"jdbc\") \\n  .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\" )\\n  .option(\"driver\" , \"com.mysql.jdbc.Driver\" ) \\n  .option(\"dbtable\" , \"[TABLENAME]\" ) \\n  .option(\"user\", \"[USERNAME]\" )\\n  .option(\"password\" , \"[PASSWORD]\" )\\n  .save())\\nAzure Cosmos DB\\nTo connect to an Azure Cosmos DB database, build or download the JDBC jar from\\nMaven  or GitHub  and add it to your classpath. Then start a Scala or PySpark shell,\\nspecifying this jar (note that this example is using Spark 2.4):\\nbin/spark-shell --jars azure-cosmosdb-spark_2.4.0_2.11-1.3.5-uber.jar\\nY ou also have the option of using --packages  to pull the connector from Spark Pack‐\\nages using its Maven coordinates:\\nexport PKG=\"com.microsoft.azure:azure-cosmosdb-spark_2.4.0_2.11:1.3.5\"\\nbin/spark-shell --packages $PKG\\n134 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 157}), Document(page_content='The following examples show how to load data from and save it to an Azure Cosmos\\nDB database using the Spark SQL data source API and JDBC in Scala and PySpark.\\nNote that it is common to use the query_custom  configuration to make use of the\\nvarious indexes within Cosmos DB:\\n// In Scala\\n// Import necessary libraries\\nimport com.microsoft.azure.cosmosdb.spark.schema._\\nimport com.microsoft.azure.cosmosdb.spark._\\nimport com.microsoft.azure.cosmosdb.spark.config.Config\\n// Loading data from Azure Cosmos DB\\n// Configure connection to your collection\\nval query = \"SELECT c.colA, c.coln FROM c WHERE c.origin = \\'SEA\\'\"\\nval readConfig  = Config(Map(\\n  \"Endpoint\"  -> \"https://[ACCOUNT].documents.azure.com:443/\" , \\n  \"Masterkey\"  -> \"[MASTER KEY]\" ,\\n  \"Database\"  -> \"[DATABASE]\" ,\\n  \"PreferredRegions\"  -> \"Central US;East US2;\" ,\\n  \"Collection\"  -> \"[COLLECTION]\" ,\\n  \"SamplingRatio\"  -> \"1.0\",\\n  \"query_custom\"  -> query\\n))\\n// Connect via azure-cosmosdb-spark to create Spark DataFrame\\nval df = spark.read.cosmosDB (readConfig )\\ndf.count\\n// Saving data to Azure Cosmos DB\\n// Configure connection to the sink collection\\nval writeConfig  = Config(Map(\\n  \"Endpoint\"  -> \"https://[ACCOUNT].documents.azure.com:443/\" ,\\n  \"Masterkey\"  -> \"[MASTER KEY]\" ,\\n  \"Database\"  -> \"[DATABASE]\" ,\\n  \"PreferredRegions\"  -> \"Central US;East US2;\" ,\\n  \"Collection\"  -> \"[COLLECTION]\" ,\\n  \"WritingBatchSize\"  -> \"100\"\\n))\\n// Upsert the DataFrame to Azure Cosmos DB\\nimport org.apache.spark.sql.SaveMode\\ndf.write.mode(SaveMode .Overwrite ).cosmosDB (writeConfig )\\n# In Python\\n# Loading data from Azure Cosmos DB\\n# Read configuration\\nquery = \"SELECT c.colA, c.coln FROM c WHERE c.origin = \\'SEA\\'\"\\nreadConfig  = {\\n  \"Endpoint\"  : \"https://[ACCOUNT].documents.azure.com:443/\" , \\n  \"Masterkey\"  : \"[MASTER KEY]\" ,\\n  \"Database\"  : \"[DATABASE]\" ,\\n  \"preferredRegions\"  : \"Central US;East US2\" ,\\nExternal Data Sources | 135', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 158}), Document(page_content='  \"Collection\"  : \"[COLLECTION]\" ,\\n  \"SamplingRatio\"  : \"1.0\",\\n  \"schema_samplesize\"  : \"1000\",\\n  \"query_pagesize\"  : \"2147483647\" ,\\n  \"query_custom\"  : query\\n}\\n# Connect via azure-cosmosdb-spark to create Spark DataFrame\\ndf = (spark\\n  .read\\n  .format(\"com.microsoft.azure.cosmosdb.spark\" )\\n  .options(**readConfig )\\n  .load())\\n# Count the number of flights\\ndf.count()\\n# Saving data to Azure Cosmos DB\\n# Write configuration\\nwriteConfig  = {\\n \"Endpoint\"  : \"https://[ACCOUNT].documents.azure.com:443/\" ,\\n \"Masterkey\"  : \"[MASTER KEY]\" ,\\n \"Database\"  : \"[DATABASE]\" ,\\n \"Collection\"  : \"[COLLECTION]\" ,\\n \"Upsert\"  : \"true\"\\n}\\n# Upsert the DataFrame to Azure Cosmos DB\\n(df.write\\n  .format(\"com.microsoft.azure.cosmosdb.spark\" )\\n  .options(**writeConfig )\\n  .save())\\nFor more information, please refer to the Azure Cosmos DB documentation .\\nMS SQL Server\\nTo connect to an MS SQL Server database, download the JDBC jar  and add it to your\\nclasspath. Then start a Scala or PySpark shell, specifying this jar:\\nbin/spark-shell --jars mssql-jdbc-7.2.2.jre8.jar\\nThe following examples show how to load data from and save it to an MS SQL Server\\ndatabase using the Spark SQL data source API and JDBC in Scala and PySpark:\\n136 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 159}), Document(page_content='// In Scala\\n// Loading data from a JDBC source\\n// Configure jdbcUrl\\nval jdbcUrl = \"jdbc:sqlserver://[DBSERVER]:1433;database=[DATABASE]\"\\n// Create a Properties() object to hold the parameters. \\n// Note, you can create the JDBC URL without passing in the\\n// user/password parameters directly.\\nval cxnProp = new Properties ()\\ncxnProp.put(\"user\", \"[USERNAME]\" ) \\ncxnProp.put(\"password\" , \"[PASSWORD]\" ) \\ncxnProp.put(\"driver\" , \"com.microsoft.sqlserver.jdbc.SQLServerDriver\" )\\n// Load data using the connection properties\\nval jdbcDF = spark.read.jdbc(jdbcUrl, \"[TABLENAME]\" , cxnProp)\\n// Saving data to a JDBC source\\njdbcDF.write.jdbc(jdbcUrl, \"[TABLENAME]\" , cxnProp)\\n# In Python\\n# Configure jdbcUrl\\njdbcUrl = \"jdbc:sqlserver://[DBSERVER]:1433;database=[DATABASE]\"\\n# Loading data from a JDBC source\\njdbcDF = (spark\\n  .read\\n  .format(\"jdbc\") \\n  .option(\"url\", jdbcUrl)\\n  .option(\"dbtable\" , \"[TABLENAME]\" )\\n  .option(\"user\", \"[USERNAME]\" )\\n  .option(\"password\" , \"[PASSWORD]\" )\\n  .load())\\n# Saving data to a JDBC source\\n(jdbcDF\\n  .write\\n  .format(\"jdbc\") \\n  .option(\"url\", jdbcUrl)\\n  .option(\"dbtable\" , \"[TABLENAME]\" )\\n  .option(\"user\", \"[USERNAME]\" )\\n  .option(\"password\" , \"[PASSWORD]\" )\\n  .save())\\nOther External Sources\\nThere are just some of the many external data sources Apache Spark can connect to;\\nother popular data sources include:\\n•Apache Cassandra\\n•Snowflake\\n•MongoDB\\nExternal Data Sources | 137', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 160}), Document(page_content='Higher-Order Functions in DataFrames and Spark SQL\\nBecause complex data types are amalgamations of simple data types, it is tempting to\\nmanipulate them directly. There are two typical solutions  for manipulating complex\\ndata types:\\n•Exploding the nested structure into individual rows, applying some function, and\\nthen re-creating the nested structure\\n•Building a user-defined function\\nThese approaches have the benefit of allowing you to think of the problem in tabular\\nformat. They typically involve (but are not limited to) using utility functions  such as\\nget_json_object() , from_json() , to_json() , explode() , and selectExpr() .\\nLet’s take a closer look at these two options.\\nOption 1: Explode and Collect\\nIn this nested SQL statement, we first explode(values) , which creates a new row\\n(with the id) for each element ( value ) within values :\\n-- In SQL\\nSELECT id, collect_list (value + 1) AS values\\nFROM  (SELECT id, EXPLODE(values) AS value\\n        FROM table) x\\nGROUP BY id\\nWhile collect_list()  returns a list of objects with duplicates, the GROUP BY  state‐\\nment requires shuffle operations, meaning the order of the re-collected array isn’t\\nnecessarily the same as that of the original array. As values  could be any number of\\ndimensions (a really wide and/or really long array) and we’re doing a GROUP BY , this\\napproach could be very expensive.\\nOption 2: User-Defined  Function\\nTo perform the same task (adding 1 to each element in values ), we can also create a\\nUDF that uses map()  to iterate through each element ( value ) and perform the addi‐\\ntion operation:\\n-- In SQL\\nSELECT id, collect_list (value + 1) AS values\\nFROM  (SELECT id, EXPLODE(values) AS value\\n        FROM table) x\\nGROUP BY id\\n138 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 161}), Document(page_content='We could then use this UDF in Spark SQL as follows:\\nspark.sql(\"SELECT id, plusOneInt(values) AS values FROM table\").show()\\nWhile this is better than using explode()  and collect_list()  as there won’t be any\\nordering issues, the serialization and deserialization process itself may be expensive.\\nIt’s also important to note, however, that collect_list()  may cause executors to\\nexperience out-of-memory issues for large data sets, whereas using UDFs would alle‐\\nviate these issues.\\nBuilt-in Functions for Complex Data Types\\nInstead of using these potentially expensive techniques, you may be able to use some\\nof the built-in functions for complex data types included as part of Apache Spark 2.4\\nand later. Some of the more common ones are listed in Table 5-3  (array types) and\\nTable 5-4  (map types); for the full list refer to this notebook  in the Databricks\\ndocumentation.\\nTable 5-3. Array type functions\\nFunction/Description Query Output\\narray_distinct(array<T>): \\narray<T>\\nRemoves duplicates within an arraySELECT array_distinct(array(1, \\n2, 3, null, 3));[1,2,3,null]\\narray_intersect(array<T>, \\narray<T>): array<T>\\nReturns the intersection of two arrays\\nwithout duplicatesSELECT array_inter\\nsect(array(1, 2, 3), array(1, \\n3, 5));[1,3]\\narray_union(array<T>, \\narray<T>): array<T>\\nReturns the union of two arrays\\nwithout duplicatesSELECT array_union(array(1, 2, \\n3), array(1, 3, 5));[1,2,3,5]\\narray_except(array<T>, \\narray<T>): array<T>\\nReturns elements in array1  but not\\nin array2 , without duplicatesSELECT array_except(array(1, \\n2, 3), array(1, 3, 5));[2]\\narray_join(array<String>, \\nString[, String]): String\\nConcatenates the elements of an array\\nusing a delimiterSELECT \\narray_join(array(\\'hello\\', \\n\\'world\\'), \\' \\');hello world\\narray_max(array<T>): T\\nReturns the maximum value within the\\narray; null  elements are skippedSELECT array_max(array(1, 20, \\nnull, 3));20\\narray_min(array<T>): T\\nReturns the minimum value within the\\narray; null  elements are skippedSELECT array_min(array(1, 20, \\nnull, 3));1\\nHigher-Order Functions in DataFrames and Spark SQL | 139', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 162}), Document(page_content='Function/Description Query Output\\narray_position(array<T>, \\nT): Long\\nReturns the (1-based) index of the first\\nelement of the given array as a LongSELECT array_position(array(3, \\n2, 1), 1);3\\narray_remove(array<T>, \\nT): array<T>\\nRemoves all elements that are equal to\\nthe given element from the given arraySELECT array_remove(array(1, \\n2, 3, null, 3), 3);[1,2,null]\\narrays_overlap(array<T>, \\narray<T>): array<T>\\nReturns true  if array1  contains at\\nleast one non- null  element also\\npresent in array2SELECT arrays_overlap(array(1, \\n2, 3), array(3, 4, 5));true\\narray_sort(array<T>): \\narray<T>\\nSorts the input array in ascending\\norder, with null elements placed at the\\nend of the arraySELECT array_sort(array(\\'b\\', \\n\\'d\\', null, \\'c\\', \\'a\\'));[\"a\",\"b\",\"c\",\"d\",null]\\nconcat(array<T>, ...): \\narray<T>\\nConcatenates strings, binaries, arrays,\\netc.SELECT concat(array(1, 2, 3), \\narray(4, 5), array(6));[1,2,3,4,5,6]\\nflatten(array<array<T>>): \\narray<T>\\nFlattens an array of arrays into a single\\narraySELECT flatten(array(array(1, \\n2), array(3, 4)));[1,2,3,4]\\narray_repeat(T, Int): \\narray<T>\\nReturns an array containing the\\nspecified  element the specified  number\\nof timesSELECT array_repeat(\\'123\\', 3); [\"123\",\"123\",\"123\"]\\nreverse(array<T>): \\narray<T>\\nReturns a reversed string or an array\\nwith the reverse order of elementsSELECT reverse(array(2, 1, 4, \\n3));[3,4,1,2]\\nsequence(T, T[, T]): \\narray<T>\\nGenerates an array of elements from\\nstart to stop (inclusive) by incremental\\nstepSELECT sequence(1, 5);\\nSELECT sequence(5, 1);\\nSELECT \\nsequence(to_date(\\'2018-01-01\\'), \\nto_date(\\'2018-03-01\\'), inter\\nval 1 month);[1,2,3,4,5]\\n[5,4,3,2,1]\\n[\"2018-01-01\", \\n\"2018-02-01\", \\n\"2018-03-01\"]\\nshuffle(array<T>): \\narray<T>\\nReturns a random permutation of the\\ngiven arraySELECT shuffle(array(1, 20, \\nnull, 3));[null,3,20,1]\\n140 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 163}), Document(page_content='Function/Description Query Output\\nslice(array<T>, Int, \\nInt): array<T>\\nReturns a subset of the given array\\nstarting from the given index (counting\\nfrom the end if the index is negative),\\nof the specified  lengthSELECT slice(array(1, 2, 3, \\n4), -2, 2);[3,4]\\narray_zip(array<T>, \\narray<U>, ...): \\narray<struct<T, U, ...>>\\nReturns a merged array of structsSELECT arrays_zip(array(1, 2), \\narray(2, 3), array(3, 4));[{\"0\":1,\"1\":2,\"2\":3},\\n{\"0\":2,\"1\":3,\"2\":4}]\\nelement_at(array<T>, \\nInt): T /\\nReturns the element of the given array\\nat the given (1-based) indexSELECT element_at(array(1, 2, \\n3), 2);2\\ncardinality(array<T>): Int\\nAn alias of size ; returns the size of\\nthe given array or a mapSELECT cardinality(array(\\'b\\', \\n\\'d\\', \\'c\\', \\'a\\'));4\\nTable 5-4. Map functions\\nFunction/Description Query Output\\nmap_form_arrays(array<K>, \\narray<V>): map<K, V>\\nCreates a map from the given pair of key/value\\narrays; elements in keys should not be nullSELECT \\nmap_from_arrays(array(1.0, \\n3.0), array(\\'2\\', \\'4\\'));{\"1.0\":\"2\", \\n\"3.0\":\"4\"}\\nmap_from_entries(array<struct<K, \\nV>>): map<K, V>\\nReturns a map created from the given arraySELECT \\nmap_from_entries(array(struct(1, \\n\\'a\\'), struct(2, \\'b\\')));{\"1\":\"a\", \\n\"2\":\"b\"}\\nmap_concat(map<K, V>, ...): \\nmap<K, V>\\nReturns the union of the input mapsSELECT map_concat(map(1, \\'a\\', \\n2, \\'b\\'), map(2, \\'c\\', 3, \\'d\\'));{\"1\":\"a\", \\n\"2\":\"c\",\"3\":\"d\"}\\nelement_at(map<K, V>, K): V\\nReturns the value of the given key, or null  if\\nthe key is not contained in the mapSELECT element_at(map(1, \\'a\\', \\n2, \\'b\\'), 2);b\\ncardinality(array<T>): Int\\nAn alias of size ; returns the size of the given\\narray or a mapSELECT cardinality(map(1, \\'a\\', \\n2, \\'b\\'));2\\nHigher-Order Functions\\nIn addition to the previously noted built-in functions, there are higher-order func‐\\ntions that take anonymous lambda functions as arguments. An example of a higher-\\norder function is the following:\\nHigher-Order Functions in DataFrames and Spark SQL | 141', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 164}), Document(page_content='-- In SQL\\ntransform (values, value -> lambda expression )\\nThe transform()  function takes an array ( values ) and anonymous function ( lambda\\nexpression) as input. The function transparently creates a new array by applying the\\nanonymous function to each element, and then assigning the result to the output\\narray (similar to the UDF approach, but more efficiently).\\nLet’s create a sample data set so we can run some examples:\\n# In Python\\nfrom pyspark.sql.types  import *\\nschema = StructType ([StructField (\"celsius\" , ArrayType (IntegerType ()))])\\nt_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\\nt_c = spark.createDataFrame (t_list, schema)\\nt_c.createOrReplaceTempView (\"tC\")\\n# Show the DataFrame\\nt_c.show()\\n// In Scala\\n// Create DataFrame with two rows of two arrays (tempc1, tempc2)\\nval t1 = Array(35, 36, 32, 30, 40, 42, 38)\\nval t2 = Array(31, 32, 34, 55, 56)\\nval tC = Seq(t1, t2).toDF(\"celsius\" )\\ntC.createOrReplaceTempView (\"tC\")\\n// Show the DataFrame\\ntC.show()\\nHere’s the output:\\n+--------------------+\\n|             celsius|\\n+--------------------+\\n|[35, 36, 32, 30, ...|\\n|[31, 32, 34, 55, 56]|\\n+--------------------+\\nWith the preceding DataFrame you can run the following higher-order function\\nqueries.\\ntransform()\\ntransform(array<T>, function<T, U>): array<U>\\nThe transform()  function produces an array by applying a function to each element\\nof the input array (similar to a map()  function):\\n// In Scala/Python\\n// Calculate Fahrenheit from Celsius for an array of temperatures\\nspark.sql(\"\"\"\\nSELECT celsius, \\n142 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 165}), Document(page_content=' transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit \\n  FROM tC\\n\"\"\").show()\\n+--------------------+--------------------+\\n|             celsius|          fahrenheit |\\n+--------------------+--------------------+\\n|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\\n|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\\n+--------------------+--------------------+\\nfilter()\\nfilter(array<T>, function<T, Boolean>): array<T>\\nThe filter()  function produces an array consisting of only the elements of the input\\narray for which the Boolean function is true :\\n// In Scala/Python\\n// Filter temperatures > 38C for array of temperatures\\nspark.sql(\"\"\"\\nSELECT celsius, \\n filter(celsius, t -> t > 38) as high \\n  FROM tC\\n\"\"\").show()\\n+--------------------+--------+\\n|             celsius|    high|\\n+--------------------+--------+\\n|[35, 36, 32, 30, ...|[40, 42]|\\n|[31, 32, 34, 55, 56]|[55, 56]|\\n+--------------------+--------+\\nexists()\\nexists(array<T>, function<T, V, Boolean>): Boolean\\nThe exists()  function returns true  if the Boolean function holds for any element in\\nthe input array:\\n// In Scala/Python\\n// Is there a temperature of 38C in the array of temperatures\\nspark.sql(\"\"\"\\nSELECT celsius, \\n       exists(celsius, t -> t = 38) as threshold\\n  FROM tC\\n\"\"\").show()\\n+--------------------+---------+\\n|             celsius|threshold |\\n+--------------------+---------+\\n|[35, 36, 32, 30, ...|     true|\\nHigher-Order Functions in DataFrames and Spark SQL | 143', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 166}), Document(page_content='|[31, 32, 34, 55, 56]|    false|\\n+--------------------+---------+\\nreduce()\\nreduce(array<T>, B, function<B, T, B>, function<B, R>)\\nThe reduce()  function reduces the elements of the array to a single value by merging\\nthe elements into a buffer B using function<B, T, B>  and applying a finishing\\nfunction<B, R>  on the final buffer:\\n// In Scala/Python\\n// Calculate average temperature and convert to F\\nspark.sql(\"\"\"\\nSELECT celsius, \\n       reduce(\\n          celsius, \\n          0, \\n          (t, acc) -> t + acc, \\n          acc -> (acc div size(celsius) * 9 div 5) + 32\\n        ) as avgFahrenheit \\n  FROM tC\\n\"\"\").show()\\n+--------------------+-------------+\\n|             celsius|avgFahrenheit |\\n+--------------------+-------------+\\n|[35, 36, 32, 30, ...|           96|\\n|[31, 32, 34, 55, 56]|          105|\\n+--------------------+-------------+\\nCommon DataFrames and Spark SQL Operations\\nPart of the power of Spark SQL comes from the wide range of DataFrame operations\\n(also known as untyped Dataset operations) it supports. The list of operations is quite\\nextensive and includes:\\n•Aggregate functions\\n•Collection functions\\n•Datetime functions\\n•Math functions\\n•Miscellaneous functions\\n•Non-aggregate functions\\n•Sorting functions\\n•String functions\\n144 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 167}), Document(page_content='•UDF functions\\n•Window functions\\nFor the full list, see the Spark SQL documentation .\\nWithin this chapter, we will focus on the following common relational operations:\\n•Unions and joins\\n•Windowing\\n•Modifications\\nTo perform these DataFrame operations, we’ll first prepare some data. In the follow‐\\ning code snippet, we:\\n1.Import two files and create two DataFrames, one for airport ( airportsna ) infor‐\\nmation and one for US flight delays ( departureDelays ).\\n2.Using expr() , convert the delay  and distance  columns from STRING  to INT.\\n3.Create a smaller table, foo, that we can focus on for our demo examples; it con‐\\ntains only information on three flights originating from Seattle (SEA) to the des‐\\ntination of San Francisco (SFO) for a small time range.\\nLet’s get started:\\n// In Scala\\nimport org.apache.spark.sql.functions._\\n// Set file paths\\nval delaysPath  = \\n  \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\\nval airportsPath  = \\n  \"/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\"\\n// Obtain airports data set\\nval airports  = spark.read\\n  .option(\"header\" , \"true\")\\n  .option(\"inferschema\" , \"true\")\\n  .option(\"delimiter\" , \"\\\\t\")\\n  .csv(airportsPath )\\nairports .createOrReplaceTempView (\"airports_na\" )\\n// Obtain departure Delays data set\\nval delays = spark.read\\n  .option(\"header\" ,\"true\")\\n  .csv(delaysPath )\\n  .withColumn (\"delay\", expr(\"CAST(delay as INT) as delay\" ))\\n  .withColumn (\"distance\" , expr(\"CAST(distance as INT) as distance\" ))\\ndelays.createOrReplaceTempView (\"departureDelays\" )\\nCommon DataFrames and Spark SQL Operations | 145', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 168}), Document(page_content='// Create temporary small table\\nval foo = delays.filter(\\n  expr(\"\"\"origin == \\'SEA\\' AND destination == \\'SFO\\' AND \\n      date like \\'01010%\\' AND delay > 0\"\"\" ))\\nfoo.createOrReplaceTempView (\"foo\")\\n# In Python\\n# Set file paths\\nfrom pyspark.sql.functions  import expr\\ntripdelaysFilePath  = \\n  \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\\nairportsnaFilePath  = \\n  \"/databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\"\\n  \\n# Obtain airports data set\\nairportsna  = (spark.read\\n  .format(\"csv\")\\n  .options(header=\"true\", inferSchema =\"true\", sep=\"\\\\t\")\\n  .load(airportsnaFilePath ))\\nairportsna .createOrReplaceTempView (\"airports_na\" )\\n# Obtain departure delays data set\\ndepartureDelays  = (spark.read\\n  .format(\"csv\")\\n  .options(header=\"true\")\\n  .load(tripdelaysFilePath ))\\ndepartureDelays  = (departureDelays\\n  .withColumn (\"delay\", expr(\"CAST(delay as INT) as delay\" ))\\n  .withColumn (\"distance\" , expr(\"CAST(distance as INT) as distance\" )))\\ndepartureDelays .createOrReplaceTempView (\"departureDelays\" )\\n# Create temporary small table\\nfoo = (departureDelays\\n  .filter(expr(\"\"\"origin == \\'SEA\\' and destination == \\'SFO\\' and \\n    date like \\'01010%\\' and delay > 0\"\"\" )))\\nfoo.createOrReplaceTempView (\"foo\")\\nThe departureDelays  DataFrame contains data on >1.3M flights while the foo Data‐\\nFrame contains just three rows with information on flights from SEA to SFO for a\\nspecific time range, as noted in the following output:\\n// Scala/Python\\nspark.sql(\"SELECT * FROM airports_na LIMIT 10\" ).show()\\n+-----------+-----+-------+----+\\n|       City|State|Country|IATA|\\n+-----------+-----+-------+----+\\n| Abbotsford |   BC| Canada| YXX|\\n|   Aberdeen |   SD|    USA| ABR|\\n146 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 169}), Document(page_content='|    Abilene|   TX|    USA| ABI|\\n|      Akron|   OH|    USA| CAK|\\n|    Alamosa|   CO|    USA| ALS|\\n|     Albany|   GA|    USA| ABY|\\n|     Albany|   NY|    USA| ALB|\\n|Albuquerque |   NM|    USA| ABQ|\\n| Alexandria |   LA|    USA| AEX|\\n|  Allentown |   PA|    USA| ABE|\\n+-----------+-----+-------+----+\\nspark.sql(\"SELECT * FROM departureDelays LIMIT 10\" ).show()\\n+--------+-----+--------+------+-----------+\\n|    date|delay|distance |origin|destination |\\n+--------+-----+--------+------+-----------+\\n|01011245 |    6|     602|   ABE|        ATL|\\n|01020600 |   -8|     369|   ABE|        DTW|\\n|01021245 |   -2|     602|   ABE|        ATL|\\n|01020605 |   -4|     602|   ABE|        ATL|\\n|01031245 |   -4|     602|   ABE|        ATL|\\n|01030605 |    0|     602|   ABE|        ATL|\\n|01041243 |   10|     602|   ABE|        ATL|\\n|01040605 |   28|     602|   ABE|        ATL|\\n|01051245 |   88|     602|   ABE|        ATL|\\n|01050605 |    9|     602|   ABE|        ATL|\\n+--------+-----+--------+------+-----------+\\nspark.sql(\"SELECT * FROM foo\" ).show()\\n+--------+-----+--------+------+-----------+\\n|    date|delay|distance |origin|destination |\\n+--------+-----+--------+------+-----------+\\n|01010710 |   31|     590|   SEA|        SFO|\\n|01010955 |  104|     590|   SEA|        SFO|\\n|01010730 |    5|     590|   SEA|        SFO|\\n+--------+-----+--------+------+-----------+\\nIn the following sections, we will execute union, join, and windowing examples with\\nthis data.\\nUnions\\nA common pattern within Apache Spark is to union two different DataFrames with\\nthe same schema together. This can be achieved using the union()  method:\\n// Scala\\n// Union two tables\\nval bar = delays.union(foo)\\nbar.createOrReplaceTempView (\"bar\")\\nbar.filter(expr(\"\"\"origin == \\'SEA\\' AND destination == \\'SFO\\'\\nAND date LIKE \\'01010%\\' AND delay > 0\"\"\" )).show()\\nCommon DataFrames and Spark SQL Operations | 147', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 170}), Document(page_content='# In Python\\n# Union two tables\\nbar = departureDelays .union(foo)\\nbar.createOrReplaceTempView (\"bar\")\\n# Show the union (filtering for SEA and SFO in a specific time range)\\nbar.filter(expr(\"\"\"origin == \\'SEA\\' AND destination == \\'SFO\\'\\nAND date LIKE \\'01010%\\' AND delay > 0\"\"\" )).show()\\nThe bar DataFrame is the union of foo with delays . Using the same filtering criteria\\nresults in the bar DataFrame, we see a duplication of the foo data, as expected:\\n-- In SQL\\nspark.sql(\"\"\"\\nSELECT * \\n  FROM bar \\n WHERE origin = \\'SEA\\' \\n   AND destination = \\'SFO\\' \\n   AND date LIKE \\'01010%\\' \\n   AND delay > 0\\n\"\"\").show()\\n+--------+-----+--------+------+-----------+\\n|    date|delay|distance |origin|destination |\\n+--------+-----+--------+------+-----------+\\n|01010710 |   31|     590|   SEA|        SFO|\\n|01010955 |  104|     590|   SEA|        SFO|\\n|01010730 |    5|     590|   SEA|        SFO|\\n|01010710 |   31|     590|   SEA|        SFO|\\n|01010955 |  104|     590|   SEA|        SFO|\\n|01010730 |    5|     590|   SEA|        SFO|\\n+--------+-----+--------+------+-----------+\\nJoins\\nA common DataFrame operation is to join two DataFrames (or tables) together. By\\ndefault, a Spark SQL join is an inner join , with the options being inner , cross ,\\nouter , full , full_outer , left , left_outer , right , right_outer , left_semi , and\\nleft_anti . More information is available in the documentation  (this is applicable to\\nScala as well as Python).\\nThe following code sample performs the default of an inner  join between the air\\nportsna  and foo DataFrames:\\n// In Scala\\nfoo.join(\\n  airports .as(\\'air), \\n  $\"air.IATA\"  === $\"origin\"\\n).select(\"City\", \"State\", \"date\", \"delay\", \"distance\" , \"destination\" ).show()\\n# In Python\\n# Join departure delays data (foo) with airport info\\n148 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 171}), Document(page_content='foo.join(\\n  airports , \\n  airports .IATA == foo.origin\\n).select(\"City\", \"State\", \"date\", \"delay\", \"distance\" , \"destination\" ).show()\\n-- In SQL\\nspark.sql(\"\"\"\\nSELECT a.City, a.State, f.date, f.delay, f.distance, f.destination \\n  FROM foo f\\n  JOIN airports_na a\\n    ON a.IATA = f.origin\\n\"\"\").show()\\nThe preceding code allows you to view the date, delay, distance, and destination\\ninformation from the foo DataFrame joined to the city and state information from\\nthe airports  DataFrame:\\n+-------+-----+--------+-----+--------+-----------+\\n|   City|State|    date|delay|distance|destination|\\n+-------+-----+--------+-----+--------+-----------+\\n|Seattle|   WA|01010710|   31|     590|        SFO|\\n|Seattle|   WA|01010955|  104|     590|        SFO|\\n|Seattle|   WA|01010730|    5|     590|        SFO|\\n+-------+-----+--------+-----+--------+-----------+\\nWindowing\\nA window function  uses values from the rows in a window (a range of input rows) to\\nreturn a set of values, typically in the form of another row. With window functions, it\\nis possible to operate on a group of rows while still returning a single value for every\\ninput row. In this section, we will show how to use the dense_rank()  window func‐\\ntion; there are many other functions, as noted in Table 5-5 .\\nTable 5-5. Window functions\\n SQL DataFrame API\\nRanking functions rank() rank()\\n dense_rank() denseRank()\\n percent_rank() percentRank()\\n ntile() ntile()\\n row_number() rowNumber()\\nAnalytic functions cume_dist() cumeDist()\\n first_value() firstValue()\\n last_value() lastValue()\\n lag() lag()\\n lead() lead()\\nCommon DataFrames and Spark SQL Operations | 149', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 172}), Document(page_content=\"Let’s start with a review of the TotalDelays  (calculated by sum(Delay) ) experienced\\nby flights originating from Seattle (SEA), San Francisco (SFO), and New Y ork City\\n(JFK) and going to a specific set of destination locations, as noted in the following\\nquery:\\n-- In SQL\\nDROP TABLE IF EXISTS departureDelaysWindow ;\\nCREATE TABLE departureDelaysWindow  AS\\nSELECT origin, destination , SUM(delay) AS TotalDelays  \\n  FROM departureDelays  \\n WHERE origin IN ('SEA', 'SFO', 'JFK') \\n   AND destination  IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL') \\n GROUP BY origin, destination ;\\nSELECT * FROM departureDelaysWindow\\n+------+-----------+-----------+\\n|origin|destination |TotalDelays |\\n+------+-----------+-----------+\\n|   JFK|        ORD|       5608|\\n|   SEA|        LAX|       9359|\\n|   JFK|        SFO|      35619|\\n|   SFO|        ORD|      27412|\\n|   JFK|        DEN|       4315|\\n|   SFO|        DEN|      18688|\\n|   SFO|        SEA|      17080|\\n|   SEA|        SFO|      22293|\\n|   JFK|        ATL|      12141|\\n|   SFO|        ATL|       5091|\\n|   SEA|        DEN|      13645|\\n|   SEA|        ATL|       4535|\\n|   SEA|        ORD|      10041|\\n|   JFK|        SEA|       7856|\\n|   JFK|        LAX|      35755|\\n|   SFO|        JFK|      24100|\\n|   SFO|        LAX|      40798|\\n|   SEA|        JFK|       4667|\\n+------+-----------+-----------+\\nWhat if for each of these origin airports you wanted to find the three destinations that\\nexperienced the most delays? Y ou could achieve this by running three different quer‐\\nies for each origin and then unioning the results together, like this:\\n-- In SQL\\nSELECT origin, destination , SUM(TotalDelays ) AS TotalDelays\\n FROM departureDelaysWindow\\nWHERE origin = '[ORIGIN]'\\nGROUP BY origin, destination\\nORDER BY SUM(TotalDelays ) DESC\\nLIMIT 3\\n150 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources\", metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 173}), Document(page_content='where [ORIGIN]  is the three different origin values of JFK, SEA, and SFO.\\nBut a better approach would be to use a window function like dense_rank()  to per‐\\nform the following calculation:\\n-- In SQL\\nspark.sql(\"\"\"\\nSELECT origin, destination, TotalDelays, rank \\n  FROM ( \\n     SELECT origin, destination, TotalDelays, dense_rank() \\n       OVER (PARTITION BY origin ORDER BY TotalDelays DESC) as rank \\n       FROM departureDelaysWindow\\n  ) t \\n WHERE rank <= 3\\n\"\"\").show()\\n+------+-----------+-----------+----+\\n|origin|destination |TotalDelays |rank|\\n+------+-----------+-----------+----+\\n|   SEA|        SFO|      22293|   1|\\n|   SEA|        DEN|      13645|   2|\\n|   SEA|        ORD|      10041|   3|\\n|   SFO|        LAX|      40798|   1|\\n|   SFO|        ORD|      27412|   2|\\n|   SFO|        JFK|      24100|   3|\\n|   JFK|        LAX|      35755|   1|\\n|   JFK|        SFO|      35619|   2|\\n|   JFK|        ATL|      12141|   3|\\n+------+-----------+-----------+----+\\nBy using the dense_rank()  window function, we can quickly ascertain that the desti‐\\nnations with the worst delays for the three origin cities were:\\n•Seattle (SEA): San Francisco (SFO), Denver (DEN), and Chicago (ORD)\\n•San Francisco (SFO): Los Angeles (LAX), Chicago (ORD), and New Y ork (JFK)\\n•New Y ork (JFK): Los Angeles (LAX), San Francisco (SFO), and Atlanta (ATL)\\nIt’s important to note that each window grouping needs to fit in a single executor and\\nwill get composed into a single partition during execution. Therefore, you need to\\nensure that your queries are not unbounded (i.e., limit the size of your window).\\nModifications\\nAnother common operation is to perform modifications  to the DataFrame. While\\nDataFrames themselves are immutable, you can modify them through operations that\\ncreate new, different DataFrames, with different columns, for example. (Recall from\\nearlier chapters that the underlying RDDs are immutable—i.e., they cannot be\\nCommon DataFrames and Spark SQL Operations | 151', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 174}), Document(page_content='changed—to ensure there is data lineage for Spark operations.) Let’s start with our\\nprevious small DataFrame example:\\n// In Scala/Python\\nfoo.show()\\n--------+-----+--------+------+-----------+\\n|    date|delay|distance |origin|destination |\\n+--------+-----+--------+------+-----------+\\n|01010710 |   31|     590|   SEA|        SFO|\\n|01010955 |  104|     590|   SEA|        SFO|\\n|01010730 |    5|     590|   SEA|        SFO|\\n+--------+-----+--------+------+-----------+\\nAdding new columns\\nTo add a new column to the foo DataFrame, use the withColumn()  method:\\n// In Scala\\nimport org.apache.spark.sql.functions.expr\\nval foo2 = foo.withColumn (\\n              \"status\" , \\n              expr(\"CASE WHEN delay <= 10 THEN \\'On-time\\' ELSE \\'Delayed\\' END\" )\\n           )\\n# In Python\\nfrom pyspark.sql.functions  import expr\\nfoo2 = (foo.withColumn (\\n          \"status\" , \\n          expr(\"CASE WHEN delay <= 10 THEN \\'On-time\\' ELSE \\'Delayed\\' END\" )\\n        ))\\nThe newly created foo2  DataFrame has the contents of the original foo DataFrame\\nplus the additional status  column defined by the CASE  statement:\\n// In Scala/Python\\nfoo2.show()\\n+--------+-----+--------+------+-----------+-------+\\n|    date|delay|distance |origin|destination | status|\\n+--------+-----+--------+------+-----------+-------+\\n|01010710 |   31|     590|   SEA|        SFO|Delayed|\\n|01010955 |  104|     590|   SEA|        SFO|Delayed|\\n|01010730 |    5|     590|   SEA|        SFO|On-time|\\n+--------+-----+--------+------+-----------+-------+\\nDropping columns\\nTo drop a column, use the drop()  method. For example, let’s remove the delay  col‐\\numn as we now have a status  column, added in the previous section:\\n152 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 175}), Document(page_content='// In Scala\\nval foo3 = foo2.drop(\"delay\")\\nfoo3.show()\\n# In Python\\nfoo3 = foo2.drop(\"delay\")\\nfoo3.show()\\n+--------+--------+------+-----------+-------+\\n|    date|distance |origin|destination | status|\\n+--------+--------+------+-----------+-------+\\n|01010710 |     590|   SEA|        SFO|Delayed|\\n|01010955 |     590|   SEA|        SFO|Delayed|\\n|01010730 |     590|   SEA|        SFO|On-time|\\n+--------+--------+------+-----------+-------+\\nRenaming columns\\nY ou can rename a column using the rename()  method:\\n// In Scala\\nval foo4 = foo3.withColumnRenamed (\"status\" , \"flight_status\" )\\nfoo4.show()\\n# In Python\\nfoo4 = foo3.withColumnRenamed (\"status\" , \"flight_status\" )\\nfoo4.show()\\n+--------+--------+------+-----------+-------------+\\n|    date|distance |origin|destination |flight_status |\\n+--------+--------+------+-----------+-------------+\\n|01010710 |     590|   SEA|        SFO|      Delayed|\\n|01010955 |     590|   SEA|        SFO|      Delayed|\\n|01010730 |     590|   SEA|        SFO|      On-time|\\n+--------+--------+------+-----------+-------------+\\nPivoting\\nWhen working with your data, sometimes you will need to swap the columns for the\\nrows—i.e., pivot  your data . Let’s grab some data to demonstrate this concept:\\n-- In SQL\\nSELECT destination , CAST(SUBSTRING (date, 0, 2) AS int) AS month, delay \\n  FROM departureDelays  \\n WHERE origin = \\'SEA\\'\\n+-----------+-----+-----+\\n|destination |month|delay|\\n+-----------+-----+-----+\\n|        ORD|    1|   92|\\n|        JFK|    1|   -7|\\n|        DFW|    1|   -5|\\n|        MIA|    1|   -3|\\nCommon DataFrames and Spark SQL Operations | 153', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 176}), Document(page_content=\"|        DFW|    1|   -3|\\n|        DFW|    1|    1|\\n|        ORD|    1|  -10|\\n|        DFW|    1|   -6|\\n|        DFW|    1|   -2|\\n|        ORD|    1|   -3|\\n+-----------+-----+-----+\\nonly showing top 10 rows\\nPivoting allows you to place names in the month  column (instead of 1 and 2 you can\\nshow Jan and Feb, respectively) as well as perform aggregate calculations (in this case\\naverage and max) on the delays by destination and month:\\n-- In SQL\\nSELECT * FROM (\\nSELECT destination , CAST(SUBSTRING (date, 0, 2) AS int) AS month, delay \\n  FROM departureDelays  WHERE origin = 'SEA' \\n) \\nPIVOT (\\n  CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay , MAX(delay) AS MaxDelay\\n  FOR month IN (1 JAN, 2 FEB)\\n)\\nORDER BY destination\\n+-----------+------------+------------+------------+------------+\\n|destination |JAN_AvgDelay |JAN_MaxDelay |FEB_AvgDelay |FEB_MaxDelay |\\n+-----------+------------+------------+------------+------------+\\n|        ABQ|       19.86|         316|       11.42|          69|\\n|        ANC|        4.44|         149|        7.90|         141|\\n|        ATL|       11.98|         397|        7.73|         145|\\n|        AUS|        3.48|          50|       -0.21|          18|\\n|        BOS|        7.84|         110|       14.58|         152|\\n|        BUR|       -2.03|          56|       -1.89|          78|\\n|        CLE|       16.00|          27|        null|        null|\\n|        CLT|        2.53|          41|       12.96|         228|\\n|        COS|        5.32|          82|       12.18|         203|\\n|        CVG|       -0.50|           4|        null|        null|\\n|        DCA|       -1.15|          50|        0.07|          34|\\n|        DEN|       13.13|         425|       12.95|         625|\\n|        DFW|        7.95|         247|       12.57|         356|\\n|        DTW|        9.18|         107|        3.47|          77|\\n|        EWR|        9.63|         236|        5.20|         212|\\n|        FAI|        1.84|         160|        4.21|          60|\\n|        FAT|        1.36|         119|        5.22|         232|\\n|        FLL|        2.94|          54|        3.50|          40|\\n|        GEG|        2.28|          63|        2.87|          60|\\n|        HDN|       -0.44|          27|       -6.50|           0|\\n+-----------+------------+------------+------------+------------+\\nonly showing top 20 rows\\n154 | Chapter 5: Spark SQL and DataFrames: Interacting with External Data Sources\", metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 177}), Document(page_content='Summary\\nThis chapter explored how Spark SQL interfaces with external components. We dis‐\\ncussed creating user-defined functions, including Pandas UDFs, and presented some\\noptions for executing Spark SQL queries (including the Spark SQL shell, Beeline, and\\nTableau). We then provided examples of how to use Spark SQL to connect with a\\nvariety of external data sources, such as SQL databases, PostgreSQL, MySQL, Tableau,\\nAzure Cosmos DB, MS SQL Server, and others.\\nWe explored Spark’s built-in functions for complex data types, and gave some exam‐\\nples of working with higher-order functions. Finally, we discussed some common\\nrelational operators and showed how to perform a selection of DataFrame operations.\\nIn the next chapter, we explore how to work with Datasets, the benefits of strongly\\ntyped operations, and when and why to use them.\\nSummary | 155', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 178}), Document(page_content='', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 179}), Document(page_content='CHAPTER 6\\nSpark SQL and Datasets\\nIn Chapters 4 and 5, we covered Spark SQL and the DataFrame API. We looked at\\nhow to connect to built-in and external data sources, took a peek at the Spark SQL\\nengine, and explored topics such as the interoperability between SQL and Data‐\\nFrames, creating and managing views and tables, and advanced DataFrame and SQL\\ntransformations.\\nAlthough we briefly introduced the Dataset API in Chapter 3 , we skimmed over the\\nsalient aspects of how Datasets—strongly typed distributed collections—are created,\\nstored, and serialized and deserialized in Spark.\\nIn this chapter, we go under the hood to understand Datasets: we’ll explore working\\nwith Datasets in Java and Scala, how Spark manages memory to accommodate Data‐\\nset constructs as part of the high-level API, and the costs associated with using\\nDatasets.\\nSingle API for Java and Scala\\nAs you may recall from Chapter 3  (Figure 3-1  and Table 3-6 ), Datasets offer a unified\\nand singular API for strongly typed objects. Among the languages supported by\\nSpark, only Scala and Java are strongly typed; hence, Python and R support only the\\nuntyped DataFrame API.\\nDatasets are domain-specific typed objects that can be operated on in parallel using\\nfunctional programming or the DSL operators you’re familiar with from the Data‐\\nFrame API.\\nThanks to this singular API, Java developers no longer risk lagging behind. For exam‐\\nple, any future interface or behavior changes to Scala’s groupBy() , flatMap() , map() ,\\n157', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 180}), Document(page_content='or filter()  API will be the same for Java too, because it’s a singular interface that is\\ncommon to both implementations.\\nScala Case Classes and JavaBeans for Datasets\\nIf you recall from Chapter 3  (Table 3-2 ), Spark has internal data types, such as String\\nType , BinaryType , IntegerType , BooleanType , and MapType , that it uses to map\\nseamlessly to the language-specific data types in Scala and Java during Spark opera‐\\ntions. This mapping is done via encoders, which we discuss later in this chapter.\\nIn order to create Dataset[T] , where T is your typed object in Scala, you need a case\\nclass  that defines the object. Using our example data from Chapter 3  (Table 3-1 ), say\\nwe have a JSON file with millions of entries about bloggers writing about Apache\\nSpark in the following format:\\n{id: 1, first: \"Jules\", last: \"Damji\", url: \"https://tinyurl.1\", date: \\n\"1/4/2016\", hits: 4535, campaigns: {\"twitter\", \"LinkedIn\"}},\\n...\\n{id: 87, first: \"Brooke\", last: \"Wenig\", url: \"https://tinyurl.2\", date:\\n\"5/5/2018\", hits: 8908, campaigns: {\"twitter\", \"LinkedIn\"}}\\nTo create a distributed Dataset[Bloggers] , we must first define a Scala case class that\\ndefines each individual field that comprises a Scala object. This case class serves as a\\nblueprint or schema for the typed object Bloggers :\\n// In Scala\\ncase class Bloggers (id:Int, first:String, last:String, url:String, date:String, \\nhits: Int, campaigns :Array[String])\\nWe can now read the file from the data source:\\nval bloggers  = \"../data/bloggers.json\"\\nval bloggersDS  = spark\\n  .read\\n  .format(\"json\")\\n  .option(\"path\", bloggers )\\n  .load()\\n  .as[Bloggers ]\\nEach row in the resulting distributed data collection is of type Bloggers .\\nSimilarly, you can create a JavaBean class of type Bloggers  in Java and then use\\nencoders to create a Dataset<Bloggers> :\\n// In Java\\nimport org.apache.spark.sql.Encoders ;\\nimport java.io.Serializable ;\\npublic class Bloggers  implements  Serializable  {\\n    private int id;\\n    private String first;\\n158 | Chapter 6: Spark SQL and Datasets', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 181}), Document(page_content='    private String last;\\n    private String url;\\n    private String date;\\n    private int hits;\\n    private Array[String] campaigns ;\\n// JavaBean getters and setters\\nint getID() { return id; }\\nvoid setID(int i) { id = i; }\\nString getFirst () { return first; }\\nvoid setFirst (String f) { first = f; }\\nString getLast() { return last; }\\nvoid setLast(String l) { last = l; }\\nString getURL() { return url; }\\nvoid setURL (String u) { url = u; }\\nString getDate() { return date; }\\nVoid setDate(String d) { date = d; }\\nint getHits() { return hits; }\\nvoid setHits(int h) { hits = h; }\\nArray[String] getCampaigns () { return campaigns ; }\\nvoid setCampaigns (Array[String] c) { campaigns  = c; }\\n}\\n// Create Encoder\\nEncoder<Bloggers > BloggerEncoder  = Encoders .bean(Bloggers .class);\\nString bloggers  = \"../bloggers.json\"\\nDataset<Bloggers >bloggersDS  = spark\\n  .read\\n  .format(\"json\")\\n  .option(\"path\", bloggers )\\n  .load()\\n  .as(BloggerEncoder );\\nAs you can see, creating Datasets in Scala and Java requires a bit of forethought, as\\nyou have to know all the individual column names and types for the rows you are\\nreading. Unlike with DataFrames, where you can optionally let Spark infer the\\nschema, the Dataset API requires that you define your data types ahead of time and\\nthat your case class or JavaBean class matches your schema.\\nThe names of the fields in the Scala case class or Java class defini‐\\ntion must match the order in the data source. The column names\\nfor each row in the data are automatically mapped to the corre‐\\nsponding names in the class and the types are automatically\\npreserved.\\nY ou may use an existing Scala case class or JavaBean class if the field names match\\nwith your input data. Working with the Dataset API is as easy, concise, and\\ndeclarative  as working with DataFrames. For most of the Dataset’s transformations,\\nSingle API for Java and Scala | 159', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 182}), Document(page_content='you can use the same relational operators you’ve learned about in the previous\\nchapters.\\nLet’s examine some aspects of working with a sample Dataset.\\nWorking with Datasets\\nOne simple and dynamic way to create a sample Dataset is using a SparkSession\\ninstance. In this scenario, for illustration purposes, we dynamically create a Scala\\nobject with three fields: uid (unique ID for a user), uname  (randomly generated user‐\\nname string), and usage  (minutes of server or service usage).\\nCreating Sample Data\\nFirst, let’s generate some sample data:\\n// In Scala\\nimport scala.util.Random._\\n// Our case class for the Dataset\\ncase class Usage(uid:Int, uname:String, usage: Int)\\nval r = new scala.util.Random(42)\\n// Create 1000 instances of scala Usage class \\n// This generates data on the fly\\nval data = for (i <- 0 to 1000) \\n  yield (Usage(i, \"user-\" + r.alphanumeric .take(5).mkString (\"\"),\\n  r.nextInt(1000)))\\n// Create a Dataset of Usage typed data\\nval dsUsage = spark.createDataset (data)\\ndsUsage.show(10)\\n+---+----------+-----+\\n|uid|     uname|usage|\\n+---+----------+-----+\\n|  0|user-Gpi2C|  525|\\n|  1|user-DgXDi|  502|\\n|  2|user-M66yO|  170|\\n|  3|user-xTOn6|  913|\\n|  4|user-3xGSz|  246|\\n|  5|user-2aWRN|  727|\\n|  6|user-EzZY1|   65|\\n|  7|user-ZlZMZ|  935|\\n|  8|user-VjxeG|  756|\\n|  9|user-iqf1P|    3|\\n+---+----------+-----+\\nonly showing top 10 rows\\nIn Java the idea is similar, but we have to use explicit Encoder s (in Scala, Spark han‐\\ndles this implicitly):\\n160 | Chapter 6: Spark SQL and Datasets', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 183}), Document(page_content='// In Java\\nimport org.apache.spark.sql.Encoders ;\\nimport org.apache.commons.lang3.RandomStringUtils ;\\nimport java.io.Serializable ;\\nimport java.util.Random ;\\nimport java.util.ArrayList ;\\nimport java.util.List ;\\n// Create a Java class as a Bean\\npublic class Usage implements  Serializable  {\\n   int uid;                // user id\\n   String uname;           // username\\n   int usage;              // usage\\n   public Usage(int uid, String uname, int usage) {\\n       this.uid = uid;\\n       this.uname = uname;\\n       this.usage = usage;\\n   }\\n   // JavaBean getters and setters \\n   public int getUid() { return this.uid; }\\n   public void setUid(int uid) { this.uid = uid; }\\n   public String getUname () { return this.uname; }\\n   public void setUname (String uname) { this.uname = uname; }\\n   public int getUsage () { return this.usage; }\\n   public void setUsage (int usage) { this.usage = usage; }\\n   public Usage() {\\n   }\\n   public String toString () {\\n       return \"uid: \\'\"  + this.uid + \"\\', uame: \\'\"  + this.uname + \"\\', \\n       usage: \\'\"  + this.usage + \"\\'\";\\n   }\\n}\\n// Create an explicit Encoder \\nEncoder<Usage> usageEncoder  = Encoders .bean(Usage.class);\\nRandom rand = new Random();\\nrand.setSeed(42);\\nList<Usage> data = new ArrayList <Usage>()\\n// Create 1000 instances of Java Usage class \\nfor (int i = 0; i < 1000; i++) {\\n  data.add(new Usage(i, \"user\" + \\n  RandomStringUtils .randomAlphanumeric (5),\\n  rand.nextInt(1000));\\n  \\n// Create a Dataset of Usage typed data\\nDataset<Usage> dsUsage = spark.createDataset (data, usageEncoder );\\nWorking with Datasets | 161', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 184}), Document(page_content='The generated Dataset between Scala and Java will differ because\\nthe random seed algorithm may be different. Hence, your Scala’s\\nand Java’s query results will differ.\\nNow that we have our generated Dataset, dsUsage , let’s perform some of the common\\ntransformations we have done in previous chapters.\\nTransforming Sample Data\\nRecall that Datasets are strongly typed collections of domain-specific objects. These\\nobjects can be transformed in parallel using functional or relational operations.\\nExamples of these transformations include map() , reduce() , filter() , select() ,\\nand aggregate() . As examples of higher-order functions , these methods can take\\nlambdas, closures, or functions as arguments and return the results. As such, they\\nlend themselves well to functional programming .\\nScala is a functional programming language, and more recently lambdas, functional\\narguments, and closures have been added to Java too. Let’s try a couple of higher-\\norder functions in Spark and use functional programming constructs with the sample\\ndata we created earlier.\\nHigher-order functions and functional programming\\nFor a simple example, let’s use filter()  to return all the users in our dsUsage  Dataset\\nwhose usage exceeds 900 minutes. One way to do this is to use a functional expres‐\\nsion as an argument to the filter()  method:\\n// In Scala\\nimport org.apache.spark.sql.functions._\\ndsUsage\\n  .filter(d => d.usage > 900)\\n  .orderBy(desc(\"usage\"))\\n  .show(5, false)\\nAnother way is to define a function and supply that function as an argument to\\nfilter() :\\ndef filterWithUsage(u: Usage) = u.usage > 900\\ndsUsage.filter(filterWithUsage(_)).orderBy(desc(\"usage\")).show(5)\\n+---+----------+-----+\\n|uid|     uname|usage|\\n+---+----------+-----+\\n|561|user-5n2xY|  999|\\n|113|user-nnAXr|  999|\\n|605|user-NL6c4|  999|\\n|634|user-L0wci|  999|\\n162 | Chapter 6: Spark SQL and Datasets', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 185}), Document(page_content='|805|user-LX27o|  996|\\n+---+----------+-----+\\nonly showing top 5 rows\\nIn the first case we used a lambda expression, {d.usage > 900} , as an argument to\\nthe filter()  method, whereas in the second case we defined a Scala function, def\\nfilterWithUsage(u: Usage) = u.usage > 900 . In both cases, the filter()  method\\niterates over each row of the Usage  object in the distributed Dataset and applies the\\nexpression or executes the function, returning a new Dataset of type Usage  for rows\\nwhere the value of the expression or function is true . (See the Scala documentation\\nfor method signature details.)\\nIn Java, the argument to filter()  is of type FilterFunction<T> . This can be defined\\neither inline anonymously or with a named function. For this example, we will define\\nour function by name and assign it to the variable f. Applying this function in\\nfilter()  will return a new Dataset with all the rows for which our filter condition is\\ntrue :\\n// In Java\\n// Define a Java filter function\\nFilterFunction <Usage> f = new FilterFunction <Usage>() {\\n   public boolean call(Usage u) {\\n       return (u.usage > 900);\\n   }\\n};\\n// Use filter with our function and order the results in descending order\\ndsUsage.filter(f).orderBy(col(\"usage\").desc()).show(5);\\n+---+----------+-----+\\n|uid|uname     |usage|\\n+---+----------+-----+\\n|67 |user-qCGvZ|997  |\\n|878|user-J2HUU|994  |\\n|668|user-pz2Lk|992  |\\n|750|user-0zWqR|991  |\\n|242|user-g0kF6|989  |\\n+---+----------+-----+\\nonly showing top 5 rows\\nNot all lambdas or functional arguments must evaluate to Boolean  values; they can\\nreturn computed values too. Consider this example using the higher-order function\\nmap() , where our aim is to find out the usage cost for each user whose usage  value is\\nover a certain threshold so we can offer those users a special price per minute.\\n// In Scala\\n// Use an if-then-else lambda expression and compute a value\\ndsUsage.map(u => {if (u.usage > 750) u.usage * .15 else u.usage * .50 })\\n  .show(5, false)\\n// Define a function to compute the usage\\nWorking with Datasets | 163', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 186}), Document(page_content='def computeCostUsage (usage: Int): Double = {\\n  if (usage > 750) usage * 0.15 else usage * 0.50\\n}\\n// Use the function as an argument to map()\\ndsUsage.map(u => {computeCostUsage (u.usage)}).show(5, false)\\n+------+\\n|value |\\n+------+\\n|262.5 |\\n|251.0 |\\n|85.0  |\\n|136.95|\\n|123.0 |\\n+------+\\nonly showing top 5 rows\\nTo use map()  in Java, you have to define a MapFunction<T> . This can either be an\\nanonymous class or a defined class that extends MapFunction<T> . For this example,\\nwe use it inline—that is, in the method call itself:\\n// In Java\\n// Define an inline MapFunction\\ndsUsage.map((MapFunction <Usage, Double>) u -> {\\n   if (u.usage > 750)\\n       return u.usage * 0.15;\\n   else\\n       return u.usage * 0.50;\\n}, Encoders .DOUBLE()).show(5); // We need to explicitly specify the Encoder\\n+------+\\n|value |\\n+------+\\n|65.0  |\\n|114.45|\\n|124.0 |\\n|132.6 |\\n|145.5 |\\n+------+\\nonly showing top 5 rows\\nThough we have computed values for the cost of usage, we don’t know which users\\nthe computed values are associated with. How do we get this information?\\nThe steps are simple:\\n1.Create a Scala case class or JavaBean class, UsageCost , with an additional field or\\ncolumn named cost .\\n2.Define a function to compute the cost  and use it in the map()  method.\\n164 | Chapter 6: Spark SQL and Datasets', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 187}), Document(page_content='Here’s what this looks like in Scala:\\n// In Scala\\n// Create a new case class with an additional field, cost\\ncase class UsageCost (uid: Int, uname:String, usage: Int, cost: Double)\\n// Compute the usage cost with Usage as a parameter\\n// Return a new object, UsageCost\\ndef computeUserCostUsage (u: Usage): UsageCost  = {\\n  val v = if (u.usage > 750) u.usage * 0.15 else u.usage * 0.50\\n    UsageCost (u.uid, u.uname, u.usage, v)\\n}\\n// Use map() on our original Dataset\\ndsUsage.map(u => {computeUserCostUsage (u)}).show(5)\\n+---+----------+-----+------+\\n|uid|     uname|usage|  cost|\\n+---+----------+-----+------+\\n|  0|user-Gpi2C|  525| 262.5|\\n|  1|user-DgXDi|  502| 251.0|\\n|  2|user-M66yO|  170|  85.0|\\n|  3|user-xTOn6|  913|136.95|\\n|  4|user-3xGSz|  246| 123.0|\\n+---+----------+-----+------+\\nonly showing top 5 rows\\nNow we have a transformed Dataset with a new column, cost , computed by the func‐\\ntion in our map()  transformation, along with all the other columns.\\nLikewise, in Java, if we want the cost associated with each user we need to define a\\nJavaBean class UsageCost  and MapFunction<T> . For the complete JavaBean example,\\nsee the book’s GitHub repo ; for brevity, we will only show the inline MapFunction<T>\\nhere:\\n// In Java\\n// Get the Encoder for the JavaBean class\\nEncoder<UsageCost > usageCostEncoder  = Encoders .bean(UsageCost .class);\\n// Apply map() function to our data\\ndsUsage.map( (MapFunction <Usage, UsageCost >) u -> {\\n       double v = 0.0;\\n       if (u.usage > 750) v = u.usage * 0.15; else v = u.usage * 0.50;\\n       return new UsageCost (u.uid, u.uname,u.usage, v); },\\n            usageCostEncoder ).show(5);\\n+------+---+----------+-----+\\n|  cost|uid|     uname|usage|\\n+------+---+----------+-----+\\n|  65.0|  0|user-xSyzf|  130|\\n|114.45|  1|user-iOI72|  763|\\n| 124.0|  2|user-QHRUk|  248|\\nWorking with Datasets | 165', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 188}), Document(page_content='| 132.6|  3|user-8GTjo|  884|\\n| 145.5|  4|user-U4cU1|  970|\\n+------+---+----------+-----+\\nonly showing top 5 rows\\nThere are a few things to observe about using higher-order functions and Datasets:\\n•We are using typed JVM objects as arguments to functions.\\n•We are using dot notation (from object-oriented programming) to access indi‐\\nvidual fields within the typed JVM object, making it easier to read.\\n•Some of our functions and lambda signatures can be type-safe, ensuring compile-\\ntime error detection and instructing Spark what data types to work on, what\\noperations to perform, etc.\\n•Our code is readable, expressive, and concise, using Java or Scala language fea‐\\ntures in lambda expressions.\\n•Spark provides the equivalent of map()  and filter()  without higher-order func‐\\ntional constructs in both Java and Scala, so you are not forced to use functional\\nprogramming with Datasets or DataFrames. Instead, you can simply use condi‐\\ntional DSL operators or SQL expressions: for example, dsUsage.filter(\"usage\\n> 900\")  or dsUsage($\"usage\" > 900) . (For more on this, see “Costs of Using\\nDatasets” on page 170 .)\\n•For Datasets we use encoders, a mechanism to efficiently convert data between\\nJVM and Spark’s internal binary format for its data types (more on that in “Data‐\\nset Encoders” on page 168 ).\\nHigher-order functions and functional programming are not\\nunique to Spark Datasets; you can use them with DataFrames too.\\nRecall that a DataFrame is a Dataset[Row] , where Row is a generic\\nuntyped JVM object that can hold different types of fields. The\\nmethod signature takes expressions or functions that operate on\\nRow, meaning that each Row’s data type can be input value to the\\nexpression or function.\\nConverting DataFrames to Datasets\\nFor strong type checking of queries and constructs, you can convert DataFrames to\\nDatasets. To convert an existing DataFrame df to a Dataset of type SomeCaseClass ,\\nsimply use the df.as[SomeCaseClass]  notation. We saw an example of this earlier:\\n166 | Chapter 6: Spark SQL and Datasets', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 189}), Document(page_content='1For more details on how Spark manages memory, check out the references provided in the text and the pre‐\\nsentations “ Apache Spark Memory Management”  and “Deep Dive into Project Tungsten Bringing Spark\\nCloser to Bare Metal” .// In Scala\\nval bloggersDS  = spark\\n  .read\\n  .format(\"json\")\\n  .option(\"path\", \"/data/bloggers/bloggers.json\" )\\n  .load()\\n  .as[Bloggers ]\\nspark.read.format(\"json\")  returns a DataFrame<Row> , which in Scala is a type alias\\nfor Dataset[Row] . Using .as[Bloggers]  instructs Spark to use encoders, discussed\\nlater in this chapter, to serialize/deserialize objects from Spark’s internal memory rep‐\\nresentation to JVM Bloggers  objects.\\nMemory Management for Datasets and DataFrames\\nSpark is an intensive in-memory distributed big data engine, so its efficient use of\\nmemory is crucial to its execution speed.1 Throughout its release history, Spark’s\\nusage of memory has significantly evolved :\\n•Spark 1.0 used RDD-based Java objects for memory storage, serialization, and\\ndeserialization, which was expensive in terms of resources and slow. Also, storage\\nwas allocated on the Java heap,  so you were at the mercy of the JVM’s garbage\\ncollection (GC) for large data sets.\\n•Spark 1.x introduced Project Tungsten . One of its prominent features was a new\\ninternal row-based format to lay out Datasets and DataFrames in off-heap mem‐\\nory, using offsets and pointers. Spark uses an efficient mechanism called encoders\\nto serialize and deserialize between the JVM and its internal Tungsten format.\\nAllocating memory off-heap means that Spark is less encumbered by GC.\\n•Spark 2.x introduced the second-generation Tungsten engine , featuring whole-\\nstage code generation and vectorized column-based memory layout. Built on\\nideas and techniques from modern compilers, this new version also capitalized\\non modern CPU and cache architectures for fast parallel data access with the\\n“single instruction, multiple data” (SIMD) approach.\\nMemory Management for Datasets and DataFrames | 167', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 190}), Document(page_content='Dataset Encoders\\nEncoders convert data in off-heap memory from Spark’s internal Tungsten format to\\nJVM Java objects. In other words, they serialize and deserialize Dataset objects from\\nSpark’s internal format to JVM objects, including primitive data types. For example,\\nan Encoder[T]  will convert from Spark’s internal Tungsten format to Dataset[T] .\\nSpark has built-in support for automatically generating encoders for primitive types\\n(e.g., string, integer, long), Scala case classes, and JavaBeans. Compared to Java and\\nKryo serialization and deserialization, Spark encoders are significantly faster .\\nIn our earlier Java example, we explicitly created an encoder:\\nEncoder<UsageCost > usageCostEncoder  = Encoders .bean(UsageCost .class);\\nHowever, for Scala, Spark automatically generates the bytecode for these efficient\\nconverters. Let’s take a peek at Spark’s internal Tungsten row-based format.\\nSpark’s Internal Format Versus Java Object Format\\nJava objects have large overheads—header info, hashcode, Unicode info, etc. Even a\\nsimple Java string such as “abcd” takes 48 bytes of storage, instead of the 4 bytes you\\nmight expect. Imagine the overhead to create, for example, a MyClass(Int, String,\\nString)  object.\\nInstead of creating JVM-based objects for Datasets or DataFrames, Spark allocates\\noff-heap  Java memory to lay out their data and employs encoders to convert the data\\nfrom in-memory representation to JVM object. For example, Figure 6-1  shows how\\nthe JVM object MyClass(Int, String, String)  would be stored internally.\\nFigure 6-1. JVM object stored in contiguous off-heap  Java memory managed by Spark\\nWhen data is stored in this contiguous manner and accessible through pointer arith‐\\nmetic and offets, encoders can quickly serialize or deserialize that data. What does\\nthat mean?\\n168 | Chapter 6: Spark SQL and Datasets', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 191}), Document(page_content='Serialization and Deserialization (SerDe)\\nA concept not new in distributed computing, where data frequently travels over the\\nnetwork among computer nodes in a cluster, serialization and deserialization  is the\\nprocess by which a typed object is encoded  (serialized) into a binary presentation or\\nformat by the sender and decoded  (deserialized) from binary format into its respec‐\\ntive data-typed object by the receiver.\\nFor example, if the JVM object MyClass  in Figure 6-1  had to be shared among nodes\\nin a Spark cluster, the sender would serialize it into an array of bytes, and the receiver\\nwould deserialize it back into a JVM object of type MyClass .\\nThe JVM has its own built-in Java serializer and deserializer, but it’s inefficient\\nbecause (as we saw in the previous section) the Java objects created by the JVM in the\\nheap memory are bloated. Hence, the process is slow.\\nThis is where the Dataset encoders come to the rescue, for a few reasons:\\n•Spark’s internal Tungsten binary format (see Figures 6-1 and 6-2) stores objects\\noff the Java heap memory, and it’s compact so those objects occupy less space.\\n•Encoders can quickly serialize by traversing across the memory using simple\\npointer arithmetic with memory addresses and offsets ( Figure 6-2 ).\\n•On the receiving end, encoders can quickly deserialize the binary representation\\ninto Spark’s internal representation. Encoders are not hindered by the JVM’s\\ngarbage collection pauses.\\nFigure 6-2. Spark’s internal Tungsten row-based format\\nHowever, most good things in life come at a price, as we discuss next.\\nDataset Encoders | 169', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 192}), Document(page_content='Costs of Using Datasets\\nIn “DataFrames Versus Datasets” on page 74 in Chapter 3 , we outlined some of the\\nbenefits of using Datasets—but these benefits come at a cost. As noted in the\\npreceding  section, when Datasets are passed to higher-order functions such as fil\\nter() , map() , or flatMap()  that take lambdas and functional arguments, there is a\\ncost associated with deserializing from Spark’s internal Tungsten format into the JVM\\nobject.\\nCompared to other serializers used before encoders were introduced in Spark, this\\ncost is minor and tolerable. However, over larger data sets and many queries, this cost\\naccrues and can affect performance.\\nStrategies to Mitigate Costs\\nOne strategy to mitigate excessive serialization and deserialization is to use\\nDSL expressions in your queries and avoid excessive use of lambdas as anonymous\\nfunctions  as arguments to higher-order functions. Because lambdas are anonymous\\nand opaque to the Catalyst optimizer until runtime, when you use them it cannot effi‐\\nciently discern what you’re doing (you’re not telling Spark what to do ) and thus can‐\\nnot optimize your queries (see “The Catalyst Optimizer” on page 77  in Chapter 3 ).\\nThe second strategy is to chain your queries together in such a way that serialization\\nand deserialization is minimized. Chaining queries together is a common practice in\\nSpark.\\nLet’s illustrate with a simple example. Suppose we have a Dataset of type Person ,\\nwhere Person  is defined as a Scala case class:\\n// In Scala\\nPerson(id: Integer, firstName : String, middleName : String, lastName : String,\\ngender: String, birthDate : String, ssn: String, salary: String)\\nWe want to issue a set of queries to this Dataset, using functional programming.\\nLet’s examine a case where we compose a query inefficiently, in such a way that we\\nunwittingly incur the cost of repeated serialization and deserialization:\\nimport java.util.Calendar\\nval earliestYear  = Calendar .getInstance .get(Calendar .YEAR) - 40\\npersonDS\\n  // Everyone above 40: lambda-1\\n  .filter(x => x.birthDate .split(\"-\")(0).toInt > earliestYear )\\n  \\n  // Everyone earning more than 80K\\n  .filter($\"salary\"  > 80000)\\n  \\n170 | Chapter 6: Spark SQL and Datasets', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 193}), Document(page_content='  // Last name starts with J: lambda-2\\n  .filter(x => x.lastName .startsWith (\"J\"))\\n  \\n  // First name starts with D\\n  .filter($\"firstName\" .startsWith (\"D\"))\\n  .count()\\nAs you can observe in Figure 6-3 , each time we move from lambda to DSL ( fil\\nter($\"salary\" > 8000) ) we incur the cost of serializing and deserializing the Person\\nJVM object.\\nFigure 6-3. An inefficient  way to chain queries with lambdas and DSL\\nBy contrast, the following query uses only DSL and no lambdas. As a result, it’s much\\nmore efficient—no serialization/deserialization is required for the entire composed\\nand chained query:\\npersonDS\\n  .filter(year($\"birthDate\" ) > earliestYear ) // Everyone above 40\\n  .filter($\"salary\"  > 80000) // Everyone earning more than 80K\\n  .filter($\"lastName\" .startsWith (\"J\")) // Last name starts with J\\n  .filter($\"firstName\" .startsWith (\"D\")) // First name starts with D\\n  .count()\\nFor the curious, you can see the timing difference between the two runs in the note‐\\nbook for this chapter in the book’s GitHub repo .\\nCosts of Using Datasets | 171', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 194}), Document(page_content='Summary\\nIn this chapter, we elaborated on how to work with Datasets in Java and Scala. We\\nexplored how Spark manages memory to accommodate Dataset constructs as part of\\nits unified and high-level API, and we considered some of the costs associated with\\nusing Datasets and how to mitigate those costs. We also showed you how to use Java\\nand Scala’s functional programming constructs in Spark.\\nFinally, we took a look under the hood at how encoders serialize and deserialize from\\nSpark’s internal Tungsten binary format to JVM objects.\\nIn the next chapter, we’ll look at how to optimize Spark by examining efficient I/O\\nstrategies, optimizing and tuning Spark configurations, and what attributes and sig‐\\nnals to look for while debugging Spark applications.\\n172 | Chapter 6: Spark SQL and Datasets', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 195}), Document(page_content='CHAPTER 7\\nOptimizing and Tuning Spark Applications\\nIn the previous chapter, we elaborated on how to work with Datasets in Java and\\nScala. We explored how Spark manages memory to accommodate Dataset constructs\\nas part of its unified and high-level API, and we considered the costs associated with\\nusing Datasets and how to mitigate those costs.\\nBesides mitigating costs, we also want to consider how to optimize and tune Spark. In\\nthis chapter, we will discuss a set of Spark configurations that enable optimizations,\\nlook at Spark’s family of join strategies, and inspect the Spark UI, looking for clues to\\nbad behavior.\\nOptimizing and Tuning Spark for Efficiency\\nWhile Spark has many configurations for tuning , this book will only cover a handful\\nof the most important and commonly tuned configurations. For a comprehensive list\\ngrouped by functional themes, you can peruse the documentation .\\nViewing and Setting Apache Spark Configurations\\nThere are three ways you can get and set Spark properties. The first is through a set of\\nconfiguration files. In your deployment’s $SPARK_HOME  directory (where you installed\\nSpark), there are a number of config files: conf/spark-defaults.conf.template , conf/\\nlog4j.properties.template , and conf/spark-env.sh.template . Changing the default values\\nin these files and saving them without the . template  suffix instructs Spark to use these\\nnew values.\\n173', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 196}), Document(page_content='Configuration changes in the conf/spark-defaults.conf  file apply to\\nthe Spark cluster and all Spark applications submitted to the\\ncluster.\\nThe second way is to specify Spark configurations directly in your Spark application\\nor on the command line when submitting the application with spark-submit , using\\nthe --conf  flag:\\nspark-submit --conf spark.sql.shuffle.partitions=5 --conf\\n\"spark.executor.memory=2g\" --class main.scala.chapter7.SparkConfig_7_1 jars/main-\\nscala-chapter7_2.12-1.0.jar\\nHere’s how you would do this in the Spark application itself:\\n// In Scala\\nimport org.apache.spark.sql.SparkSession\\ndef printConfigs (session: SparkSession ) = {\\n   // Get conf\\n   val mconf = session.conf.getAll\\n   // Print them\\n   for (k <- mconf.keySet) { println(s\"${k} -> ${mconf(k)}\\\\n\") }\\n}\\ndef main(args: Array[String]) {\\n // Create a session\\n val spark = SparkSession .builder\\n   .config(\"spark.sql.shuffle.partitions\" , 5)\\n   .config(\"spark.executor.memory\" , \"2g\")\\n   .master(\"local[*]\" )\\n   .appName(\"SparkConfig\" )\\n   .getOrCreate ()\\n printConfigs (spark)\\n spark.conf.set(\"spark.sql.shuffle.partitions\" ,\\n   spark.sparkContext .defaultParallelism )\\n println(\" ****** Setting Shuffle Partitions to Default Parallelism\" )\\n printConfigs (spark)\\n}\\nspark.driver.host -> 10.8.154.34\\nspark.driver.port -> 55243\\nspark.app.name -> SparkConfig\\nspark.executor .id -> driver\\nspark.master -> local[*]\\nspark.executor .memory -> 2g\\nspark.app.id -> local-1580162894307\\nspark.sql.shuffle.partitions  -> 5\\n174 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 197}), Document(page_content='The third option is through a programmatic interface via the Spark shell. As with\\neverything else in Spark, APIs are the primary method of interaction. Through the\\nSparkSession  object, you can access most Spark config settings.\\nIn a Spark REPL, for example, this Scala code shows the Spark configs on a local host\\nwhere Spark is launched in local mode (for details on the different modes available,\\nsee “Deployment modes” on page 12  in Chapter 1 ):\\n// In Scala\\n// mconf is a Map[String, String] \\nscala> val mconf = spark.conf.getAll\\n...\\nscala> for (k <- mconf.keySet) { println(s\"${k} -> ${mconf(k)}\\\\n\") }\\nspark.driver.host -> 10.13.200.101\\nspark.driver.port -> 65204\\nspark.repl.class.uri -> spark://10.13.200.101:65204/classes\\nspark.jars ->\\nspark.repl.class.outputDir -> /private/var/folders/jz/qg062ynx5v39wwmfxmph5nn...\\nspark.app.name -> Spark shell\\nspark.submit.pyFiles ->\\nspark.ui.showConsoleProgress -> true\\nspark.executor.id -> driver\\nspark.submit.deployMode -> client\\nspark.master -> local[*]\\nspark.home -> /Users/julesdamji/spark/spark-3.0.0-preview2-bin-hadoop2.7\\nspark.sql.catalogImplementation -> hive\\nspark.app.id -> local-1580144503745\\nY ou can also view only the Spark SQL–specific Spark configs:\\n// In Scala\\nspark.sql(\"SET -v\" ).select(\"key\", \"value\").show(5, false)\\n# In Python\\nspark.sql(\"SET -v\" ).select(\"key\", \"value\").show(n=5, truncate =False)\\n+------------------------------------------------------------+-----------+\\n|key                                                         |value      |\\n+------------------------------------------------------------+-----------+\\n|spark.sql.adaptive .enabled                                  |false      |\\n|spark.sql.adaptive .nonEmptyPartitionRatioForBroadcastJoin    |0.2        |\\n|spark.sql.adaptive .shuffle.fetchShuffleBlocksInBatch .enabled|true       |\\n|spark.sql.adaptive .shuffle.localShuffleReader .enabled       |true       |\\n|spark.sql.adaptive .shuffle.maxNumPostShufflePartitions       |<undefined >|\\n+------------------------------------------------------------+-----------+\\nonly showing top 5 rows\\nAlternatively, you can access Spark’s current configuration through the Spark UI’s\\nEnvironment tab, which we discuss later in this chapter, as read-only values, as shown\\nin Figure 7-1 .\\nOptimizing and Tuning Spark for Efficiency  | 175', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 198}), Document(page_content='Figure 7-1. The Spark 3.0 UI’s Environment tab\\nTo set or modify an existing configuration programmatically, first check if the prop‐\\nerty is modifiable. spark.conf.isModifiable(\" <config_name> \") will return true  or\\nfalse . All modifiable configs can be set to new values using the API:\\n// In Scala\\nscala> spark.conf.get(\"spark.sql.shuffle.partitions\" )\\nres26: String = 200\\nscala> spark.conf.set(\"spark.sql.shuffle.partitions\" , 5)\\nscala> spark.conf.get(\"spark.sql.shuffle.partitions\" )\\nres28: String = 5\\n# In Python\\n>>> spark.conf.get(\"spark.sql.shuffle.partitions\" )\\n\\'200\\'\\n>>> spark.conf.set(\"spark.sql.shuffle.partitions\" , 5)\\n>>> spark.conf.get(\"spark.sql.shuffle.partitions\" )\\n\\'5\\'\\nAmong all the ways that you can set Spark properties, an order of precedence deter‐\\nmines which values are honored. Any values or flags defined in spark-defaults.conf\\nwill be read first, followed by those supplied on the command line with spark-\\nsubmit , and finally those set via SparkSession  in the Spark application. All these\\nproperties will be merged, with any duplicate properties reset in the Spark application\\ntaking precedence. Likewise, values supplied on the command line will supersede set‐\\ntings in the configuration file, provided they are not overwritten in the application\\nitself.\\nTweaking or supplying the right configurations helps with performance, as you’ll see\\nin the next section. The recommendations here are derived from practitioners’ obser‐\\nvations in the community and focus on how to maximize cluster resource utilization\\nfor Spark to accommodate large-scale workloads.\\n176 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 199}), Document(page_content='1See “Tuning Apache Spark for Large Scale Workloads”  and “Hive Bucketing in Apache Spark” .\\nScaling Spark for Large Workloads\\nLarge Spark workloads are often batch jobs—some run on a nightly basis, while some\\nare scheduled at regular intervals during the day. In either case, these jobs may pro‐\\ncess tens of terabytes of data or more. To avoid job failures due to resource starvation\\nor gradual performance degradation, there are a handful of Spark configurations that\\nyou can enable or alter. These configurations affect three Spark components: the\\nSpark driver, the executor, and the shuffle service running on the executor.\\nThe Spark driver’s responsibility is to coordinate with the cluster manager to launch\\nexecutors in a cluster and schedule Spark tasks on them. With large workloads, you\\nmay have hundreds of tasks. This section explains a few configurations you can tweak\\nor enable to optimize your resource utilization, parallelize tasks, and avoid bottle‐\\nnecks for large numbers of tasks. Some of the optimization ideas and insights have\\nbeen derived from big data companies like Facebook that use Spark at terabyte scale,\\nwhich they shared with the Spark community at the Spark + AI Summit.1\\nStatic versus dynamic resource allocation\\nWhen you specify compute resources as command-line arguments to spark-submit ,\\nas we did earlier, you cap the limit. This means that if more resources are needed later\\nas tasks queue up in the driver due to a larger than anticipated workload, Spark can‐\\nnot accommodate or allocate extra resources.\\nIf instead you use Spark’s dynamic resource allocation configuration , the Spark driver\\ncan request more or fewer compute resources as the demand of large workloads flows\\nand ebbs. In scenarios where your workloads are dynamic—that is, they vary in their\\ndemand for compute capacity—using dynamic allocation helps to accommodate sud‐\\nden peaks.\\nOne use case where this can be helpful is streaming, where the data flow volume may\\nbe uneven. Another is on-demand data analytics, where you might have a high vol‐\\nume of SQL queries during peak hours. Enabling dynamic resource allocation allows\\nSpark to achieve better utilization of resources, freeing executors when not in use and\\nacquiring new ones when needed.\\nAs well as when working with large or varying workloads, dynamic\\nallocation is also useful in a multitenant environment , where Spark\\nmay be deployed alongside other applications or services in YARN,\\nMesos, or Kubernetes. Be advised, however, that Spark’s shifting\\nresource demands may impact other applications demanding\\nresources at the same time.\\nOptimizing and Tuning Spark for Efficiency  | 177', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 200}), Document(page_content='To enable and configure dynamic allocation, you can use settings like the following.\\nNote that the numbers here are arbitrary; the appropriate settings will depend on the\\nnature of your workload and they should be adjusted accordingly. Some of these\\nconfigs cannot be set inside a Spark REPL, so you will have to set them\\nprogrammatically:\\nspark.dynamicAllocation.enabled true\\nspark.dynamicAllocation.minExecutors 2\\nspark.dynamicAllocation.schedulerBacklogTimeout 1m\\nspark.dynamicAllocation.maxExecutors 20\\nspark.dynamicAllocation.executorIdleTimeout 2min\\nBy default spark.dynamicAllocation.enabled  is set to false . When enabled with\\nthe settings shown here, the Spark driver will request that the cluster manager create\\ntwo executors to start with, as a minimum ( spark.dynamicAllocation.minExecu\\ntors ). As the task queue backlog increases, new executors will be requested each time\\nthe backlog timeout ( spark.dynamicAllocation.schedulerBacklogTimeout ) is\\nexceeded. In this case, whenever there are pending tasks that have not been scheduled\\nfor over 1 minute, the driver will request that a new executor be launched to schedule\\nbacklogged tasks, up to a maximum of 20 ( spark.dynamicAllocation.maxExecu\\ntors ). By contrast, if an executor finishes a task and is idle for 2 minutes\\n(spark.dynamicAllocation.executorIdleTimeout ), the Spark driver will terminate\\nit.\\nConfiguring  Spark executors’ memory and the shuffle  service\\nSimply enabling dynamic resource allocation is not sufficient. Y ou also have to under‐\\nstand how executor memory is laid out and used by Spark so that executors are not\\nstarved of memory or troubled by JVM garbage collection.\\nThe amount of memory available to each executor is controlled by\\nspark.executor.memory . This is divided into three sections, as depicted in\\nFigure 7-2 : execution memory, storage memory, and reserved memory. The default\\ndivision is 60% for execution memory and 40% for storage, after allowing for 300 MB\\nfor reserved memory, to safeguard against OOM errors. The Spark documentation\\nadvises that this will work for most cases, but you can adjust what fraction of\\nspark.executor.memory  you want either section to use as a baseline. When storage\\nmemory is not being used, Spark can acquire it for use in execution memory for exe‐\\ncution purposes, and vice versa.\\n178 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 201}), Document(page_content='Figure 7-2. Executor memory layout\\nExecution memory is used for Spark shuffles, joins, sorts, and aggregations. Since dif‐\\nferent queries may require different amounts of memory, the fraction ( spark.mem\\nory.fraction  is 0.6 by default) of the available memory to dedicate to this can be\\ntricky to tune but it’s easy to adjust. By contrast, storage memory is primarily used for\\ncaching user data structures and partitions derived from DataFrames.\\nDuring map and shuffle operations, Spark writes to and reads from the local disk’s\\nshuffle files, so there is heavy I/O activity. This can result in a bottleneck, because the\\ndefault configurations are suboptimal for large-scale Spark jobs. Knowing what con‐\\nfigurations to tweak can mitigate this risk during this phase of a Spark job.\\nIn Table 7-1 , we capture a few recommended configurations to adjust so that the map,\\nspill, and merge processes during these operations are not encumbered by inefficient\\nI/O and to enable these operations to employ buffer memory before writing the final\\nshuffle partitions to disk. Tuning the shuffle service  running on each executor can\\nalso aid in increasing overall performance for large Spark workloads.\\nOptimizing and Tuning Spark for Efficiency  | 179', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 202}), Document(page_content='Table 7-1. Spark configurations  to tweak for I/O during map and shuffle  operations\\nConfiguration Default value, recommendation, and description\\nspark.driver.memory Default is 1g (1 GB). This is the amount of memory allocated to the Spark\\ndriver to receive data from executors. This is often changed during spark-\\nsubmit  with --driver-memory .\\nOnly change this if you expect the driver to receive large amounts of data\\nback from operations like collect() , or if you run out of driver memory.\\nspark.shuffle.file.buffer Default is 32 KB. Recommended is 1 MB. This allows Spark to do more\\nbuffering  before writing final  map results to disk.\\nspark.file.transferTo Default is true . Setting it to false  will force Spark to use the file buffer  to\\ntransfer files  before finally  writing to disk; this will decrease the I/O activity.\\nspark.shuffle.unsafe.file.out\\nput.bufferDefault is 32 KB. This controls the amount of buffering  possible when\\nmerging files  during shuffle  operations. In general, large values (e.g., 1 MB)\\nare more appropriate for larger workloads, whereas the default can work for\\nsmaller workloads.\\nspark.io.compression.lz4.block\\nSizeDefault is 32 KB. Increase to 512 KB. You can decrease the size of the shuffle\\nfile by increasing the compressed size of the block.\\nspark.shuffle.service.\\nindex.cache.sizeDefault is 100m. Cache entries are limited to the specified  memory footprint\\nin byte.\\nspark.shuffle.registration.\\ntimeoutDefault is 5000 ms. Increase to 120000 ms.\\nspark.shuffle.registration.max\\nAttemptsDefault is 3. Increase to 5 if needed.\\nThe recommendations in this table won’t work for all situations,\\nbut they should give you an idea of how to adjust these configura‐\\ntions based on your workload. Like with everything else in perfor‐\\nmance tuning, you have to experiment until you find the right\\nbalance.\\nMaximizing Spark parallelism\\nMuch of Spark’s efficiency is due to its ability to run multiple tasks in parallel at scale.\\nTo understand how you can maximize parallelism—i.e., read and process as much\\ndata in parallel as possible—you have to look into how Spark reads data into memory\\nfrom storage and what partitions mean to Spark.\\nIn data management parlance, a partition is a way to arrange data into a subset of\\nconfigurable and readable chunks or blocks of contiguous data on disk. These subsets\\nof data can be read or processed independently and in parallel, if necessary, by more\\nthan a single thread in a process. This independence matters because it allows for\\nmassive parallelism of data processing.\\nSpark is embarrassingly efficient at processing its tasks in parallel. As you learned in\\nChapter 2 , for large-scale workloads a Spark job will have many stages, and within\\n180 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 203}), Document(page_content='each stage there will be many tasks. Spark will at best schedule a thread per task per\\ncore, and each task will process a distinct partition. To optimize resource utilization\\nand maximize parallelism, the ideal is at least as many partitions as there are cores on\\nthe executor, as depicted in Figure 7-3 . If there are more partitions than there are\\ncores on each executor, all the cores are kept busy. Y ou can think of partitions as\\natomic units of parallelism: a single thread running on a single core can work on a\\nsingle partition.\\nFigure 7-3. Relationship of Spark tasks, cores, partitions, and parallelism\\nHow partitions are created.    As mentioned previously, Spark’s tasks process data as par‐\\ntitions read from disk into memory. Data on disk is laid out in chunks or contiguous\\nfile blocks, depending on the store. By default, file blocks on data stores range in size\\nfrom 64 MB to 128 MB. For example, on HDFS and S3 the default size is 128 MB\\n(this is configurable). A contiguous collection of these blocks constitutes a partition.\\nThe size of a partition in Spark is dictated by spark.sql.files.maxPartitionBytes .\\nThe default is 128 MB. Y ou can decrease the size, but that may result in what’s known\\nas the “small file problem”—many small partition files, introducing an inordinate\\namount of disk I/O and performance degradation thanks to filesystem operations\\nsuch as opening, closing, and listing directories, which on a distributed filesystem can\\nbe slow.\\nPartitions are also created when you explicitly use certain methods of the DataFrame\\nAPI. For example, while creating a large DataFrame or reading a large file from disk,\\nyou can explicitly instruct Spark to create a certain number of partitions:\\nOptimizing and Tuning Spark for Efficiency  | 181', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 204}), Document(page_content='2For some tips on configuring shuffle partitions, see “Tuning Apache Spark for Large Scale Workloads” , “Hive\\nBucketing in Apache Spark” , and “Why Y ou Should Care about Data Layout in the Filesystem” .\\n// In Scala\\nval ds = spark.read.textFile (\"../README.md\" ).repartition (16)\\nds: org.apache.spark.sql.Dataset [String] = [value: string]\\nds.rdd.getNumPartitions\\nres5: Int = 16\\nval numDF = spark.range(1000L * 1000 * 1000).repartition (16)\\nnumDF.rdd.getNumPartitions\\nnumDF: org.apache.spark.sql.Dataset [Long] = [id: bigint]\\nres12: Int = 16\\nFinally, shuffle  partitions  are created during the shuffle stage. By default, the number\\nof shuffle partitions is set to 200 in spark.sql.shuffle.partitions . Y ou can adjust\\nthis number depending on the size of the data set you have, to reduce the amount of\\nsmall partitions being sent across the network to executors’ tasks.\\nThe default value for spark.sql.shuffle.partitions  is too high\\nfor smaller or streaming workloads; you may want to reduce it to a\\nlower value such as the number of cores on the executors or less.\\nCreated during operations like groupBy()  or join() , also known as wide transforma‐\\ntions, shuffle partitions consume both network and disk I/O resources. During these\\noperations, the shuffle will spill results to executors’ local disks at the location speci‐\\nfied in spark.local.directory . Having performant SSD disks for this operation will\\nboost the performance.\\nThere is no magic formula for the number of shuffle partitions to set for the shuffle\\nstage; the number may vary depending on your use case, data set, number of cores,\\nand the amount of executor memory available—it’s a trial-and-error approach.2\\nIn addition to scaling Spark for large workloads, to boost your performance you’ll\\nwant to consider caching or persisting your frequently accessed DataFrames or tables.\\nWe explore various caching and persistence options in the next section.\\n182 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 205}), Document(page_content='Caching and Persistence of Data\\nWhat is the difference between caching and persistence? In Spark they are synony‐\\nmous. Two API calls, cache()  and persist() , offer these capabilities. The latter pro‐\\nvides more control over how and where your data is stored—in memory and on disk,\\nserialized and unserialized. Both contribute to better performance for frequently\\naccessed DataFrames or tables.\\nDataFrame.cache()\\ncache()  will store as many of the partitions read in memory across Spark executors\\nas memory allows (see Figure 7-2 ). While a DataFrame may be fractionally cached,\\npartitions cannot be fractionally cached (e.g., if you have 8 partitions but only 4.5\\npartitions can fit in memory, only 4 will be cached). However, if not all your parti‐\\ntions are cached, when you want to access the data again, the partitions that are not\\ncached will have to be recomputed, slowing down your Spark job.\\nLet’s look at an example of how caching a large DataFrame improves performance\\nwhen accessing a DataFrame:\\n// In Scala\\n// Create a DataFrame with 10M records\\nval df = spark.range(1 * 10000000 ).toDF(\"id\").withColumn (\"square\" , $\"id\" * $\"id\")\\ndf.cache() // Cache the data\\ndf.count() // Materialize the cache\\nres3: Long = 10000000\\nCommand took 5.11 seconds\\ndf.count() // Now get it from the cache\\nres4: Long = 10000000\\nCommand took 0.44 seconds\\nThe first count()  materializes the cache, whereas the second one accesses the cache,\\nresulting in a close to 12 times faster access time for this data set.\\nWhen you use cache()  or persist() , the DataFrame is not fully\\ncached until you invoke an action that goes through every record\\n(e.g., count() ). If you use an action like take(1) , only one parti‐\\ntion will be cached because Catalyst realizes that you do not need\\nto compute all the partitions just to retrieve one record.\\nObserving how a DataFrame is stored across one executor on a local host, as dis‐\\nplayed in Figure 7-4 , we can see they all fit in memory (recall that at a low level Data‐\\nFrames are backed by RDDs).\\nCaching and Persistence of Data | 183', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 206}), Document(page_content='Figure 7-4. Cache distributed across 12 partitions in executor memory\\nDataFrame.persist()\\npersist(StorageLevel. LEVEL) is nuanced, providing control over how your data is\\ncached via StorageLevel . Table 7-2  summarizes the different storage levels. Data on\\ndisk is always serialized using either Java or Kryo serialization .\\n184 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 207}), Document(page_content='Table 7-2. StorageLevels\\nStorageLevel Description\\nMEMORY_ONLY Data is stored directly as objects and stored only in memory.\\nMEMORY_ONLY_SER Data is serialized as compact byte array representation and stored only in memory. To use\\nit, it has to be deserialized at a cost.\\nMEMORY_AND_DISK Data is stored directly as objects in memory, but if there’s insufficient  memory the rest is\\nserialized and stored on disk.\\nDISK_ONLY Data is serialized and stored on disk.\\nOFF_HEAP Data is stored off-heap.  Off-heap  memory is used in Spark for storage and query execution ;\\nsee “Configuring  Spark executors’ memory and the shuffle  service” on page 178 .\\nMEMORY_AND_DISK_SER Like MEMORY_AND_DISK , but data is serialized when stored in memory. (Data is always\\nserialized when stored on disk.)\\nEach StorageLevel  (except OFF_HEAP ) has an equivalent\\nLEVEL_NAME_2 , which means replicate twice on two different Spark\\nexecutors: MEMORY_ONLY_2 , MEMORY_AND_DISK_SER_2 , etc. While\\nthis option is expensive, it allows data locality in two places, pro‐\\nviding fault tolerance and giving Spark the option to schedule a\\ntask local to a copy of the data.\\nLet’s look at the same example as in the previous section, but using the persist()\\nmethod:\\n// In Scala\\nimport org.apache.spark.storage.StorageLevel\\n// Create a DataFrame with 10M records\\nval df = spark.range(1 * 10000000 ).toDF(\"id\").withColumn (\"square\" , $\"id\" * $\"id\")\\ndf.persist(StorageLevel .DISK_ONLY ) // Serialize the data and cache it on disk\\ndf.count() // Materialize the cache\\nres2: Long = 10000000\\nCommand took 2.08 seconds\\ndf.count() // Now get it from the cache \\nres3: Long = 10000000\\nCommand took 0.38 seconds\\nAs you can see from Figure 7-5 , the data is persisted on disk, not in memory. To\\nunpersist your cached data, just call DataFrame.unpersist() .\\nCaching and Persistence of Data | 185', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 208}), Document(page_content='Figure 7-5. Cache distributed across 12 partitions in executor disk\\nFinally, not only can you cache DataFrames, but you can also cache the tables or\\nviews derived from DataFrames. This gives them more readable names in the Spark\\nUI. For example:\\n// In Scala\\ndf.createOrReplaceTempView (\"dfTable\" )\\nspark.sql(\"CACHE TABLE dfTable\" )\\nspark.sql(\"SELECT count(*) FROM dfTable\" ).show()\\n+--------+\\n|count(1)|\\n+--------+\\n|10000000 |\\n+--------+\\nCommand took 0.56 seconds\\n186 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 209}), Document(page_content='When to Cache and Persist\\nCommon use cases for caching are scenarios where you will want to access a large\\ndata set repeatedly for queries or transformations. Some examples include:\\n•DataFrames commonly used during iterative machine learning training\\n•DataFrames accessed commonly for doing frequent transformations during ETL\\nor building data pipelines\\nWhen Not to Cache and Persist\\nNot all use cases dictate the need to cache. Some scenarios that may not warrant cach‐\\ning your DataFrames include:\\n•DataFrames that are too big to fit in memory\\n•An inexpensive transformation on a DataFrame not requiring frequent use,\\nregardless of size\\nAs a general rule you should use memory caching judiciously, as it can incur resource\\ncosts in serializing and deserializing, depending on the StorageLevel  used.\\nNext, we’ll shift our focus to discuss a couple of common Spark join operations that\\ntrigger expensive movement of data, demanding compute and network resources\\nfrom the cluster, and how we can alleviate this movement by organizing the data.\\nA Family of Spark Joins\\nJoin operations are a common type of transformation in big data analytics in which\\ntwo data sets, in the form of tables or DataFrames, are merged over a common\\nmatching key. Similar to relational databases, the Spark DataFrame and Dataset APIs\\nand Spark SQL offer a series of join transformations: inner joins, outer joins, left\\njoins, right joins, etc. All of these operations trigger a large amount of data movement\\nacross Spark executors.\\nAt the heart of these transformations is how Spark computes what data to produce,\\nwhat keys and associated data to write to the disk, and how to transfer those keys and\\ndata to nodes as part of operations like groupBy() , join() , agg() , sortBy() , and\\nreduceByKey() . This movement is commonly referred to as the shuffle .\\nSpark has five distinct join strategies  by which it exchanges , moves, sorts, groups, and\\nmerges data across executors: the broadcast hash join (BHJ), shuffle hash join (SHJ),\\nshuffle sort merge join (SMJ), broadcast nested loop join (BNLJ), and shuffle-and-\\nreplicated nested loop join (a.k.a. Cartesian product join). We’ll focus on only two of\\nthese here (BHJ and SMJ), because they’re the most common ones you’ll encounter.\\nA Family of Spark Joins | 187', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 210}), Document(page_content='Broadcast Hash Join\\nAlso known as a map-side-only join , the broadcast hash join is employed when two\\ndata sets, one small (fitting in the driver’s and executor’s memory) and another large\\nenough to ideally be spared from movement, need to be joined over certain condi‐\\ntions or columns. Using a Spark broadcast variable , the smaller data set is broadcas‐\\nted by the driver to all Spark executors, as shown in Figure 7-6 , and subsequently\\njoined with the larger data set on each executor. This strategy avoids the large\\nexchange.\\nFigure 7-6. BHJ: the smaller data set is broadcast to all executors\\nBy default Spark will use a broadcast join if the smaller data set is less than 10 MB.\\nThis configuration is set in spark.sql.autoBroadcastJoinThreshold ; you can\\ndecrease or increase the size depending on how much memory you have on each\\nexecutor and in the driver. If you are confident that you have enough memory you\\ncan use a broadcast join with DataFrames larger than 10 MB (even up to 100 MB).\\nA common use case is when you have a common set of keys between two Data‐\\nFrames, one holding less information than the other, and you need a merged view of\\nboth. For example, consider a simple case where you have a large data set of soccer\\nplayers around the world, playersDF , and a smaller data set of soccer clubs they play\\nfor, clubsDF , and you wish to join them over a common key:\\n// In Scala \\nimport org.apache.spark.sql.functions.broadcast\\nval joinedDF  = playersDF .join(broadcast (clubsDF), \"key1 === key2\" )\\n188 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 211}), Document(page_content=\"In this code we are forcing Spark to do a broadcast join, but it will\\nresort to this type of join by default if the size of the smaller data set\\nis below the spark.sql.autoBroadcastJoinThreshold .\\nThe BHJ is the easiest and fastest join Spark offers, since it does not involve any shuf‐\\nfle of the data set; all the data is available locally to the executor after a broadcast. Y ou\\njust have to be sure that you have enough memory both on the Spark driver’s and the\\nexecutors’ side to hold the smaller data set in memory.\\nAt any time after the operation, you can see in the physical plan what join operation\\nwas performed by executing:\\njoinedDF.explain(mode)\\nIn Spark 3.0, you can use joinedDF.explain(' mode') to display a readable and\\ndigestible output. The modes include 'simple' , 'extended' , 'codegen' , 'cost' , and\\n'formatted' .\\nWhen to use a broadcast hash join\\nUse this type of join under the following conditions for maximum benefit:\\n•When each key within the smaller and larger data sets is hashed to the same par‐\\ntition by Spark\\n•When one data set is much smaller than the other (and within the default config\\nof 10 MB, or more if you have sufficient memory)\\n•When you only want to perform an equi-join, to combine two data sets based on\\nmatching unsorted keys\\n•When you are not worried by excessive network bandwidth usage or OOM\\nerrors, because the smaller data set will be broadcast to all Spark executors\\nSpecifying a value of -1 in spark.sql.autoBroadcastJoinThreshold  will cause\\nSpark to always resort to a shuffle sort merge join, which we discuss in the next\\nsection.\\nShuffle  Sort Merge Join\\nThe sort-merge algorithm is an efficient way to merge two large data sets over a com‐\\nmon key that is sortable, unique, and can be assigned to or stored in the same parti‐\\ntion—that is, two data sets with a common hashable key that end up being on the\\nsame partition. From Spark’s perspective, this means that all rows within each data set\\nwith the same key are hashed on the same partition on the same executor. Obviously,\\nthis means data has to be colocated or exchanged between executors.\\nA Family of Spark Joins | 189\", metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 212}), Document(page_content='As the name indicates, this join scheme has two phases: a sort phase followed by a\\nmerge phase. The sort phase sorts each data set by its desired join key; the merge\\nphase iterates over each key in the row from each data set and merges the rows if the\\ntwo keys match.\\nBy default, the SortMergeJoin  is enabled via spark.sql.join.preferSortMerge\\nJoin . Here is a code snippet from a notebook of standalone applications available for\\nthis chapter in the book’s GitHub repo . The main idea is to take two large Data‐\\nFrames, with one million records, and join them on two common keys, uid ==\\nusers_id .\\nThis data is synthetic but illustrates the point:\\n// In Scala\\nimport scala.util.Random\\n// Show preference over other joins for large data sets\\n// Disable broadcast join\\n// Generate data\\n...\\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\" , \"-1\")\\n// Generate some sample data for two data sets\\nvar states = scala.collection .mutable.Map[Int, String]()\\nvar items = scala.collection .mutable.Map[Int, String]()\\nval rnd = new scala.util.Random(42)\\n// Initialize states and items purchased\\nstates += (0 -> \"AZ\", 1 -> \"CO\", 2-> \"CA\", 3-> \"TX\", 4 -> \"NY\", 5-> \"MI\")\\nitems += (0 -> \"SKU-0\", 1 -> \"SKU-1\", 2-> \"SKU-2\", 3-> \"SKU-3\", 4 -> \"SKU-4\", \\n    5-> \"SKU-5\")\\n// Create DataFrames\\nval usersDF = (0 to 1000000).map(id => (id, s\"user_${id}\",\\n    s\"user_${id}@databricks.com\" , states(rnd.nextInt(5))))\\n    .toDF(\"uid\", \"login\", \"email\", \"user_state\" )\\nval ordersDF  = (0 to 1000000)\\n    .map(r => (r, r, rnd.nextInt(10000), 10 * r* 0.2d,\\n    states(rnd.nextInt(5)), items(rnd.nextInt(5))))\\n    .toDF(\"transaction_id\" , \"quantity\" , \"users_id\" , \"amount\" , \"state\", \"items\")\\n// Do the join \\nval usersOrdersDF  = ordersDF .join(usersDF, $\"users_id\"  === $\"uid\")\\n// Show the joined results\\nusersOrdersDF .show(false)\\n+--------------+--------+--------+--------+-----+-----+---+---+----------+\\n|transaction_id |quantity |users_id |amount  |state|items|uid|...|user_state |\\n+--------------+--------+--------+--------+-----+-----+---+---+----------+\\n|3916          |3916    |148     |7832.0  |CA   |SKU-1|148|...|CO        |\\n|36384         |36384   |148     |72768.0 |NY   |SKU-2|148|...|CO        |\\n190 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 213}), Document(page_content='|41839         |41839   |148     |83678.0 |CA   |SKU-3|148|...|CO        |\\n|48212         |48212   |148     |96424.0 |CA   |SKU-4|148|...|CO        |\\n|48484         |48484   |148     |96968.0 |TX   |SKU-3|148|...|CO        |\\n|50514         |50514   |148     |101028.0 |CO   |SKU-0|148|...|CO        |\\n|65694         |65694   |148     |131388.0 |TX   |SKU-4|148|...|CO        |\\n|65723         |65723   |148     |131446.0 |CA   |SKU-1|148|...|CO        |\\n|93125         |93125   |148     |186250.0 |NY   |SKU-3|148|...|CO        |\\n|107097        |107097  |148     |214194.0 |TX   |SKU-2|148|...|CO        |\\n|111297        |111297  |148     |222594.0 |AZ   |SKU-3|148|...|CO        |\\n|117195        |117195  |148     |234390.0 |TX   |SKU-4|148|...|CO        |\\n|253407        |253407  |148     |506814.0 |NY   |SKU-4|148|...|CO        |\\n|267180        |267180  |148     |534360.0 |AZ   |SKU-0|148|...|CO        |\\n|283187        |283187  |148     |566374.0 |AZ   |SKU-3|148|...|CO        |\\n|289245        |289245  |148     |578490.0 |AZ   |SKU-0|148|...|CO        |\\n|314077        |314077  |148     |628154.0 |CO   |SKU-3|148|...|CO        |\\n|322170        |322170  |148     |644340.0 |TX   |SKU-3|148|...|CO        |\\n|344627        |344627  |148     |689254.0 |NY   |SKU-3|148|...|CO        |\\n|345611        |345611  |148     |691222.0 |TX   |SKU-3|148|...|CO        |\\n+--------------+--------+--------+--------+-----+-----+---+---+----------+\\nonly showing top 20 rows\\nExamining our final execution plan, we notice that Spark employed a SortMergeJoin ,\\nas expected, to join the two DataFrames. The Exchange  operation is the shuffle of the\\nresults of the map operation on each executor:\\nusersOrdersDF.explain() \\n== Physical Plan ==\\nInMemoryTableScan [transaction_id#40, quantity#41, users_id#42, amount#43,\\nstate#44, items#45, uid#13, login#14, email#15, user_state#16]\\n   +- InMemoryRelation [transaction_id#40, quantity#41, users_id#42, amount#43,\\nstate#44, items#45, uid#13, login#14, email#15, user_state#16], \\nStorageLevel(disk, memory, deserialized, 1 replicas)\\n         +- *(3) SortMergeJoin  [users_id#42], [uid#13], Inner\\n            :- *(1) Sort [users_id#42 ASC NULLS FIRST], false, 0\\n            :  +- Exchange hashpartitioning(users_id#42, 16), true, [id=#56]\\n            :     +- LocalTableScan [transaction_id#40, quantity#41, users_id#42,\\namount#43, state#44, items#45]\\n            +- *(2) Sort [uid#13 ASC NULLS FIRST], false, 0\\n               +- Exchange hashpartitioning (uid#13, 16), true, [id=#57]\\n                  +- LocalTableScan [uid#13, login#14, email#15, user_state#16]\\nA Family of Spark Joins | 191', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 214}), Document(page_content='Furthermore, the Spark UI (which we will discuss in the next section) shows three\\nstages for the entire job: the Exchange  and Sort  operations happen in the final stage,\\nfollowed by merging of the results, as depicted in Figures 7-7 and 7-8. The Exchange\\nis expensive and requires partitions to be shuffled across the network between\\nexecutors.\\nFigure 7-7. Before bucketing: stages of the Spark\\n192 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 215}), Document(page_content='Figure 7-8. Before bucketing: Exchange is required\\nOptimizing the shuffle  sort merge join\\nWe can eliminate the Exchange  step from this scheme if we create partitioned buckets\\nfor common sorted keys or columns on which we want to perform frequent equi-\\njoins. That is, we can create an explicit number of buckets to store specific sorted col‐\\numns (one key per bucket). Presorting and reorganizing data in this way boosts\\nperformance, as it allows us to skip the expensive Exchange  operation and go straight\\nto WholeStageCodegen .\\nIn the following code snippet from the notebook for this chapter (available in the\\nbook’s GitHub repo ) we sort and bucket by the users_id  and uid columns on which\\nwe’ll join, and save the buckets as Spark managed tables in Parquet format:\\nA Family of Spark Joins | 193', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 216}), Document(page_content='// In Scala\\nimport org.apache.spark.sql.functions._\\nimport org.apache.spark.sql.SaveMode\\n// Save as managed tables by bucketing them in Parquet format\\nusersDF.orderBy(asc(\"uid\"))\\n  .write.format(\"parquet\" )\\n  .bucketBy (8, \"uid\")\\n  .mode(SaveMode .OverWrite )\\n  .saveAsTable (\"UsersTbl\" )\\nordersDF .orderBy(asc(\"users_id\" ))\\n  .write.format(\"parquet\" )\\n  .bucketBy (8, \"users_id\" )\\n  .mode(SaveMode .OverWrite )\\n  .saveAsTable (\"OrdersTbl\" )\\n// Cache the tables\\nspark.sql(\"CACHE TABLE UsersTbl\" )\\nspark.sql(\"CACHE TABLE OrdersTbl\" )\\n// Read them back in\\nval usersBucketDF  = spark.table(\"UsersTbl\" )\\nval ordersBucketDF  = spark.table(\"OrdersTbl\" )\\n// Do the join and show the results\\nval joinUsersOrdersBucketDF  = ordersBucketDF\\n    .join(usersBucketDF , $\"users_id\"  === $\"uid\")\\njoinUsersOrdersBucketDF .show(false)\\n+--------------+--------+--------+---------+-----+-----+---+---+----------+\\n|transaction_id |quantity |users_id |amount   |state|items|uid|...|user_state |\\n+--------------+--------+--------+---------+-----+-----+---+---+----------+\\n|144179        |144179  |22      |288358.0  |TX   |SKU-4|22 |...|CO        |\\n|145352        |145352  |22      |290704.0  |NY   |SKU-0|22 |...|CO        |\\n|168648        |168648  |22      |337296.0  |TX   |SKU-2|22 |...|CO        |\\n|173682        |173682  |22      |347364.0  |NY   |SKU-2|22 |...|CO        |\\n|397577        |397577  |22      |795154.0  |CA   |SKU-3|22 |...|CO        |\\n|403974        |403974  |22      |807948.0  |CO   |SKU-2|22 |...|CO        |\\n|405438        |405438  |22      |810876.0  |NY   |SKU-1|22 |...|CO        |\\n|417886        |417886  |22      |835772.0  |CA   |SKU-3|22 |...|CO        |\\n|420809        |420809  |22      |841618.0  |NY   |SKU-4|22 |...|CO        |\\n|659905        |659905  |22      |1319810.0 |AZ   |SKU-1|22 |...|CO        |\\n|899422        |899422  |22      |1798844.0 |TX   |SKU-4|22 |...|CO        |\\n|906616        |906616  |22      |1813232.0 |CO   |SKU-2|22 |...|CO        |\\n|916292        |916292  |22      |1832584.0 |TX   |SKU-0|22 |...|CO        |\\n|916827        |916827  |22      |1833654.0 |TX   |SKU-1|22 |...|CO        |\\n|919106        |919106  |22      |1838212.0 |TX   |SKU-1|22 |...|CO        |\\n|921921        |921921  |22      |1843842.0 |AZ   |SKU-4|22 |...|CO        |\\n|926777        |926777  |22      |1853554.0 |CO   |SKU-2|22 |...|CO        |\\n|124630        |124630  |22      |249260.0  |CO   |SKU-0|22 |...|CO        |\\n194 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 217}), Document(page_content='|129823        |129823  |22      |259646.0  |NY   |SKU-4|22 |...|CO        |\\n|132756        |132756  |22      |265512.0  |AZ   |SKU-2|22 |...|CO        |\\n+--------------+--------+--------+---------+-----+-----+---+---+----------+\\nonly showing top 20 rows\\nThe joined output is sorted by uid and users_id , because we saved the tables sorted\\nin ascending order. As such, there’s no need to sort during the SortMergeJoin . Look‐\\ning at the Spark UI ( Figure 7-9 ), we can see that we skipped the Exchange  and went\\nstraight to WholeStageCodegen .\\nThe physical plan also shows no Exchange  was performed, compared to the physical\\nplan before bucketing:\\njoinUsersOrdersBucketDF.explain()\\n== Physical Plan ==\\n*(3) SortMergeJoin [users_id#165], [uid#62], Inner\\n:- *(1) Sort [users_id#165 ASC NULLS FIRST], false, 0\\n:  +- *(1) Filter isnotnull(users_id#165)\\n:     +- Scan In-memory table `OrdersTbl` [transaction_id#163, quantity#164,\\nusers_id#165, amount#166, state#167, items#168], [isnotnull(users_id#165)]\\n:           +- InMemoryRelation [transaction_id#163, quantity#164, users_id#165,\\namount#166, state#167, items#168], StorageLevel(disk, memory, deserialized, 1\\nreplicas)\\n:                 +- *(1) ColumnarToRow\\n:                    +- FileScan parquet \\n...\\nA Family of Spark Joins | 195', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 218}), Document(page_content='Figure 7-9. After  bucketing: Exchange is not required\\n196 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 219}), Document(page_content='When to use a shuffle  sort merge join\\nUse this type of join under the following conditions for maximum benefit:\\n•When each key within two large data sets can be sorted and hashed to the same\\npartition by Spark\\n•When you want to perform only equi-joins to combine two data sets based on\\nmatching sorted keys\\n•When you want to prevent Exchange  and Sort  operations to save large shuffles\\nacross the network\\nSo far we have covered operational aspects related to tuning and optimizing Spark,\\nand how Spark exchanges data during two common join operations. We also demon‐\\nstrated how you can boost the performance of a shuffle sort merge join operation by\\nusing bucketing to avoid large exchanges of data.\\nAs you’ve seen in the preceding figures, the Spark UI is a useful way to visualize these\\noperations. It shows collected metrics and the state of the program, revealing a wealth\\nof information and clues about possible performance bottlenecks. In the final section\\nof this chapter, we discuss what to look for in the Spark UI.\\nInspecting the Spark UI\\nSpark provides an elaborate web UI that allows us to inspect various components of\\nour applications. It offers details on memory usage, jobs, stages, and tasks, as well as\\nevent timelines, logs, and various metrics and statistics that can give you insight into\\nwhat transpires in your Spark applications, both at the Spark driver level and in indi‐\\nvidual executors.\\nA spark-submit  job will launch the Spark UI, and you can connect to it on the local\\nhost (in local mode) or through the Spark driver (in other modes) at the default port\\n4040.\\nJourney Through the Spark UI Tabs\\nThe Spark UI has six tabs, as shown in Figure 7-10 , each providing opportunities for\\nexploration. Let’s take a look at what each tab reveals to us.\\nFigure 7-10. Spark UI tabs\\nInspecting the Spark UI | 197', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 220}), Document(page_content='This discussion applies to Spark 2.x and Spark 3.0. While much of the UI is the same\\nin Spark 3.0, it also adds a seventh tab, Structured Streaming. This is previewed in\\nChapter 12 .\\nJobs and Stages\\nAs you learned in Chapter 2 , Spark breaks an application down into jobs, stages, and\\ntasks. The Jobs and Stages tabs allow you to navigate through these and drill down to\\na granular level to examine the details of individual tasks. Y ou can view their comple‐\\ntion status and review metrics related to I/O, memory consumption, duration of exe‐\\ncution, etc.\\nFigure 7-11  shows the Jobs tab with the expanded Event Timeline, showing when\\nexecutors were added to or removed from the cluster. It also provides a tabular list of\\nall completed jobs in the cluster. The Duration column indicates the time it took for\\neach job (identified by the Job Id in the first column) to finish. If this time is high, it’s\\na good indication that you might want to investigate the stages in that job to see what\\ntasks might be causing delays. From this summary page you can also access a details\\npage for each job, including a DAG visualization and list of completed stages.\\nFigure 7-11. The Jobs tab offers  a view of the event timeline and list of all completed jobs\\n198 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 221}), Document(page_content='The Stages tab provides a summary of the current state of all stages of all jobs in the\\napplication. Y ou can also access a details page for each stage, providing a DAG and\\nmetrics on its tasks ( Figure 7-12 ). As well as some other optional statistics, you can\\nsee the average duration of each task, time spent in garbage collection (GC), and\\nnumber of shuffle bytes/records read. If shuffle data is being read from remote execu‐\\ntors, a high Shuffle Read Blocked Time can signal I/O issues. A high GC time signals\\ntoo many objects on the heap (your executors may be memory-starved). If a stage’s\\nmax task time is much larger than the median, then you probably have data skew\\ncaused by uneven data distribution in your partitions. Look for these tell-tale signs.\\nFigure 7-12. The Stages tab provides details on stages and their tasks\\nInspecting the Spark UI | 199', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 222}), Document(page_content='Y ou can also see aggregated metrics for each executor and a breakdown of the indi‐\\nvidual tasks on this page.\\nExecutors\\nThe Executors tab provides information on the executors created for the application.\\nAs you can see in Figure 7-13 , you can drill down into the minutiae of details about\\nresource usage (disk, memory, cores), time spent in GC, amount of data written and\\nread during shuffle, etc.\\nFigure 7-13. The Executors tab shows granular statistics and metrics on the executors\\nused by your Spark application\\nIn addition to the summary statistics, you can view how memory is used by each\\nindividual executor, and for what purpose. This also helps to examine resource usage\\nwhen you have used the cache()  or persist()  method on a DataFrame or managed\\ntable, which we discuss next.\\nStorage\\nIn the Spark code in “Shuffle Sort Merge Join” we cached two managed tables after\\nbucketing. The Storage tab, shown in Figure 7-14 , provides information on any tables\\nor DataFrames cached by the application as a result of the cache()  or persist()\\nmethod.\\n200 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 223}), Document(page_content='Figure 7-14. The Storage tab shows details on memory usage\\nGoing a bit further by clicking on the link “In-memory table `UsersTbl`” in\\nFigure 7-14  displays how the table is cached in memory and on disk across 1 executor\\nand 8 partitions—this number corresponds to the number of buckets we created for\\nthis table (see Figure 7-15 ).\\nFigure 7-15. Spark UI showing cached table distribution across executor memory\\nInspecting the Spark UI | 201', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 224}), Document(page_content='SQL\\nThe effects of Spark SQL queries that are executed as part of your Spark application\\nare traceable and viewable through the SQL tab. Y ou can see when the queries were\\nexecuted and by which jobs, and their duration. For example, in our SortMergeJoin\\nexample we executed some queries; all of them are displayed in Figure 7-16 , with\\nlinks to drill further down.\\nFigure 7-16. The SQL tab shows details on the completed SQL queries\\nClicking on the description of a query displays details of the execution plan with all\\nthe physical operators, as shown in Figure 7-17 . Under each physical operator of the\\nplan—here, Scan In-memory table , HashAggregate , and Exchange —are SQL\\nmetrics.\\nThese metrics are useful when we want to inspect the details of a physical operator\\nand discover what transpired: how many rows were scanned, how many shuffle bytes\\nwere written, etc.\\n202 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 225}), Document(page_content='Figure 7-17. Spark UI showing detailed statistics on a SQL query\\nEnvironment\\nThe Environment tab, shown in Figure 7-18 , is just as important as the others. Know‐\\ning about the environment in which your Spark application is running reveals many\\nclues that are useful for troubleshooting. In fact, it’s imperative to know what envi‐\\nronment variables are set, what jars are included, what Spark properties are set (and\\ntheir respective values, especially if you tweaked some of the configs mentioned in\\n“Optimizing and Tuning Spark for Efficiency” on page 173), what system properties\\nare set, what runtime environment (such as JVM or Java version) is used, etc. All\\nthese read-only details are a gold mine of information supplementing your investiga‐\\ntive efforts should you notice any abnormal behavior in your Spark application.\\nInspecting the Spark UI | 203', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 226}), Document(page_content='Figure 7-18. The Environment tab shows the runtime properties of your Spark cluster\\nDebugging Spark applications\\nIn this section, we have navigated through the various tabs in the Spark UI. As you’ve\\nseen, the UI provides a wealth of information that you can use for debugging and\\ntroubleshooting issues with your Spark applications. In addition to what we’ve cov‐\\nered here, it also provides access to both driver and executor stdout/stderr logs,\\nwhere you might have logged debugging information.\\nDebugging through the UI is a different process than stepping through an application\\nin your favorite IDE—more like sleuthing, following trails of bread crumbs—though\\n204 | Chapter 7: Optimizing and Tuning Spark Applications', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 227}), Document(page_content='if you prefer that approach, you can also debug a Spark application in an IDE such as\\nIntelliJ IDEA  on a local host.\\nThe Spark 3.0 UI tabs  reveal insightful bread crumbs about what happened, along\\nwith access to both driver and executor stdout/stderr logs, where you might have log‐\\nged debugging information.\\nInitially, this plethora of information can be overwhelming to a novice. But with time\\nyou’ll gain an understanding of what to look for in each tab, and you’ll begin to be\\nable to detect and diagnose anomalies more quickly. Patterns will become clear, and\\nby frequently visiting these tabs and getting familiar with them after running some\\nSpark examples, you’ll get accustomed to tuning and inspecting your Spark applica‐\\ntions via the UI.\\nSummary\\nIn this chapter we have discussed a number of optimization techniques for tuning\\nyour Spark applications. As you saw, by adjusting some of the default Spark configu‐\\nrations, you can improve scaling for large workloads, enhance parallelism, and mini‐\\nmize memory starvation among Spark executors. Y ou also got a glimpse of how you\\ncan use caching and persisting strategies with appropriate levels to expedite access to\\nyour frequently used data sets, and we examined two commonly used joins Spark\\nemploys during complex aggregations and demonstrated how by bucketing Data‐\\nFrames by sorted keys, you can skip over expensive shuffle operations.\\nFinally, to get a visual perspective on performance, the Spark UI completed the pic‐\\nture. Informative and detailed though the UI is, it’s not equivalent to step-debugging\\nin an IDE; yet we showed how you can become a Spark sleuth by examining and\\ngleaning insights from the metrics and statistics, compute and memory usage data,\\nand SQL query execution traces available on the half-dozen Spark UI tabs.\\nIn the next chapter, we’ll dive into Structured Streaming and show you how the Struc‐\\ntured APIs that you learned about in earlier chapters allow you to write both stream‐\\ning and batch applications in a continuous manner, enabling you to build reliable\\ndata lakes and pipelines.\\nSummary | 205', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 228}), Document(page_content='', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 229}), Document(page_content='CHAPTER 8\\nStructured Streaming\\nIn earlier chapters, you learned how to use structured APIs to process very large but\\nfinite volumes of data. However, often data arrives continuously and needs to be pro‐\\ncessed in a real-time manner. In this chapter, we will discuss how the same Structured\\nAPIs can be used for processing data streams as well.\\nEvolution of the Apache Spark Stream Processing Engine\\nStream processing is defined as the continuous processing of endless streams of data.\\nWith the advent of big data, stream processing systems transitioned from single-node\\nprocessing engines to multiple-node, distributed processing engines. Traditionally,\\ndistributed stream processing has been implemented with a record-at-a-time process‐\\ning model , as illustrated in Figure 8-1 .\\nFigure 8-1. Traditional record-at-a-time processing model\\n207', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 230}), Document(page_content='1For a more detailed explanation, see the original research paper “Discretized Streams: Fault-Tolerant Stream‐\\ning Computation at Scale”  by Matei Zaharia et al. (2013).The processing pipeline is composed of a directed graph of nodes, as shown in\\nFigure 8-1 ; each node continuously receives one record at a time, processes it, and\\nthen forwards the generated record(s) to the next node in the graph. This processing\\nmodel can achieve very low latencies—that is, an input record can be processed by\\nthe pipeline and the resulting output can be generated within milliseconds. However,\\nthis model is not very efficient at recovering from node failures and straggler nodes\\n(i.e., nodes that are slower than others); it can either recover from a failure very fast\\nwith a lot of extra failover resources, or use minimal extra resources but recover\\nslowly.1\\nThe Advent of Micro-Batch Stream Processing\\nThis traditional approach was challenged by Apache Spark when it introduced Spark\\nStreaming (also called DStreams). It introduced the idea of micro-batch stream pro‐\\ncessing , where the streaming computation is modeled as a continuous series of small,\\nmap/reduce-style batch processing jobs (hence, “micro-batches”) on small chunks of\\nthe stream data. This is illustrated in Figure 8-2 .\\nFigure 8-2. Structured Streaming uses a micro-batch processing model\\nAs shown here, Spark Streaming divides the data from the input stream into, say, 1-\\nsecond micro-batches. Each batch is processed in the Spark cluster in a distributed\\nmanner with small deterministic tasks that generate the output in micro-batches.\\nBreaking down the streaming computation into these small tasks gives us two advan‐\\ntages over the traditional, continuous-operator model:\\n208 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 231}), Document(page_content='•Spark’s agile task scheduling can very quickly and efficiently recover from failures\\nand straggler executors by rescheduling one or more copies of the tasks on any of\\nthe other executors.\\n•The deterministic nature of the tasks ensures that the output data is the same no\\nmatter how many times the task is reexecuted. This crucial characteristic enables\\nSpark Streaming to provide end-to-end exactly-once processing guarantees, that\\nis, the generated output results will be such that every input record was processed\\nexactly once.\\nThis efficient fault tolerance does come at the cost of latency—the micro-batch model\\ncannot achieve millisecond-level latencies; it usually achieves latencies of a few sec‐\\nonds (as low as half a second in some cases). However, we have observed that for an\\noverwhelming majority of stream processing use cases, the benefits of micro-batch\\nprocessing outweigh the drawback of second-scale latencies. This is because most\\nstreaming pipelines have at least one of the following characteristics:\\n•The pipeline does not need latencies lower than a few seconds. For example,\\nwhen the streaming output is only going to be read by hourly jobs, it is not useful\\nto generate output with subsecond latencies.\\n•There are larger delays in other parts of the pipeline. For example, if the writes by\\na sensor into Apache Kafka (a system for ingesting data streams) are batched to\\nachieve higher throughput, then no amount of optimization in the downstream\\nprocessing systems can make the end-to-end latency lower than the batching\\ndelays.\\nFurthermore, the DStream API was built upon Spark’s batch RDD API. Therefore,\\nDStreams had the same functional semantics and fault-tolerance model as RDDs.\\nSpark Streaming thus proved that it is possible for a single, unified processing engine\\nto provide consistent APIs and semantics for batch, interactive, and streaming work‐\\nloads. This fundamental paradigm shift in stream processing propelled Spark Stream‐\\ning to become one of the most widely used open source stream processing engines.\\nLessons Learned from Spark Streaming (DStreams)\\nDespite all the advantages, the DStream API was not without its flaws. Here are a few\\nkey areas for improvement that were identified:\\nEvolution of the Apache Spark Stream Processing Engine | 209', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 232}), Document(page_content='Lack of a single API for batch and stream processing\\nEven though DStreams and RDDs have consistent APIs (i.e., same operations\\nand same semantics), developers still had to explicitly rewrite their code to use\\ndifferent classes when converting their batch jobs to streaming jobs.\\nLack of separation between logical and physical plans\\nSpark Streaming executes the DStream operations in the same sequence in which\\nthey were specified by the developer. Since developers effectively specify the exact\\nphysical plan, there is no scope for automatic optimizations, and developers have\\nto hand-optimize their code to get the best performance.\\nLack of native support for event-time windows\\nDStreams define window operations based only on the time when each record is\\nreceived by Spark Streaming (known as processing time ). However, many use\\ncases need to calculate windowed aggregates based on the time when the records\\nwere generated (known as event time ) instead of when they were received or pro‐\\ncessed. The lack of native support of event-time windows made it hard for devel‐\\nopers to build such pipelines with Spark Streaming.\\nThese drawbacks shaped the design philosophy of Structured Streaming, which we\\nwill discuss next.\\nThe Philosophy of Structured Streaming\\nBased on these lessons from DStreams, Structured Streaming was designed from\\nscratch with one core philosophy—for developers, writing stream processing pipe‐\\nlines should be as easy as writing batch pipelines. In a nutshell, the guiding principles\\nof Structured Streaming are:\\nA single, unified  programming model and interface for batch and stream processing\\nThis unified model offers a simple API interface for both batch and streaming\\nworkloads. Y ou can use familiar SQL or batch-like DataFrame queries (like those\\nyou’ve learned about in the previous chapters) on your stream as you would on a\\nbatch, leaving dealing with the underlying complexities of fault tolerance, opti‐\\nmizations, and tardy data to the engine. In the coming sections, we will examine\\nsome of the queries you might write.\\nA broader definition  of stream processing\\nBig data processing applications have grown complex enough that the line\\nbetween real-time processing and batch processing has blurred significantly. The\\naim with Structured Streaming was to broaden its applicability from traditional\\nstream processing to a larger class of applications; any application that periodi‐\\ncally (e.g., every few hours) to continuously (like traditional streaming applica‐\\ntions) processes data should be expressible using Structured Streaming.\\n210 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 233}), Document(page_content='Next, we’ll discuss the programming model used by Structured Streaming.\\nThe Programming Model of Structured Streaming\\n“Table” is a well-known concept that developers are familiar with when building\\nbatch applications. Structured Streaming extends this concept to streaming applica‐\\ntions by treating a stream as an unbounded, continuously appended table, as illustra‐\\nted in Figure 8-3 .\\nFigure 8-3. The Structured Streaming programming model: data stream as an unboun‐\\nded table\\nEvery new record received in the data stream is like a new row being appended to the\\nunbounded input table. Structured Streaming will not actually retain all the input,\\nbut the output produced by Structured Streaming until time T will be equivalent to\\nhaving all of the input until T in a static, bounded table and running a batch job on\\nthe table.\\nAs shown in Figure 8-4 , the developer then defines a query on this conceptual input\\ntable, as if it were a static table, to compute the result table that will be written to an\\noutput sink. Structured Streaming will automatically convert this batch-like query to\\na streaming execution plan. This is called incrementalization : Structured Streaming\\nfigures out what state needs to be maintained to update the result each time a record\\narrives. Finally, developers specify triggering policies to control when to update the\\nresults. Each time a trigger fires, Structured Streaming checks for new data (i.e., a new\\nrow in the input table) and incrementally updates the result.\\nThe Programming Model of Structured Streaming | 211', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 234}), Document(page_content='Figure 8-4. The Structured Streaming processing model\\nThe last part of the model is the output mode. Each time the result table is updated,\\nthe developer will want to write the updates to an external system, such as a filesys‐\\ntem (e.g., HDFS, Amazon S3) or a database (e.g., MySQL, Cassandra). We usually\\nwant to write output incrementally. For this purpose, Structured Streaming provides\\nthree output modes:\\nAppend mode\\nOnly the new rows appended to the result table since the last trigger will be writ‐\\nten to the external storage. This is applicable only in queries where existing rows\\nin the result table cannot change (e.g., a map on an input stream).\\nUpdate mode\\nOnly the rows that were updated in the result table since the last trigger will be\\nchanged in the external storage. This mode works for output sinks that can be\\nupdated in place, such as a MySQL table.\\nComplete mode\\nThe entire updated result table will be written to external storage.\\n212 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 235}), Document(page_content='Unless complete mode is specified, the result table will not be fully\\nmaterialized by Structured Streaming. Just enough information\\n(known as “state”) will be maintained to ensure that the changes in\\nthe result table can be computed and the updates can be output.\\nThinking of the data streams as tables not only makes it easier to conceptualize the\\nlogical computations on the data, but also makes it easier to express them in code.\\nSince Spark’s DataFrame is a programmatic representation of a table, you can use the\\nDataFrame API to express your computations on streaming data. All you need to do\\nis define an input DataFrame (i.e., the input table) from a streaming data source, and\\nthen you apply operations on the DataFrame in the same way as you would on a\\nDataFrame defined on a batch source.\\nIn the next section, you will see how easy it is to write Structured Streaming queries\\nusing DataFrames.\\nThe Fundamentals of a Structured Streaming Query\\nIn this section, we are going to cover some high-level concepts that you’ll need to\\nunderstand to develop Structured Streaming queries. We will first walk through the\\nkey steps to define and start a streaming query, then we will discuss how to monitor\\nthe active query and manage its life cycle.\\nFive Steps to Define  a Streaming Query\\nAs discussed in the previous section, Structured Streaming uses the same DataFrame\\nAPI as batch queries to express the data processing logic. However, there are a few\\nkey differences you need to know about for defining a Structured Streaming query. In\\nthis section, we will explore the steps involved in defining a streaming query by\\nbuilding a simple query that reads streams of text data over a socket and counts the\\nwords.\\nStep 1: Define  input sources\\nAs with batch queries, the first step is to define a DataFrame from a streaming source.\\nHowever, when reading batch data sources, we need spark.read  to create a DataFra\\nmeReader , whereas with streaming sources we need spark.readStream  to create a\\nDataStreamReader . DataStreamReader  has most of the same methods as DataFrameR\\neader , so you can use it in a similar way. Here is an example of creating a DataFrame\\nfrom a text data stream to be received over a socket connection:\\nThe Fundamentals of a Structured Streaming Query | 213', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 236}), Document(page_content='# In Python\\nspark = SparkSession ...\\nlines = (spark\\n  .readStream .format(\"socket\" )\\n  .option(\"host\", \"localhost\" )\\n  .option(\"port\", 9999)\\n  .load())\\n// In Scala \\nval spark = SparkSession ...\\nval lines = spark\\n  .readStream .format(\"socket\" )\\n  .option(\"host\", \"localhost\" )\\n  .option(\"port\", 9999)\\n  .load()\\nThis code generates the lines  DataFrame as an unbounded table of newline-\\nseparated text data read from localhost:9999. Note that, similar to batch sources with\\nspark.read , this does not immediately start reading the streaming data; it only sets\\nup the configurations necessary for reading the data once the streaming query is\\nexplicitly started.\\nBesides sockets, Apache Spark natively supports reading data streams from Apache\\nKafka and all the various file-based formats that DataFrameReader  supports (Parquet,\\nORC, JSON, etc.). The details of these sources and their supported options are dis‐\\ncussed later in this chapter. Furthermore, a streaming query can define multiple input\\nsources, both streaming and batch, which can be combined using DataFrame opera‐\\ntions like unions and joins (also discussed later in this chapter).\\nStep 2: Transform data\\nNow we can apply the usual DataFrame operations, such as splitting the lines into\\nindividual words and then counting them, as shown in the following code:\\n# In Python\\nfrom pyspark.sql.functions  import *\\nwords = lines.select(split(col(\"value\"), \"\\\\\\\\s\").alias(\"word\"))\\ncounts = words.groupBy(\"word\").count()\\n// In Scala\\nimport org.apache.spark.sql.functions._\\nval words = lines.select(split(col(\"value\"), \"\\\\\\\\s\").as(\"word\"))\\nval counts = words.groupBy(\"word\").count()\\ncounts  is a streaming DataFrame  (that is, a DataFrame on unbounded, streaming\\ndata) that represents the running word counts that will be computed once the stream‐\\ning query is started and the streaming input data is being continuously processed.\\nNote that these operations to transform the lines  streaming DataFrame would work\\nin the exact same way if lines  were a batch DataFrame. In general, most DataFrame\\n214 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 237}), Document(page_content='operations that can be applied on a batch DataFrame can also be applied on a stream‐\\ning DataFrame. To understand which operations are supported in Structured Stream‐\\ning, you have to recognize the two broad classes of data transformations:\\nStateless transformations\\nOperations like select() , filter() , map() , etc. do not require any information\\nfrom previous rows to process the next row; each row can be processed by itself.\\nThe lack of previous “state” in these operations make them stateless. Stateless\\noperations can be applied to both batch and streaming DataFrames.\\nStateful transformations\\nIn contrast, an aggregation operation like count()  requires maintaining state to\\ncombine data across multiple rows. More specifically, any DataFrame operations\\ninvolving grouping, joining, or aggregating are stateful transformations. While\\nmany of these operations are supported in Structured Streaming, a few combina‐\\ntions of them are not supported because it is either computationally hard or\\ninfeasible to compute them in an incremental manner.\\nThe stateful operations supported by Structured Streaming and how to manage their\\nstate at runtime are discussed later in the chapter.\\nStep 3: Define  output sink and output mode\\nAfter transforming the data, we can define how to write the processed output data\\nwith DataFrame.writeStream  (instead of DataFrame.write , used for batch data).\\nThis creates a DataStreamWriter  which, similar to DataFrameWriter , has additional\\nmethods to specify the following:\\n•Output writing details (where and how to write the output)\\n•Processing details (how to process data and how to recover from failures)\\nLet’s start with the output writing details (we will focus on the processing details in\\nthe next step). For example, the following snippet shows how to write the final\\ncounts  to the console:\\n# In Python\\nwriter = counts.writeStream .format(\"console\" ).outputMode (\"complete\" )\\n// In Scala\\nval writer = counts.writeStream .format(\"console\" ).outputMode (\"complete\" )\\nHere we have specified \"console\"  as the output streaming sink and \"complete\"  as\\nthe output mode. The output mode of a streaming query specifies what part of the\\nupdated output to write out after processing new input data. In this example, as a\\nchunk of new input data is processed and the word counts are updated, we can\\nchoose to print to the console either the counts of all the words seen until now (that\\nThe Fundamentals of a Structured Streaming Query | 215', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 238}), Document(page_content='is, complete mode ), or only those words that were updated in the last chunk of input\\ndata. This is decided by the specified output mode, which can be one of the following\\n(as we already saw in “The Programming Model of Structured Streaming” on page\\n211:\\nAppend mode\\nThis is the default mode, where only the new rows added to the result table/Data‐\\nFrame (for example, the counts  table) since the last trigger will be output to the\\nsink. Semantically, this mode guarantees that any row that is output is never\\ngoing to be changed or updated by the query in the future. Hence, append mode\\nis supported by only those queries (e.g., stateless queries) that will never modify\\npreviously output data. In contrast, our word count query can update previously\\ngenerated counts; therefore, it does not support append mode.\\nComplete mode\\nIn this mode, all the rows of the result table/DataFrame will be output at the end\\nof every trigger. This is supported by queries where the result table is likely to be\\nmuch smaller than the input data and therefore can feasibly be retained in mem‐\\nory. For example, our word count query supports complete mode because the\\ncounts data is likely to be far smaller than the input data.\\nUpdate mode\\nIn this mode, only the rows of the result table/DataFrame that were updated\\nsince the last trigger will be output at the end of every trigger. This is in contrast\\nto append mode, as the output rows may be modified by the query and output\\nagain in the future. Most queries support update mode.\\nComplete details on the output modes supported by different quer‐\\nies can be found in the latest Structured Streaming Programming\\nGuide .\\nBesides writing the output to the console, Structured Streaming natively supports\\nstreaming writes to files and Apache Kafka. In addition, you can write to arbitrary\\nlocations using the foreachBatch()  and foreach()  API methods. In fact, you can\\nuse foreachBatch()  to write streaming outputs using existing batch data sources\\n(but you will lose exactly-once guarantees). The details of these sinks and their sup‐\\nported options are discussed later in this chapter.\\nStep 4: Specify processing details\\nThe final step before starting the query is to specify details of how to process the data.\\nContinuing with our word count example, we are going to specify the processing\\ndetails as follows:\\n216 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 239}), Document(page_content='# In Python\\ncheckpointDir  = \"...\"\\nwriter2 = (writer\\n  .trigger(processingTime =\"1 second\" )\\n  .option(\"checkpointLocation\" , checkpointDir ))\\n// In Scala\\nimport org.apache.spark.sql.streaming._\\nval checkpointDir  = \"...\"\\nval writer2 = writer\\n  .trigger(Trigger.ProcessingTime (\"1 second\" ))\\n  .option(\"checkpointLocation\" , checkpointDir )\\nHere we have specified two types of details using the DataStreamWriter  that we cre‐\\nated with DataFrame.writeStream :\\nTriggering details\\nThis indicates when to trigger the discovery and processing of newly available\\nstreaming data. There are four options:\\nDefault\\nWhen the trigger is not explicitly specified, then by default, the streaming\\nquery executes data in micro-batches where the next micro-batch is trig‐\\ngered as soon as the previous micro-batch has completed.\\nProcessing time with trigger interval\\nY ou can explicitly specify the ProcessingTime  trigger with an interval, and\\nthe query will trigger micro-batches at that fixed interval.\\nOnce\\nIn this mode, the streaming query will execute exactly one micro-batch—it\\nprocesses all the new data available in a single batch and then stops itself.\\nThis is useful when you want to control the triggering and processing from\\nan external scheduler that will restart the query using any custom schedule\\n(e.g., to control cost by only executing a query once per day ).\\nContinuous\\nThis is an experimental mode (as of Spark 3.0) where the streaming query\\nwill process data continuously instead of in micro-batches. While only a\\nsmall subset of DataFrame operations allow this mode to be used, it can pro‐\\nvide much lower latency (as low as milliseconds) than the micro-batch trig‐\\nger modes. Refer to the latest Structured Streaming Programming Guide  for\\nthe most up-to-date information.\\nThe Fundamentals of a Structured Streaming Query | 217', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 240}), Document(page_content='Checkpoint location\\nThis is a directory in any HDFS-compatible filesystem where a streaming query\\nsaves its progress information—that is, what data has been successfully pro‐\\ncessed. Upon failure, this metadata is used to restart the failed query exactly\\nwhere it left off. Therefore, setting this option is necessary for failure recovery\\nwith exactly-once guarantees.\\nStep 5: Start the query\\nOnce everything has been specified, the final step is to start the query, which you can\\ndo with the following:\\n# In Python\\nstreamingQuery  = writer2.start()\\n// In Scala\\nval streamingQuery  = writer2.start()\\nThe returned object of type streamingQuery  represents an active query and can be\\nused to manage the query, which we will cover later in this chapter.\\nNote that start()  is a nonblocking method, so it will return as soon as the query has\\nstarted in the background. If you want the main thread to block until the streaming\\nquery has terminated, you can use streamingQuery.awaitTermination() . If the\\nquery fails in the background with an error, awaitTermination()  will also fail with\\nthat same exception.\\nY ou can wait up to a timeout duration using awaitTermination(timeoutMillis) ,\\nand you can explicitly stop the query with streamingQuery.stop() .\\nPutting it all together\\nTo summarize, here is the complete code for reading streams of text data over a\\nsocket, counting the words, and printing the counts to the console:\\n# In Python\\nfrom pyspark.sql.functions  import *\\nspark = SparkSession ...\\nlines = (spark\\n  .readStream .format(\"socket\" )\\n  .option(\"host\", \"localhost\" )\\n  .option(\"port\", 9999)\\n  .load())\\nwords = lines.select(split(col(\"value\"), \"\\\\\\\\s\").alias(\"word\"))\\ncounts = words.groupBy(\"word\").count()\\ncheckpointDir  = \"...\"\\nstreamingQuery  = (counts\\n  .writeStream\\n  .format(\"console\" )\\n218 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 241}), Document(page_content='2This execution loop runs for micro-batch-based trigger modes (i.e., ProcessingTime  and Once ), but not for\\nthe Continuous  trigger mode.  .outputMode (\"complete\" )\\n  .trigger(processingTime =\"1 second\" )\\n  .option(\"checkpointLocation\" , checkpointDir )\\n  .start())\\nstreamingQuery .awaitTermination ()\\n// In Scala\\nimport org.apache.spark.sql.functions._\\nimport org.apache.spark.sql.streaming._\\nval spark = SparkSession ...\\nval lines = spark\\n  .readStream .format(\"socket\" )\\n  .option(\"host\", \"localhost\" )\\n  .option(\"port\", 9999)\\n  .load()\\nval words = lines.select(split(col(\"value\"), \"\\\\\\\\s\").as(\"word\"))\\nval counts = words.groupBy(\"word\").count()\\nval checkpointDir  = \"...\"\\nval streamingQuery  = counts.writeStream\\n  .format(\"console\" )\\n  .outputMode (\"complete\" )\\n  .trigger(Trigger.ProcessingTime (\"1 second\" ))\\n  .option(\"checkpointLocation\" , checkpointDir )\\n  .start()\\nstreamingQuery .awaitTermination ()\\nAfter the query has started, a background thread continuously reads new data from\\nthe streaming source, processes it, and writes it to the streaming sinks. Next, let’s take\\na quick peek under the hood at how this is executed.\\nUnder the Hood of an Active Streaming Query\\nOnce the query starts, the following sequence of steps transpires in the engine, as\\ndepicted in Figure 8-5 . The DataFrame operations are converted into a logical plan,\\nwhich is an abstract representation of the computation that Spark SQL uses to plan a\\nquery:\\n1.Spark SQL analyzes and optimizes this logical plan to ensure that it can be exe‐\\ncuted incrementally and efficiently on streaming data.\\n2.Spark SQL starts a background thread that continuously executes the following\\nloop:2\\nThe Fundamentals of a Structured Streaming Query | 219', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 242}), Document(page_content='a.Based on the configured trigger interval, the thread checks the streaming\\nsources for the availability of new data.\\nb.If available, the new data is executed by running a micro-batch. From the\\noptimized logical plan, an optimized Spark execution plan is generated that\\nreads the new data from the source, incrementally computes the updated\\nresult, and writes the output to the sink according to the configured output\\nmode.\\nc.For every micro-batch, the exact range of data processed (e.g., the set of files\\nor the range of Apache Kafka offsets) and any associated state are saved in the\\nconfigured checkpoint location so that the query can deterministically reproc‐\\ness the exact range if needed.\\n3.This loop continues until the query is terminated, which can occur for one of the\\nfollowing reasons:\\na.A failure has occurred in the query (either a processing error or a failure in\\nthe cluster).\\nb.The query is explicitly stopped using streamingQuery.stop() .\\nc.If the trigger is set to Once , then the query will stop on its own after executing\\na single micro-batch containing all the available data.\\nFigure 8-5. Incremental execution of streaming queries\\n220 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 243}), Document(page_content='A key point you should remember about Structured Streaming is\\nthat underneath it is using Spark SQL to execute the data. As such,\\nthe full power of Spark SQL ’s hyperoptimized execution engine is\\nutilized to maximize the stream processing throughput, providing\\nkey performance advantages.\\nNext, we will discuss how to restart a streaming query after termination and the life\\ncycle of a streaming query.\\nRecovering from Failures with Exactly-Once Guarantees\\nTo restart a terminated query in a completely new process, you have to create a new\\nSparkSession , redefine all the DataFrames, and start the streaming query on the final\\nresult using the same checkpoint location as the one used when the query was started\\nthe first time. For our word count example, you can simply reexecute the entire code\\nsnippet shown earlier, from the definition of spark  in the first line to the final\\nstart()  in the last line.\\nThe checkpoint location must be the same across restarts because this directory con‐\\ntains the unique identity of a streaming query and determines the life cycle of the\\nquery. If the checkpoint directory is deleted or the same query is started with a differ‐\\nent checkpoint directory, it is like starting a new query from scratch. Specifically,\\ncheckpoints have record-level information (e.g., Apache Kafka offsets) to track the\\ndata range the last incomplete micro-batch was processing. The restarted query will\\nuse this information to start processing records precisely after the last successfully\\ncompleted micro-batch. If the previous query had planned a micro-batch but had ter‐\\nminated before completion, then the restarted query will reprocess the same range of\\ndata before processing new data. Coupled with Spark’s deterministic task execution,\\nthe regenerated output will be the same as it was expected to be before the restart.\\nStructured Streaming can ensure end-to-end exactly-once guarantees  (that is, the out‐\\nput is as if each input record was processed exactly once) when the following condi‐\\ntions have been satisfied:\\nReplayable streaming sources\\nThe data range of the last incomplete micro-batch can be reread from the source.\\nDeterministic computations\\nAll data transformations deterministically produce the same result when given\\nthe same input data.\\nIdempotent streaming sink\\nThe sink can identify reexecuted micro-batches and ignore duplicate writes that\\nmay be caused by restarts.\\nThe Fundamentals of a Structured Streaming Query | 221', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 244}), Document(page_content='Note that our word count example does not provide exactly-once guarantees because\\nthe socket source is not replayable and the console sink is not idempotent.\\nAs a final note regarding restarting queries, it is possible to make minor modifica‐\\ntions to a query between restarts. Here are a few ways you can modify the query:\\nDataFrame transformations\\nY ou can make minor modifications to the transformations between restarts. For\\nexample, in our streaming word count example, if you want to ignore lines that\\nhave corrupted byte sequences that can crash the query, you can add a filter in\\nthe transformation:\\n# In Python\\n# isCorruptedUdf = udf to detect corruption in string\\nfilteredLines  = lines.filter(\"isCorruptedUdf(value) = false\" )\\nwords = filteredLines .select(split(col(\"value\"), \"\\\\\\\\s\").alias(\"word\"))\\n// In Scala\\n// val isCorruptedUdf = udf to detect corruption in string\\nval filteredLines  = lines.filter(\"isCorruptedUdf(value) = false\" )\\nval words = filteredLines .select(split(col(\"value\"), \"\\\\\\\\s\").as(\"word\"))\\nUpon restarting with this modified words  DataFrame, the restarted query will\\napply the filter on all data processed since the restart (including the last incom‐\\nplete micro-batch), preventing the query from failing again.\\nSource and sink options\\nWhether a readStream  or writeStream  option can be changed between restarts\\ndepends on the semantics of the specific source or sink. For example, you should\\nnot change the host  and port  options for the socket source if data is going to be\\nsent to that host and port. But you can add an option to the console sink to print\\nup to one hundred changed counts after every trigger:\\nwriteStream.format(\"console\").option(\"numRows\", \"100\")...\\nProcessing details\\nAs discussed earlier, the checkpoint location must not be changed between\\nrestarts. However, other details like trigger interval can be changed without\\nbreaking fault-tolerance guarantees.\\nFor more information on the narrow set of changes that are allowed between restarts,\\nsee the latest Structured Streaming Programming Guide .\\n222 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 245}), Document(page_content='Monitoring an Active Query\\nAn important part of running a streaming pipeline in production is tracking its\\nhealth. Structured Streaming provides several ways to track the status and processing\\nmetrics of an active query.\\nQuerying current status using StreamingQuery\\nY ou can query the current health of an active query using the StreamingQuery\\ninstance. Here are two methods:\\nGet current metrics using StreamingQuery.    When a query processes some data in a\\nmicro-batch, we consider it to have made some progress. lastProgress()  returns\\ninformation on the last completed micro-batch. For example, printing the returned\\nobject (StreamingQueryProgress  in Scala/Java or a dictionary in Python) will pro‐\\nduce something like this:\\n// In Scala/Python\\n{\\n  \"id\" : \"ce011fdc-8762-4dcb-84eb-a77333e28109\",\\n  \"runId\" : \"88e2ff94-ede0-45a8-b687-6316fbef529a\",\\n  \"name\" : \"MyQuery\",\\n  \"timestamp\" : \"2016-12-14T18:45:24.873Z\",\\n  \"numInputRows\" : 10,\\n  \"inputRowsPerSecond\" : 120.0,\\n  \"processedRowsPerSecond\" : 200.0,\\n  \"durationMs\" : {\\n    \"triggerExecution\" : 3,\\n    \"getOffset\" : 2\\n  },\\n  \"stateOperators\" : [ ],\\n  \"sources\" : [ {\\n    \"description\" : \"KafkaSource[Subscribe[topic-0]]\",\\n    \"startOffset\" : {\\n      \"topic-0\" : {\\n        \"2\" : 0,\\n        \"1\" : 1,\\n        \"0\" : 1\\n      }\\n    },\\n    \"endOffset\" : {\\n      \"topic-0\" : {\\n        \"2\" : 0,\\n        \"1\" : 134,\\n        \"0\" : 534\\n      }\\n    },\\n    \"numInputRows\" : 10,\\n    \"inputRowsPerSecond\" : 120.0,\\n    \"processedRowsPerSecond\" : 200.0\\nThe Fundamentals of a Structured Streaming Query | 223', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 246}), Document(page_content='  } ],\\n  \"sink\" : {\\n    \"description\" : \"MemorySink\"\\n  }\\n}\\nSome of the noteworthy columns are:\\nid\\nUnique identifier tied to a checkpoint location. This stays the same throughout\\nthe lifetime of a query (i.e., across restarts).\\nrunId\\nUnique identifier for the current (re)started instance of the query. This changes\\nwith every restart.\\nnumInputRows\\nNumber of input rows that were processed in the last micro-batch.\\ninputRowsPerSecond\\nCurrent rate at which input rows are being generated at the source (average over\\nthe last micro-batch duration).\\nprocessedRowsPerSecond\\nCurrent rate at which rows are being processed and written out by the sink (aver‐\\nage over the last micro-batch duration). If this rate is consistently lower than the\\ninput rate, then the query is unable to process data as fast as it is being generated\\nby the source. This is a key indicator of the health of the query.\\nsources  and sink\\nProvides source/sink-specific details of the data processed in the last batch.\\nGet current status using StreamingQuery.status().    This provides information on what the\\nbackground query thread is doing at this moment. For example, printing the returned\\nobject will produce something like this:\\n// In Scala/Python\\n{\\n  \"message\" : \"Waiting for data to arrive\",\\n  \"isDataAvailable\" : false,\\n  \"isTriggerActive\" : false\\n}\\nPublishing metrics using Dropwizard Metrics\\nSpark supports reporting metrics via a popular library called Dropwizard Metrics .\\nThis library allows metrics to be published to many popular monitoring frameworks\\n(Ganglia, Graphite, etc.). These metrics are by default not enabled for Structured\\nStreaming queries due to their high volume of reported data. To enable them, apart\\n224 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 247}), Document(page_content='from configuring Dropwizard Metrics for Spark , you have to explicitly set the\\nSparkSession  configuration spark.sql.streaming.metricsEnabled  to true  before\\nstarting your query.\\nNote that only a subset of the information available through\\nStreamingQuery.lastProgress()  is published through Dropwizard Metrics. If you\\nwant to continuously publish more progress information to arbitrary locations, you\\nhave to write custom listeners, as discussed next.\\nPublishing metrics using custom StreamingQueryListeners\\nStreamingQueryListener  is an event listener interface with which you can inject\\narbitrary logic to continuously publish metrics. This developer API is available only\\nin Scala/Java. There are two steps to using custom listeners:\\n1.Define your custom listener. The StreamingQueryListener  interface provides\\nthree methods that can be defined by your implementation to get three types of\\nevents related to a streaming query: start, progress (i.e., a trigger was executed),\\nand termination. Here is an example:\\n// In Scala\\nimport org.apache.spark.sql.streaming._\\nval myListener  = new StreamingQueryListener () {\\n  override  def onQueryStarted (event: QueryStartedEvent ): Unit = {\\n    println(\"Query started: \"  + event.id)\\n  }\\n  override  def onQueryTerminated (event: QueryTerminatedEvent ): Unit = {\\n    println(\"Query terminated: \"  + event.id)\\n  }\\n  override  def onQueryProgress (event: QueryProgressEvent ): Unit = {\\n    println(\"Query made progress: \"  + event.progress )\\n  }\\n}\\n2.Add your listener to the SparkSession  before starting the query:\\n// In Scala\\nspark.streams.addListener (myListener )\\nAfter adding the listener, all events of streaming queries running on this Spark\\nSession  will start calling the listener’s methods.\\nThe Fundamentals of a Structured Streaming Query | 225', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 248}), Document(page_content='Streaming Data Sources and Sinks\\nNow that we have covered the basic steps you need to express an end-to-end Struc‐\\ntured Streaming query, let’s examine how to use the built-in streaming data sources\\nand sinks. As a reminder, you can create DataFrames from streaming sources using\\nSparkSession.readStream()  and write the output from a result DataFrame using\\nDataFrame.writeStream() . In each case, you can specify the source type using the\\nmethod format() . We will see a few concrete examples later.\\nFiles\\nStructured Streaming supports reading and writing data streams to and from files in\\nthe same formats as the ones supported in batch processing: plain text, CSV , JSON,\\nParquet, ORC, etc. Here we will discuss how to operate Structured Streaming on files.\\nReading from files\\nStructured Streaming can treat files written into a directory as a data stream. Here is\\nan example:\\n# In Python\\nfrom pyspark.sql.types  import *\\ninputDirectoryOfJsonFiles  =  ... \\nfileSchema  = (StructType ()\\n  .add(StructField (\"key\", IntegerType ()))\\n  .add(StructField (\"value\", IntegerType ())))\\ninputDF = (spark\\n  .readStream\\n  .format(\"json\")\\n  .schema(fileSchema )\\n  .load(inputDirectoryOfJsonFiles ))\\n// In Scala\\nimport org.apache.spark.sql.types._\\nval inputDirectoryOfJsonFiles  =  ... \\nval fileSchema  = new StructType ()\\n  .add(\"key\", IntegerType )\\n  .add(\"value\", IntegerType )\\nval inputDF = spark.readStream\\n  .format(\"json\")\\n  .schema(fileSchema )\\n  .load(inputDirectoryOfJsonFiles )\\nThe returned streaming DataFrame will have the specified schema. Here are a few key\\npoints to remember when using files:\\n226 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 249}), Document(page_content='•All the files must be of the same format and are expected to have the same\\nschema. For example, if the format is \"json\" , all the files must be in the JSON\\nformat with one JSON record per line. The schema of each JSON record must\\nmatch the one specified with readStream() . Violation of these assumptions can\\nlead to incorrect parsing (e.g., unexpected null  values) or query failures.\\n•Each file must appear in the directory listing atomically—that is, the whole file\\nmust be available at once for reading, and once it is available, the file cannot be\\nupdated or modified. This is because Structured Streaming will process the file\\nwhen the engine finds it (using directory listing) and internally mark it as pro‐\\ncessed. Any changes to that file will not be processed.\\n•When there are multiple new files to process but it can only pick some of them in\\nthe next micro-batch (e.g., because of rate limits), it will select the files with the\\nearliest timestamps. Within the micro-batch, however, there is no predefined\\norder of reading of the selected files; all of them will be read in parallel.\\nThis streaming file source supports a number of common options,\\nincluding the file format–specific options supported by\\nspark.read()  (see “Data Sources for DataFrames and SQL Tables”\\non page 94 in Chapter 4 ) and several streaming-specific options\\n(e.g., maxFilesPerTrigger  to limit the file processing rate). See the\\nprogramming guide  for full details.\\nWriting to files\\nStructured Streaming supports writing streaming query output to files in the same\\nformats as reads. However, it only supports append mode, because while it is easy to\\nwrite new files in the output directory (i.e., append data to a directory), it is hard to\\nmodify existing data files (as would be expected with update and complete modes). It\\nalso supports partitioning. Here is an example:\\n# In Python\\noutputDir  = ...\\ncheckpointDir  = ...\\nresultDF  = ...\\nstreamingQuery  = (resultDF .writeStream\\n  .format(\"parquet\" )\\n  .option(\"path\", outputDir )\\n  .option(\"checkpointLocation\" , checkpointDir )\\n  .start())\\nStreaming Data Sources and Sinks | 227', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 250}), Document(page_content='// In Scala\\nval outputDir  = ...\\nval checkpointDir  = ...\\nval resultDF  = ...\\n \\nval streamingQuery  = resultDF\\n  .writeStream\\n  .format(\"parquet\" )\\n  .option(\"path\", outputDir )\\n  .option(\"checkpointLocation\" , checkpointDir )\\n  .start()\\nInstead of using the \"path\"  option, you can specify the output directory directly as\\nstart(outputDir) .\\nA few key points to remember:\\n•Structured Streaming achieves end-to-end exactly-once guarantees when writing\\nto files by maintaining a log of the data files that have been written to the direc‐\\ntory. This log is maintained in the subdirectory _spark_metadata . Any Spark\\nquery on the directory (not its subdirectories) will automatically use the log to\\nread the correct set of data files so that the exactly-once guarantee is maintained\\n(i.e., no duplicate data or partial files are read). Note that other processing\\nengines may not be aware of this log and hence may not provide the same\\nguarantee.\\n•If you change the schema of the result DataFrame between restarts, then the out‐\\nput directory will have data in multiple schemas. These schemas have to be rec‐\\nonciled when querying the directory.\\nApache Kafka\\nApache Kafka  is a popular publish/subscribe system that is widely used for storage of\\ndata streams. Structured Streaming has built-in support for reading from and writing\\nto Apache Kafka.\\nReading from Kafka\\nTo perform distributed reads from Kafka, you have to use options to specify how to\\nconnect to the source. Say you want to subscribe to data from the topic \"events\" .\\nHere is how you can create a streaming DataFrame:\\n# In Python\\ninputDF = (spark\\n  .readStream\\n  .format(\"kafka\")\\n  .option(\"kafka.bootstrap.servers\" , \"host1:port1,host2:port2\" )\\n  .option(\"subscribe\" , \"events\" )\\n  .load())\\n228 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 251}), Document(page_content='// In Scala\\nval inputDF = spark\\n  .readStream\\n  .format(\"kafka\")\\n  .option(\"kafka.bootstrap.servers\" , \"host1:port1,host2:port2\" )\\n  .option(\"subscribe\" , \"events\" )\\n  .load()\\nThe returned DataFrame will have the schema described in Table 8-1 .\\nTable 8-1. Schema of the DataFrame generated by the Kafka  source\\nColumn name Column type Description\\nkey binary Key data of the record as bytes.\\nvalue binary Value data of the record as bytes.\\ntopic string Kafka topic the record was in. This is useful when subscribed to multiple topics.\\npartition int Partition of the Kafka topic the record was in.\\noffset long Offset  value of the record.\\ntimestamp long Timestamp associated with the record.\\ntimestampType int Enumeration for the type of the timestamp associated with the record.\\nY ou can also choose to subscribe to multiple topics, a pattern of topics, or even a spe‐\\ncific partition of a topic. Furthermore, you can choose whether to read only new data\\nin the subscribed-to topics or process all the available data in those topics. Y ou can\\neven read Kafka data from batch queries—that is, treat Kafka topics like tables. See\\nthe Kafka Integration Guide  for more details.\\nWriting to Kafka\\nFor writing to Kafka, Structured Streaming expects the result DataFrame to have a\\nfew columns of specific names and types, as outlined in Table 8-2 .\\nTable 8-2. Schema of DataFrame that can be written to the Kafka  sink\\nColumn name Column type Description\\nkey  (optional) string  or\\nbinaryIf present, the bytes will be written as the Kafka record key; otherwise, the key\\nwill be empty.\\nvalue  (required) string  or\\nbinaryThe bytes will be written as the Kafka record value.\\ntopic  (required only if\\n\"topic\"  is not\\nspecified  as option)string If \"topic\"  is not specified  as an option, this determines the topic to write the\\nkey/value to. This is useful for fanning out the writes to multiple topics. If the\\n\"topic\"  option has been specified,  this value is ignored.\\nStreaming Data Sources and Sinks | 229', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 252}), Document(page_content='Y ou can write to Kafka in all three output modes, though complete mode is not rec‐\\nommended as it will repeatedly output the same records. Here is a concrete example\\nof writing the output of our earlier word count query into Kafka in update mode:\\n# In Python\\ncounts = ... # DataFrame[word: string, count: long]\\nstreamingQuery  = (counts\\n  .selectExpr (\\n    \"cast(word as string) as key\" , \\n    \"cast(count as string) as value\" )\\n  .writeStream\\n  .format(\"kafka\")\\n  .option(\"kafka.bootstrap.servers\" , \"host1:port1,host2:port2\" )\\n  .option(\"topic\", \"wordCounts\" )\\n  .outputMode (\"update\" )\\n  .option(\"checkpointLocation\" , checkpointDir )\\n  .start())\\n// In Scala\\nval counts = ... // DataFrame[word: string, count: long]\\nval streamingQuery  = counts\\n  .selectExpr (\\n    \"cast(word as string) as key\" , \\n    \"cast(count as string) as value\" )\\n  .writeStream\\n  .format(\"kafka\")\\n  .option(\"kafka.bootstrap.servers\" , \"host1:port1,host2:port2\" )\\n  .option(\"topic\", \"wordCounts\" )\\n  .outputMode (\"update\" )\\n  .option(\"checkpointLocation\" , checkpointDir )\\n  .start()\\nSee the Kafka Integration Guide  for more details.\\nCustom Streaming Sources and Sinks\\nIn this section, we will discuss how to read and write to storage systems that do not\\nhave built-in support in Structured Streaming. In particular, you’ll see how to use the\\nforeachBatch()  and foreach()  methods to implement custom logic to write to your\\nstorage.\\nWriting to any storage system\\nThere are two operations that allow you to write the output of a streaming query to\\narbitrary storage systems: foreachBatch()  and foreach() . They have slightly differ‐\\nent use cases: while foreach()  allows custom write logic on every row, foreach\\nBatch()  allows arbitrary operations and custom logic on the output of each micro-\\nbatch. Let’s explore their usage in more detail.\\n230 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 253}), Document(page_content='Using foreachBatch().    foreachBatch()  allows you to specify a function that is exe‐\\ncuted on the output of every micro-batch of a streaming query. It takes two parame‐\\nters: a DataFrame or Dataset that has the output of a micro-batch, and the unique\\nidentifier of the micro-batch. As an example, say we want to write the output of our\\nearlier word count query to Apache Cassandra . As of Spark Cassandra Connector\\n2.4.2 , there is no support for writing streaming DataFames. But you can use the con‐\\nnector’s batch DataFrame support to write the output of each batch (i.e., updated\\nword counts) to Cassandra, as shown here:\\n# In Python\\nhostAddr  = \"<ip address>\"\\nkeyspaceName  = \"<keyspace>\"\\ntableName  = \"<tableName>\"\\nspark.conf.set(\"spark.cassandra.connection.host\" , hostAddr )\\ndef writeCountsToCassandra (updatedCountsDF , batchId):\\n    # Use Cassandra batch data source to write the updated counts\\n    (updatedCountsDF\\n      .write\\n      .format(\"org.apache.spark.sql.cassandra\" )\\n      .mode(\"append\" )\\n      .options(table=tableName , keyspace =keyspaceName )\\n      .save())\\n      \\nstreamingQuery  = (counts\\n  .writeStream\\n  .foreachBatch (writeCountsToCassandra )\\n  .outputMode (\"update\" )\\n  .option(\"checkpointLocation\" , checkpointDir )\\n  .start())\\n// In Scala\\nimport org.apache.spark.sql.DataFrame\\nval hostAddr  = \"<ip address>\"\\nval keyspaceName  = \"<keyspace>\"\\nval tableName  = \"<tableName>\"\\nspark.conf.set(\"spark.cassandra.connection.host\" , hostAddr )\\ndef writeCountsToCassandra (updatedCountsDF : DataFrame , batchId: Long) {\\n    // Use Cassandra batch data source to write the updated counts\\n    updatedCountsDF\\n      .write\\n      .format(\"org.apache.spark.sql.cassandra\" )\\n      .options(Map(\"table\" -> tableName , \"keyspace\"  -> keyspaceName ))\\n      .mode(\"append\" )\\n      .save()\\n    }\\nStreaming Data Sources and Sinks | 231', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 254}), Document(page_content='3For the full list of unsupported operations, see the Structured Streaming Programming Guide .val streamingQuery  = counts\\n  .writeStream\\n  .foreachBatch (writeCountsToCassandra  _)\\n  .outputMode (\"update\" )\\n  .option(\"checkpointLocation\" , checkpointDir )\\n  .start()\\nWith foreachBatch() , you can do the following:\\nReuse existing batch data sources\\nAs shown in the previous example, with foreachBatch()  you can use existing\\nbatch data sources (i.e., sources that support writing batch DataFrames) to write\\nthe output of streaming queries.\\nWrite to multiple locations\\nIf you want to write the output of a streaming query to multiple locations (e.g.,\\nan OLAP data warehouse and an OLTP database), then you can simply write the\\noutput DataFrame/Dataset multiple times. However, each attempt to write can\\ncause the output data to be recomputed (including possible rereading of the\\ninput data). To avoid recomputations, you should cache the batchOutputData\\nFrame , write it to multiple locations, and then uncache it:\\n# In Python\\ndef writeCountsToMultipleLocations (updatedCountsDF , batchId):\\n  updatedCountsDF .persist()\\n  updatedCountsDF .write.format(...).save()  # Location 1\\n  updatedCountsDF .write.format(...).save()  # Location 2\\n  updatedCountsDF .unpersist ()\\n// In Scala\\ndef writeCountsToMultipleLocations (\\n  updatedCountsDF : DataFrame , \\n  batchId: Long) {\\n    updatedCountsDF .persist()\\n    updatedCountsDF .write.format(...).save()  // Location 1\\n    updatedCountsDF .write.format(...).save()  // Location 2\\n    updatedCountsDF .unpersist ()\\n }\\nApply additional DataFrame operations\\nMany DataFrame API operations are not supported3 on streaming DataFrames\\nbecause Structured Streaming does not support generating incremental plans in\\nthose cases. Using foreachBatch() , you can apply some of these operations on\\neach micro-batch output. However, you will have to reason about the end-to-end\\nsemantics of doing the operation yourself.\\n232 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 255}), Document(page_content='foreachBatch()  only provides at-least-once write guarantees. Y ou\\ncan get exactly-once guarantees by using the batchId  to dedupli‐\\ncate multiple writes from reexecuted micro-batches.\\nUsing foreach().    If foreachBatch()  is not an option (for example, if a corresponding\\nbatch data writer does not exist), then you can express your custom writer logic using\\nforeach() . Specifically, you can express the data-writing logic by dividing it into\\nthree methods: open() , process() , and close() . Structured Streaming will use these\\nmethods to write each partition of the output records. Here is an abstract example:\\n# In Python\\n# Variation 1: Using function\\ndef process_row (row):\\n    # Write row to storage\\n    pass\\nquery = streamingDF .writeStream .foreach(process_row ).start()  \\n# Variation 2: Using the ForeachWriter class\\nclass ForeachWriter :\\n  def open(self, partitionId , epochId):\\n    # Open connection to data store\\n    # Return True if write should continue\\n    # This method is optional in Python \\n    # If not specified, the write will continue automatically\\n    return True\\n  def process(self, row):\\n    # Write string to data store using opened connection\\n    # This method is NOT optional in Python\\n    pass\\n  def close(self, error):\\n    # Close the connection. This method is optional in Python\\n    pass\\nresultDF .writeStream .foreach(ForeachWriter ()).start()\\n// In Scala\\nimport org.apache.spark.sql.ForeachWriter\\nval foreachWriter  = new ForeachWriter [String] {  // typed with Strings\\n    def open(partitionId : Long, epochId: Long): Boolean = {\\n      // Open connection to data store\\n      // Return true if write should continue\\n    }\\n    def process(record: String): Unit = {\\n      // Write string to data store using opened connection\\nStreaming Data Sources and Sinks | 233', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 256}), Document(page_content='    }\\n    def close(errorOrNull : Throwable ): Unit = {\\n      // Close the connection\\n    }\\n }\\nresultDSofStrings .writeStream .foreach(foreachWriter ).start()\\nThe detailed semantics of these methods as executed are discussed in the Structured\\nStreaming Programming Guide .\\nReading from any storage system\\nUnfortunately, as of Spark 3.0, the APIs to build custom streaming sources and sinks\\nare still experimental. The DataSourceV2 initiative in Spark 3.0 introduces the\\nstreaming APIs but they are yet to be declared as stable. Hence, there is no official\\nway to read from arbitrary storage systems.\\nData Transformations\\nIn this section, we are going to dig deeper into the data transformations supported in\\nStructured Streaming. As briefly discussed earlier, only the DataFrame operations\\nthat can be executed incrementally are supported in Structured Streaming. These\\noperations are broadly classified into stateless  and stateful  operations. We will define\\neach type of operation and explain how to identify which operations are stateful.\\nIncremental Execution and Streaming State\\nAs we discussed in “Under the Hood of an Active Streaming Query”  on page 219, the\\nCatalyst optimizer in Spark SQL converts all the DataFrame operations to an opti‐\\nmized logical plan. The Spark SQL planner, which decides how to execute a logical\\nplan, recognizes that this is a streaming logical plan that needs to operate on continu‐\\nous data streams. Accordingly, instead of converting the logical plan to a one-time\\nphysical execution plan, the planner generates a continuous sequence of execution\\nplans. Each execution plan updates the final result DataFrame incrementally—that is,\\nthe plan processes only a chunk of new data from the input streams and possibly\\nsome intermediate, partial result computed by the previous execution plan.\\nEach execution is considered as a micro-batch, and the partial intermediate result\\nthat is communicated between the executions is called the streaming “state. ” Data‐\\nFrame operations can be broadly classified into stateless and stateful operations based\\non whether executing the operation incrementally requires maintaining a state. In the\\nrest of this section, we are going to explore the distinction between stateless and state‐\\nful operations and how their presence in a streaming query requires different runtime\\nconfiguration and resource management.\\n234 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 257}), Document(page_content='Some logical operations are fundamentally either impractical or\\nvery expensive to compute incrementally, and hence they are not\\nsupported in Structured Streaming. For example, any attempt to\\nstart a streaming query with an operation like cube()  or rollup()\\nwill throw an UnsupportedOperationException .\\nStateless Transformations\\nAll projection operations (e.g., select() , explode() , map() , flatMap() ) and selec‐\\ntion operations (e.g., filter() , where() ) process each input record individually\\nwithout needing any information from previous rows. This lack of dependence on\\nprior input data makes them stateless operations.\\nA streaming query having only stateless operations supports the append and update\\noutput modes, but not complete mode. This makes sense: since any processed output\\nrow of such a query cannot be modified by any future data, it can be written out to all\\nstreaming sinks in append mode (including append-only ones, like files of any for‐\\nmat). On the other hand, such queries naturally do not combine information across\\ninput records, and therefore may not reduce the volume of the data in the result.\\nComplete mode is not supported because storing the ever-growing result data is usu‐\\nally costly. This is in sharp contrast with stateful transformations, as we will discuss\\nnext.\\nStateful Transformations\\nThe simplest example of a stateful transformation is DataFrame.groupBy().count() ,\\nwhich generates a running count of the number of records received since the begin‐\\nning of the query. In every micro-batch, the incremental plan adds the count of new\\nrecords to the previous count generated by the previous micro-batch. This partial\\ncount communicated between plans is the state. This state is maintained in the mem‐\\nory of the Spark executors and is checkpointed to the configured location in order to\\ntolerate failures. While Spark SQL automatically manages the life cycle of this state to\\nensure correct results, you typically have to tweak a few knobs to control the resource\\nusage for maintaining state. In this section, we are going to explore how different\\nstateful operators manage their state under the hood.\\nData Transformations | 235', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 258}), Document(page_content='Distributed and fault-tolerant state management\\nRecall from Chapters 1 and 2 that a Spark application running in a cluster has a\\ndriver and one or more executors. Spark’s scheduler running in the driver breaks\\ndown your high-level operations into smaller tasks and puts them in task queues, and\\nas resources become available, the executors pull the tasks from the queues to execute\\nthem. Each micro-batch in a streaming query essentially performs one such set of\\ntasks that read new data from streaming sources and write updated output to\\nstreaming  sinks. For stateful stream processing queries, besides writing to sinks, each\\nmicro-batch of tasks generates intermediate state data which will be consumed by the\\nnext micro-batch. This state data generation is completely partitioned and distributed\\n(as all reading, writing, and processing is in Spark), and it is cached in the executor\\nmemory for efficient consumption. This is illustrated in Figure 8-6 , which shows how\\nthe state is managed in our original streaming word count query.\\nFigure 8-6. Distributed state management in Structured Streaming\\nEach micro-batch reads a new set of words, shuffles them within the executors to\\ngroup them, computes the counts within the micro-batch, and finally adds them to\\nthe running counts to produce the new counts. These new counts are both the output\\nand the state for the next micro-batch, and hence they are cached in the memory of\\nthe executors. The next micro-batch of data is grouped between executors in exactly\\nthe same way as before, so that each word is always processed by the same executor,\\nand can therefore locally read and update its running count.\\n236 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 259}), Document(page_content='However, it is not sufficient to just keep this state in memory, as any failure (either of\\nan executor or of the entire application) will cause the in-memory state to be lost. To\\navoid loss, we synchronously save the key/value state update as change logs in the\\ncheckpoint location provided by the user. These changes are co-versioned with the\\noffset ranges processed in each batch, and the required version of the state can be\\nautomatically reconstructed by reading the checkpointed logs. In case of any failure,\\nStructured Streaming is able to re-execute the failed micro-batch by reprocessing the\\nsame input data along with the same state that it had before that micro-batch, thus\\nproducing the same output data as it would have if there had been no failure. This is\\ncritical for ensuring end-to-end exactly-once guarantees.\\nTo summarize, for all stateful operations, Structured Streaming ensures the correct‐\\nness of the operation by automatically saving and restoring the state in a distributed\\nmanner. Depending on the stateful operation, all you may have to do is tune the state\\ncleanup policy such that old keys and values can be automatically dropped from the\\ncached state. This is what we will discuss next.\\nTypes of stateful operations\\nThe essence of streaming state is to retain summaries of past data. Sometimes old\\nsummaries need to be cleaned up from the state to make room for new summaries.\\nBased on how this is done, we can distinguish two types of stateful operations:\\nManaged stateful operations\\nThese automatically identify and clean up old state, based on an operation-\\nspecific definition of “old. ” Y ou can tune what is defined as old in order to control\\nthe resource usage (e.g., executor memory used to store state). The operations\\nthat fall into this category are those for:\\n•Streaming aggregations\\n•Stream–stream joins\\n•Streaming deduplication\\nUnmanaged stateful operations\\nThese operations let you define your own custom state cleanup logic. The opera‐\\ntions in this category are:\\n•MapGroupsWithState\\n•FlatMapGroupsWithState\\nThese operations allow you to define arbitrary stateful operations (sessionization,\\netc.).\\nEach of these operations are discussed in detail in the following sections.\\nData Transformations | 237', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 260}), Document(page_content='Stateful Streaming Aggregations\\nStructured Streaming can incrementally execute most DataFrame aggregation opera‐\\ntions. Y ou can aggregate data by keys (e.g., streaming word count) and/or by time\\n(e.g., count records received every hour). In this section, we are going to discuss the\\nsemantics and operational details of tuning these different types of streaming aggre‐\\ngations. We’ll also briefly discuss the few types of aggregations that are not supported\\nin streaming. Let’s begin with aggregations not involving time.\\nAggregations Not Based on Time\\nAggregations not involving time can be broadly classified into two categories:\\nGlobal aggregations\\nAggregations across all the data in the stream. For example, say you have a\\nstream of sensor readings as a streaming DataFrame named sensorReadings .\\nY ou can calculate the running count of the total number of readings received\\nwith the following query:\\n# In Python\\nrunningCount  = sensorReadings .groupBy().count()\\n// In Scala\\nval runningCount  = sensorReadings .groupBy().count()\\nY ou cannot use direct aggregation operations like Data\\nFrame.count()  and Dataset.reduce()  on streaming Data‐\\nFrames. This is because, for static DataFrames, these\\noperations immediately return the final computed aggregates,\\nwhereas for streaming DataFrames the aggregates have to be\\ncontinuously updated. Therefore, you have to always use Data\\nFrame.groupBy()  or Dataset.groupByKey()  for aggregations\\non streaming DataFrames.\\nGrouped aggregations\\nAggregations within each group or key present in the data stream. For example, if\\nsensorReadings  contains data from multiple sensors, you can calculate the run‐\\nning average reading of each sensor (say, for setting up a baseline value for each\\nsensor) with the following:\\n# In Python \\nbaselineValues  = sensorReadings .groupBy(\"sensorId\" ).mean(\"value\")\\n// In Scala\\nval baselineValues  = sensorReadings .groupBy(\"sensorId\" ).mean(\"value\")\\n238 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 261}), Document(page_content='Besides counts and averages, streaming DataFrames support the following types of\\naggregations (similar to batch DataFrames):\\nAll built-in aggregation functions\\nsum() , mean() , stddev() , countDistinct() , collect_set() , approx_count_dis\\ntinct() , etc. Refer to the API documentation ( Python  and Scala ) for more\\ndetails.\\nMultiple aggregations computed together\\nY ou can apply multiple aggregation functions to be computed together in the fol‐\\nlowing manner:\\n# In Python\\nfrom pyspark.sql.functions  import *\\nmultipleAggs  = (sensorReadings\\n  .groupBy(\"sensorId\" )\\n  .agg(count(\"*\"), mean(\"value\").alias(\"baselineValue\" ), \\n    collect_set (\"errorCode\" ).alias(\"allErrorCodes\" )))\\n// In Scala\\nimport org.apache.spark.sql.functions. *\\nval multipleAggs  = sensorReadings\\n  .groupBy(\"sensorId\" )\\n  .agg(count(\"*\"), mean(\"value\").alias(\"baselineValue\" ),\\n    collect_set (\"errorCode\" ).alias(\"allErrorCodes\" ))\\nUser-defined  aggregation functions\\nAll user-defined aggregation functions are supported. See the Spark SQL pro‐\\ngramming guide  for more details on untyped and typed user-defined aggregation\\nfunctions.\\nRegarding the execution of such streaming aggregations, we have already illustrated\\nin previous sections how the running aggregates are maintained as a distributed state.\\nIn addition to this, there are two very important points to remember for aggregations\\nnot based on time: the output mode to use for such queries and planning the resource\\nusage by state. These are discussed toward the end of this section. Next, we are going\\nto discuss aggregations that combine data within time windows.\\nAggregations with Event-Time Windows\\nIn many cases, rather than running aggregations over the whole stream, you want\\naggregations over data bucketed by time windows. Continuing with our sensor exam‐\\nple, say each sensor is expected to send at most one reading per minute and we want\\nto detect if any sensor is reporting an unusually high number of times. To find such\\nanomalies, we can count the number of readings received from each sensor in five-\\nminute intervals. In addition, for robustness, we should be computing the time inter‐\\nval based on when the data was generated at the sensor and not based on when the\\nStateful Streaming Aggregations | 239', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 262}), Document(page_content='data was received, as any transit delay would skew the results. In other words, we\\nwant to use the event time —that is, the timestamp in the record representing when\\nthe reading was generated. Say the sensorReadings  DataFrame has the generation\\ntimestamp as a column named eventTime . We can express this five-minute count as\\nfollows:\\n# In Python\\nfrom pyspark.sql.functions  import *\\n(sensorReadings\\n  .groupBy(\"sensorId\" , window(\"eventTime\" , \"5 minute\" ))\\n  .count())\\n// In Scala\\nimport org.apache.spark.sql.functions. *\\nsensorReadings\\n  .groupBy(\"sensorId\" , window(\"eventTime\" , \"5 minute\" ))\\n  .count()\\nThe key thing to note here is the window()  function, which allows us to express the\\nfive-minute windows as a dynamically computed grouping column. When started,\\nthis query will effectively do the following for each sensor reading:\\n•Use the eventTime  value to compute the five-minute time window the sensor\\nreading falls into.\\n•Group the reading based on the composite group (<computed window> ,\\nSensorId) .\\n•Update the count of the composite group.\\nLet’s understand this with an illustrative example. Figure 8-7  shows how a few sensor\\nreadings are mapped to groups of five-minute tumbling (i.e., nonoverlapping) win‐\\ndows based on their event time. The two timelines show when each received event\\nwill be processed by Structured Streaming, and the timestamp in the event data (usu‐\\nally, the time when the event was generated at the sensor).\\n240 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 263}), Document(page_content='Figure 8-7. Mapping of event time to tumbling windows\\nEach five-minute window over event time is considered for the grouping based on\\nwhich the counts will be calculated. Note that events may come late and out of order\\nin terms of event time. As shown in the figure, the event with event time 12:07 was\\nreceived and processed after the event with time 12:11. However, irrespective of when\\nthey arrive, each event is assigned to the appropriate group based on its event time. In\\nfact, depending on the window specification, each event can be assigned to multiple\\ngroups. For example, if you want to compute counts corresponding to 10-minute\\nwindows sliding every 5 minutes, then you can do the following:\\n# In Python\\n(sensorReadings\\n  .groupBy(\"sensorId\" , window(\"eventTime\" , \"10 minute\" , \"5 minute\" ))\\n  .count())\\n// In Scala\\nsensorReadings\\n  .groupBy(\"sensorId\" , window(\"eventTime\" , \"10 minute\" , \"5 minute\" ))\\n  .count()\\nIn this query, every event will be assigned to two overlapping windows as illustrated\\nin Figure 8-8 .\\nStateful Streaming Aggregations | 241', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 264}), Document(page_content='Figure 8-8. Mapping of event time to multiple overlapping windows\\nEach unique tuple of (<assigned time window> , sensorId)  is considered a dynam‐\\nically generated group for which counts will be computed. For example, the event\\n[eventTime = 12:07, sensorId = id1]  gets mapped to two time windows and\\ntherefore two groups, (12:00-12:10, id1)  and (12:05-12:15, id1) . The counts for\\nthese two windows are each incremented by 1. Figure 8-9  illustrates this for the previ‐\\nously shown events.\\nAssuming that the input records were processed with a trigger interval of five\\nminutes, the tables at the bottom of Figure 8-9  show the state of the result table (i.e.,\\nthe counts) at each of the micro-batches. As the event time moves forward, new\\ngroups are automatically created and their aggregates are automatically updated. Late\\nand out-of-order events get handled automatically, as they simply update older\\ngroups.\\n242 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 265}), Document(page_content='Figure 8-9. Updated counts in the result table after  each five-minute  trigger\\nHowever, from the point of view of resource usage, this poses a different problem—\\nindefinitely growing state size. As new groups are created corresponding to the latest\\ntime windows, the older groups continue to occupy the state memory, waiting for any\\nlate data to update them. Even if in practice there is a bound on how late the input\\ndata can be (e.g., data cannot be more than seven days late), the query does not know\\nthat information. Hence, it does not know when to consider a window as “too old to\\nreceive updates” and drop it from the state. To provide a lateness bound to a query\\n(and prevent unbounded state), you can specify watermarks , as we discuss next.\\nHandling late data with watermarks\\nA watermark  is defined as a moving threshold in event time that trails behind the\\nmaximum event time seen by the query in the processed data. The trailing gap,\\nknown as the watermark delay , defines how long the engine will wait for late data to\\narrive. By knowing the point at which no more data will arrive for a given group, the\\nengine can automatically finalize the aggregates of certain groups and drop them\\nfrom the state. This limits the total amount of state that the engine has to maintain to\\ncompute the results of the query.\\nFor example, suppose you know that your sensor data will not be late by more than\\n10 minutes. Then you can set the watermark as follows:\\n# In Python\\n(sensorReadings\\n  .withWatermark (\"eventTime\" , \"10 minutes\" )\\n  .groupBy(\"sensorId\" , window(\"eventTime\" , \"10 minutes\" , \"5 minutes\" ))\\n  .mean(\"value\"))\\nStateful Streaming Aggregations | 243', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 266}), Document(page_content='// In Scala\\nsensorReadings\\n  .withWatermark (\"eventTime\" , \"10 minutes\" )\\n  .groupBy(\"sensorId\" , window(\"eventTime\" , \"10 minutes\" , \"5 minute\" ))\\n  .mean(\"value\")\\nNote that you must call withWatermark()  before the groupBy()  and on the same\\ntimestamp column as that used to define windows. When this query is executed,\\nStructured Streaming will continuously track the maximum observed value of the\\neventTime  column and accordingly update the watermark, filter the “too late” data,\\nand clear old state. That is, any data late by more than 10 minutes will be ignored, and\\nall time windows that are more than 10 minutes older than the latest (by event time)\\ninput data will be cleaned up from the state. To clarify how this query will be exe‐\\ncuted, consider the timeline in Figure 8-10  showing how a selection of input records\\nwere processed.\\nFigure 8-10. Illustration of how the engine tracks the maximum event time across events,\\nupdates the watermark, and accordingly handles late data\\nThis figure shows a two-dimensional plot of records processed in terms of their pro‐\\ncessing times (x-axis) and their event times (y-axis). The records are processed in\\nmicro-batches of five minutes and marked with circles. The tables at the bottom show\\nthe state of the result table after each micro-batch completes.\\nEach record was received and processed after all the records to its left. Consider the\\ntwo records [12:15, id1]  (processed around 12:17) and [12:13, id3]  (processed\\naround 12:18). The record for id3 was considered late (and therefore marked in solid\\n244 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 267}), Document(page_content='red) because it was generated by the sensor before the record for id1 but it was pro‐\\ncessed after the latter. However, in the micro-batch for processing-time range\\n12:15–12:20,  the watermark used was 12:04 which was calculated based on the maxi‐\\nmum event time seen till the previous micro-batch (that is, 12:14 minus the 10-\\nminute watermark delay). Therefore, the late record [12:13, id3]  was not\\nconsidered to be too late and was successfully counted. In contrast, in the next micro-\\nbatch, the record [12:04, id1]  was considered to be too late compared to the new\\nwatermark of 12:11 and was discarded.\\nY ou can set the watermark delay based on the requirements of your application—\\nlarger values for this parameter allow data to arrive later, but at the cost of increased\\nstate size (i.e., memory usage), and vice versa.\\nSemantic guarantees with watermarks.    Before we conclude this section about water‐\\nmarks, let’s consider the precise semantic guarantee that watermarking provides. A\\nwatermark of 10 minutes guarantees that the engine will never drop any data  that is\\ndelayed by less than 10 minutes compared to the latest event time seen in the input\\ndata. However, the guarantee is strict only in one direction. Data delayed by more\\nthan 10 minutes is not guaranteed to be dropped—that is, it may get aggregated.\\nWhether an input record more than 10 minutes late will actually be aggregated or not\\ndepends on the exact timing of when the record was received and when the micro-\\nbatch processing it was triggered.\\nSupported output modes\\nUnlike streaming aggregations not involving time, aggregations with time windows\\ncan use all three output modes. However, there are other implications regarding state\\ncleanup that you need to be aware of, depending on the mode:\\nUpdate mode\\nIn this mode, every micro-batch will output only the rows where the aggregate\\ngot updated. This mode can be used with all types of aggregations. Specifically\\nfor time window aggregations, watermarking will ensure that the state will get\\ncleaned up regularly. This is the most useful and efficient mode to run queries\\nwith streaming aggregations. However, you cannot use this mode to write aggre‐\\ngates to append-only streaming sinks, such as any file-based formats like Parquet\\nand ORC (unless you use Delta Lake, which we will discuss in the next chapter).\\nComplete mode\\nIn this mode, every micro-batch will output all the updated aggregates, irrespec‐\\ntive of their age or whether they contain changes. While this mode can be used\\non all types of aggregations, for time window aggregations, using complete mode\\nmeans state will not be cleaned up even if a watermark is specified. Outputting all\\naggregates requires all past state, and hence aggregation data must be preserved\\nStateful Streaming Aggregations | 245', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 268}), Document(page_content='even if a watermark has been defined. Use this mode on time window aggrega‐\\ntions with caution, as this can lead to an indefinite increase in state size and\\nmemory usage.\\nAppend mode\\nThis mode can be used only with aggregations on event-time windows and with\\nwatermarking enabled . Recall that append mode does not allow previously output\\nresults to change. For any aggregation without watermarks, every aggregate may\\nbe updated with any future data, and hence these cannot be output in append\\nmode. Only when watermarking is enabled on aggregations on event-time win‐\\ndows does the query know when an aggregate is not going to update any further.\\nHence, instead of outputting the updated rows, append mode outputs each key\\nand its final aggregate value only when the watermark ensures that the aggregate\\nis not going to be updated again. The advantage of this mode is that it allows you\\nto write aggregates to append-only streaming sinks (e.g., files). The disadvantage\\nis that the output will be delayed by the watermark duration—the query has to\\nwait for the trailing watermark to exceed the time window of a key before its\\naggregate can be finalized.\\nStreaming Joins\\nStructured Streaming supports joining a streaming Dataset with another static or\\nstreaming Dataset. In this section we will explore what types of joins (inner, outer,\\netc.) are supported, and how to use watermarks to limit the state stored for stateful\\njoins. We will start with the simple case of joining a data stream and a static Dataset.\\nStream–Static Joins\\nMany use cases require joining a data stream with a static Dataset. For example, let’s\\nconsider the case of ad monetization. Suppose you are an advertisement company\\nthat shows ads on websites and you make money when users click on them. Let’s\\nassume that you have a static Dataset of all the ads to be shown (known as impres‐\\nsions), and another stream of events for each time users click on the displayed ads. To\\ncalculate the click revenue, you have to match each click in the event stream to the\\ncorresponding ad impression in the table. Let’s first represent the data as two Data‐\\nFrames, a static one and a streaming one, as shown here:\\n# In Python\\n# Static DataFrame [adId: String, impressionTime: Timestamp, ...]\\n# reading from your static data source \\nimpressionsStatic  = spark.read. ... \\n# Streaming DataFrame [adId: String, clickTime: Timestamp, ...] \\n# reading from your streaming source\\nclicksStream  = spark.readStream . ...\\n246 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 269}), Document(page_content='// In Scala\\n// Static DataFrame [adId: String, impressionTime: Timestamp, ...]\\n// reading from your static data source \\nval impressionsStatic  = spark.read. ...\\n// Streaming DataFrame [adId: String, clickTime: Timestamp, ...]\\n// reading from your streaming source \\nval clicksStream  = spark.readStream . ...\\nTo match the clicks with the impressions, you can simply apply an inner equi-join\\nbetween them using the common adId  column:\\n# In Python\\nmatched = clicksStream .join(impressionsStatic , \"adId\")\\n// In Scala\\nval matched = clicksStream .join(impressionsStatic , \"adId\")\\nThis is the same code as you would have written if both impressions and clicks were\\nstatic DataFrames—the only difference is that you use spark.read()  for batch pro‐\\ncessing and spark.readStream()  for a stream. When this code is executed, every\\nmicro-batch of clicks is inner-joined against the static impression table to generate\\nthe output stream of matched events.\\nBesides inner joins, Structured Streaming also supports two types of stream–static\\nouter joins:\\n•Left outer join when the left side is a streaming DataFrame\\n•Right outer join when the right side is a streaming DataFrame\\nThe other kinds of outer joins (e.g., full outer and left outer with a streaming Data‐\\nFrame on the right) are not supported because they are not easy to run incrementally.\\nIn both supported cases, the code is exactly as it would be for a left/right outer join\\nbetween two static DataFrames:\\n# In Python\\nmatched = clicksStream .join(impressionsStatic , \"adId\", \"leftOuter\" )\\n// In Scala\\nval matched = clicksStream .join(impressionsStatic , Seq(\"adId\"), \"leftOuter\" )\\nThere are a few key points to note about stream–static joins:\\n•Stream–static joins are stateless operations, and therefore do not require any kind\\nof watermarking.\\n•The static DataFrame is read repeatedly while joining with the streaming data of\\nevery micro-batch, so you can cache the static DataFrame to speed up the reads.\\n•If the underlying data in the data source on which the static DataFrame was\\ndefined changes, whether those changes are seen by the streaming query depends\\nStreaming Joins | 247', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 270}), Document(page_content='on the specific behavior of the data source. For example, if the static DataFrame\\nwas defined on files, then changes to those files (e.g., appends) will not be picked\\nup until the streaming query is restarted.\\nIn this stream–static example, we made a significant assumption: that the impression\\ntable is a static table. In reality, there will be a stream of new impressions generated as\\nnew ads are displayed. While stream–static joins are good for enriching data in one\\nstream with additional static (or slowly changing) information, this approach is\\ninsufficient when both sources of data are changing rapidly. For that you need\\nstream–stream joins, which we will discuss next.\\nStream–Stream Joins\\nThe challenge of generating joins between two data streams is that, at any point in\\ntime, the view of either Dataset is incomplete, making it much harder to find matches\\nbetween inputs. The matching events from the two streams may arrive in any order\\nand may be arbitrarily delayed. For example, in our advertising use case an impres‐\\nsion event and its corresponding click event may arrive out of order, with arbitrary\\ndelays between them. Structured Streaming accounts for such delays by buffering the\\ninput data from both sides as the streaming state, and continuously checking for\\nmatches as new data is received. The conceptual idea is sketched out in Figure 8-11 .\\nFigure 8-11. Ad monetization using a stream–stream join\\nLet’s consider this in more detail, first with inner joins and then with outer joins.\\nInner joins with optional watermarking\\nSay we have redefined our impressions  DataFrame to be a streaming DataFrame. To\\nget the stream of matching impressions and their corresponding clicks, we can use\\nthe same code we used earlier for static joins and stream–static joins:\\n248 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 271}), Document(page_content='# In Python\\n# Streaming DataFrame [adId: String, impressionTime: Timestamp, ...]\\nimpressions  = spark.readStream . ... \\n# Streaming DataFrame[adId: String, clickTime: Timestamp, ...]\\nclicks = spark.readStream . ...\\nmatched = impressions .join(clicks, \"adId\")\\n// In Scala\\n// Streaming DataFrame [adId: String, impressionTime: Timestamp, ...] \\nval impressions  = spark.readStream . ...\\n// Streaming DataFrame[adId: String, clickTime: Timestamp, ...] \\nval clicks = spark.readStream . ...\\nval matched = impressions .join(clicks, \"adId\")\\nEven though the code is the same, the execution is completely different. When this\\nquery is executed, the processing engine will recognize it to be a stream–stream join\\ninstead of a stream–static join. The engine will buffer all clicks and impressions as\\nstate, and will generate a matching impression-and-click as soon as a received click\\nmatches a buffered impression (or vice versa, depending on which was received first).\\nLet’s visualize how this inner join works using the example timeline of events in\\nFigure 8-12 .\\nFigure 8-12. Illustrative timeline of clicks, impressions, and their joined output\\nIn Figure 8-12 , the blue dots represent the event times of impression and click events\\nthat were received across different micro-batches (separated by the dashed grey\\nlines). For the purposes of this illustration, assume that each event was actually\\nreceived at the same wall clock time as the event time. Note the different scenarios\\nunder which the related events are being joined. Both events with adId  = ⧮ were\\nStreaming Joins | 249', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 272}), Document(page_content='received in the same micro-batch, so their joined output was generated by that micro-\\nbatch. However, for adId  = ⧉ the impression was received at 12:04, much earlier\\nthan its corresponding click at 12:13. Structured Streaming will first receive the\\nimpression at 12:04 and buffer it in the state. For each received click, the engine will\\ntry to join it with all buffered impressions (and vice versa). Eventually, in a later\\nmicro-batch running around 12:13, the engine receives the click for adId  = ⧉ and\\ngenerates the joined output.\\nHowever, in this query, we have not given any indication of how long the engine\\nshould buffer an event to find a match. Therefore, the engine may buffer an event for‐\\never and accumulate an unbounded amount of streaming state. To limit the stream‐\\ning state maintained by stream–stream joins, you need to know the following\\ninformation about your use case:\\n•What is the maximum time range between the generation of the two events at their\\nrespective sources?  In the context of our use case, let’s assume that a click can\\noccur within zero seconds to one hour after the corresponding impression.\\n•What is the maximum duration an event can be delayed in transit between the\\nsource and the processing engine?  For example, ad clicks from a browser may get\\ndelayed due to intermittent connectivity and arrive much later than expected,\\nand out of order. Let’s say that impressions and clicks can be delayed by at most\\ntwo and three hours, respectively.\\nThese delay limits and event-time constraints can be encoded in the DataFrame oper‐\\nations using watermarks and time range conditions. In other words, you will have to\\ndo the following additional steps in the join to ensure state cleanup:\\n1.Define watermark delays on both inputs, such that the engine knows how\\ndelayed the input can be (similar to with streaming aggregations).\\n2.Define a constraint on event time across the two inputs, such that the engine can\\nfigure out when old rows of one input are not going to be required (i.e., will not\\nsatisfy the time constraint) for matches with the other input. This constraint can\\nbe defined in one of the following ways:\\na.Time range join conditions (e.g., join condition = \"leftTime BETWEEN\\nrightTime AND rightTime + INTERVAL 1 HOUR\" )\\nb.Join on event-time windows (e.g., join condition = \"leftTimeWindow =\\nrightTimeWindow\" )\\nIn our advertisement use case, our inner join code will get a little bit more\\ncomplicated:\\n250 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 273}), Document(page_content='# In Python\\n# Define watermarks\\nimpressionsWithWatermark  = (impressions\\n  .selectExpr (\"adId AS impressionAdId\" , \"impressionTime\" )\\n  .withWatermark (\"impressionTime\" , \"2 hours\" ))\\nclicksWithWatermark  = (clicks\\n  .selectExpr (\"adId AS clickAdId\" , \"clickTime\" )\\n  .withWatermark (\"clickTime\" , \"3 hours\" ))\\n# Inner join with time range conditions\\n(impressionsWithWatermark .join(clicksWithWatermark ,\\n  expr(\"\"\" \\n    clickAdId = impressionAdId AND \\n    clickTime BETWEEN impressionTime AND impressionTime + interval 1 hour\"\"\" )))\\n// In Scala\\n// Define watermarks\\nval impressionsWithWatermark  = impressions\\n  .selectExpr (\"adId AS impressionAdId\" , \"impressionTime\" )\\n  .withWatermark (\"impressionTime\" , \"2 hours \" )\\nval clicksWithWatermark  = clicks\\n  .selectExpr (\"adId AS clickAdId\" , \"clickTime\" )\\n  .withWatermark (\"clickTime\" , \"3 hours\" )\\n// Inner join with time range conditions\\nimpressionsWithWatermark .join(clicksWithWatermark ,\\n  expr(\"\"\" \\n    clickAdId = impressionAdId AND \\n    clickTime BETWEEN impressionTime AND impressionTime + interval 1 hour\"\"\" ))\\nWith these time constraints for each event, the processing engine can automatically\\ncalculate how long events need to be buffered to generate correct results, and when\\nthe events can be dropped from the state. For example, it will evaluate the following\\n(illustrated in Figure 8-13 ):\\n•Impressions need to be buffered for at most four hours (in event time), as a\\nthree-hour-late click may match with an impression made four hours ago (i.e.,\\nthree hours late + up to one-hour delay between the impression and click).\\n•Conversely, clicks need to be buffered for at most two hours (in event time), as a\\ntwo-hour-late impression may match with a click received two hours ago.\\nStreaming Joins | 251', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 274}), Document(page_content='Figure 8-13. Structured Streaming automatically calculates thresholds for state cleanup\\nusing watermark delays and time range conditions\\nThere are a few key points to remember about inner joins:\\n•For inner joins, specifying watermarking and event-time constraints are both\\noptional. In other words, at the risk of potentially unbounded state, you may\\nchoose not to specify them. Only when both are specified will you get state\\ncleanup.\\n•Similar to the guarantees provided by watermarking on aggregations, a water‐\\nmark delay of two hours guarantees that the engine will never drop or not match\\nany data that is less than two hours delayed, but data delayed by more than two\\nhours may or may not get processed.\\nOuter joins with watermarking\\nThe previous inner join will output only those ads for which both events have been\\nreceived. In other words, ads that received no clicks will not be reported at all.\\nInstead, you may want all ad impressions to be reported, with or without the associ‐\\nated click data, to enable additional analysis later (e.g., click-through rates). This\\nbrings us to stream–stream outer joins . All you need to do to implement this is specify\\nthe outer join type:\\n# In Python\\n# Left outer join with time range conditions\\n(impressionsWithWatermark .join(clicksWithWatermark ,\\n  expr(\"\"\" \\n    clickAdId = impressionAdId AND \\n    clickTime BETWEEN impressionTime AND impressionTime + interval 1 hour\"\"\" ),\\n  \"leftOuter\" ))  # only change: set the outer join type\\n// In Scala\\n// Left outer join with time range conditions\\n252 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 275}), Document(page_content='impressionsWithWatermark .join(clicksWithWatermark ,\\n  expr(\"\"\" \\n    clickAdId = impressionAdId AND \\n    clickTime BETWEEN impressionTime AND impressionTime + interval 1 hour\"\"\" ),\\n  \"leftOuter\" )  // Only change: set the outer join type\\nAs expected of outer joins, this query will start generating output for every impres‐\\nsion, with or without (i.e., using NULL s) the click data. However, there are a few addi‐\\ntional points to note about outer joins:\\n•Unlike with inner joins, the watermark delay and event-time constraints are not\\noptional for outer joins. This is because for generating the NULL  results, the\\nengine must know when an event is not going to match with anything else in the\\nfuture. For correct outer join results and state cleanup, the watermarking and\\nevent-time constraints must be specified.\\n•Consequently, the outer NULL  results will be generated with a delay as the engine\\nhas to wait for a while to ensure that there neither were nor would be any\\nmatches. This delay is the maximum buffering time (with respect to event time)\\ncalculated by the engine for each event as discussed in the previous section (i.e.,\\nfour hours for impressions and two hours for clicks).\\nArbitrary Stateful Computations\\nMany use cases require more complicated logic than the SQL operations we have dis‐\\ncussed up to now. For example, say you want to track the statuses (e.g., signed in,\\nbusy, idle) of users by tracking their activities (e.g., clicks) in real time. To build this\\nstream processing pipeline, you will have to track each user’s activity history as a state\\nwith arbitrary data structure, and continuously apply arbitrarily complex changes on\\nthe data structure based on the user’s actions. The operation mapGroupsWithState()\\nand its more flexible counterpart flatMapGroupsWithState()  are designed for such\\ncomplex analytical use cases.\\nAs of Spark 3.0, these two operations are only available in Scala and\\nJava.\\nIn this section, we will start with a simple example with mapGroupsWithState()  to\\nillustrate the four key steps to modeling custom state data and defining custom oper‐\\nations on it. Then we will discuss the concept of timeouts and how you can use them\\nto expire state that has not been updated for a while. We will end with\\nflatMapGroupsWithState() , which gives you even more flexibility.\\nArbitrary Stateful Computations | 253', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 276}), Document(page_content='Modeling Arbitrary Stateful Operations with mapGroupsWithState()\\nState with an arbitrary schema and arbitrary transformations on the state is modeled\\nas a user-defined function that takes the previous version of the state value and new\\ndata as inputs, and generates the updated state and computed result as outputs. Pro‐\\ngrammatically in Scala, you will have to define a function with the following signature\\n(K, V, S, and U are data types, as explained shortly):\\n// In Scala\\ndef arbitraryStateUpdateFunction (\\n    key: K, \\n    newDataForKey : Iterator [V], \\n    previousStateForKey : GroupState [S]\\n): U\\nThis function is provided to a streaming query using the operations groupByKey()\\nand mapGroupsWithState() , as follows:\\n// In Scala\\nval inputDataset : Dataset[V] =  // input streaming Dataset\\ninputDataset\\n  .groupByKey (keyFunction )   // keyFunction() generates key from input\\n  .mapGroupsWithState (arbitraryStateUpdateFunction )\\nWhen this streaming query is started, in each micro-batch Spark will call this\\narbitraryStateUpdateFunction()  for each unique key in the micro-batch’s data.\\nLet’s take a closer look at what the parameters are and what parameter values Spark\\nwill call the function with:\\nkey: K\\nK is the data type of the common keys defined in the state and the input. Spark\\nwill call this function for each unique key in the data.\\nnewDataForKey: Iterator[V]\\nV is the data type of the input Dataset. When Spark calls this function for a key,\\nthis parameter will have all the new input data corresponding to that key. Note\\nthat the order in which the input data objects will be present in the iterator is not\\ndefined.\\npreviousStateForKey: GroupState[S]\\nS is the data type of the arbitrary state you are going to maintain, and Group\\nState[S]  is a typed wrapper object that provides methods to access and manage\\nthe state value. When Spark calls this function for a key, this object will provide\\nthe state value set the previous time Spark called this function for that key (i.e.,\\nfor one of the previous micro-batches).\\n254 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 277}), Document(page_content='U\\nU is the data type of the output of the function.\\nThere are a couple of additional parameters that you have to pro‐\\nvide. All the types ( K, V, S, U) must be encodable by Spark SQL ’s\\nencoders. Accordingly, in mapGroupsWithState() , you have to pro‐\\nvide the typed encoders for S and U either implicitly in Scala or\\nexplicitly in Java. See “Dataset Encoders”  on page 168 in Chapter 6\\nfor more details.\\nLet’s examine how to express the desired state update function in this format with an\\nexample. Say we want to understand user behavior based on their actions. Conceptu‐\\nally, it’s quite simple: in every micro-batch, for each active user, we will use the new\\nactions taken by the user and update the user’s “status. ” Programmatically, we can\\ndefine the state update function with the following steps:\\n1.Define the data types. We need to define the exact types of K, V, S, and U. In this\\ncase, we’ll use the following:\\na.Input data ( V) = case class UserAction(userId: String, action:\\nString)\\nb.Keys (K) = String  (that is, the userId )\\nc.State (S) = case class UserStatus(userId: String, active: Boolean)\\nd.Output (U) = UserStatus , as we want to output the latest user status\\nNote that all these data types are supported in encoders.\\n2.Define the function. Based on the chosen types, let’s translate the conceptual idea\\ninto code. When this function is called with new user actions, there are two main\\nsituations we need to handle: whether a previous state (i.e., previous user status)\\nexists for that key (i.e., userId ) or not. Accordingly, we will initialize the user’s\\nstatus, or update the existing status with the new actions. We will explicitly\\nupdate the state with the new running count, and finally return the updated\\nuserId -userStatus  pair:\\n// In Scala\\nimport org.apache.spark.sql.streaming._\\n def updateUserStatus (\\n    userId: String, \\n    newActions : Iterator [UserAction ],\\n    state: GroupState [UserStatus ]): UserStatus  = {\\n  val userStatus  = state.getOption .getOrElse  {\\n    new UserStatus (userId, false)\\nArbitrary Stateful Computations | 255', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 278}), Document(page_content='  }\\n  newActions .foreach { action => \\n    userStatus .updateWith (action) \\n  }\\n  state.update(userStatus ) \\n  return userStatus\\n}\\n3.Apply the function on the actions. We will group the input actions Dataset\\nusing groupByKey()  and then apply the updateUserStatus  function using\\nmapGroupsWithState() :\\n// In Scala\\nval userActions : Dataset[UserAction ] = ...\\nval latestStatuses  = userActions\\n  .groupByKey (userAction  => userAction .userId) \\n  .mapGroupsWithState (updateUserStatus  _)\\nOnce we start this streaming query with console output, we will see the updated user\\nstatuses being printed.\\nBefore we move on to more advanced topics, there are a few notable points to\\nremember:\\n•When the function is called, there is no well-defined order for the input records\\nin the new data iterator (e.g., newActions ). If you need to update the state with\\nthe input records in a specific order (e.g., in the order the actions were per‐\\nformed), then you have to explicitly reorder them (e.g., based on the event time‐\\nstamp or some other ordering ID). In fact, if there is a possibility that actions\\nmay be read out of order from the source, then you have to consider the possibil‐\\nity that a future micro-batch may receive data that should be processed before the\\ndata in the current batch. In that case, you have to buffer the records as part of\\nthe state.\\n•In a micro-batch, the function is called on a key once only if the micro-batch has\\ndata for that key. For example, if a user becomes inactive and provides no new\\nactions for a long time, then by default, the function will not be called for a long\\ntime. If you want to update or remove state based on a user’s inactivity over an\\nextended period you have to use timeouts, which we will discuss in the next\\nsection.\\n•The output of mapGroupsWithState()  is assumed by the incremental processing\\nengine to be continuously updated key/value records, similar to the output of\\naggregations. This limits what operations are supported in the query after\\nmapGroupsWithState() , and what sinks are supported. For example, appending\\nthe output into files is not supported. If you want to apply arbitrary stateful\\n256 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 279}), Document(page_content='operations  with greater flexibility, then you have to use flatMapGroupsWith\\nState() . We will discuss that after timeouts.\\nUsing Timeouts to Manage Inactive Groups\\nIn the preceding example of tracking active user sessions, as more users become\\nactive, the number of keys in the state will keep increasing, and so will the memory\\nused by the state. Now, in a real-world scenario, users are likely not going to stay\\nactive all the time. It may not be very useful to keep the status of inactive users in the\\nstate, as it is not going to change again until those users become active again. Hence,\\nwe may want to explicitly drop all information for inactive users. However, a user\\nmay not explicitly take any action to become inactive (e.g., explicitly logging off), and\\nwe may have to define inactivity as lack of any action for a threshold duration. This\\nbecomes tricky to encode in the function, as the function is not called for a user until\\nthere are new actions from that user.\\nTo encode time-based inactivity, mapGroupsWithState()  supports timeouts that are\\ndefined as follows:\\n•Each time the function is called on a key, a timeout can be set on the key based\\non a duration or a threshold timestamp.\\n•If that key does not receive any data, such that the timeout condition is met, the\\nkey is marked as “timed out. ” The next micro-batch will call the function on this\\ntimed-out key even if there is no data for that key in that micro-batch. In this\\nspecial function call, the new input data iterator will be empty (since there is no\\nnew data) and GroupState.hasTimedOut()  will return true . This is the best way\\nto identify inside the function whether the call was due to new data or a timeout.\\nThere are two types of timeouts, based on our two notions of time: processing time\\nand event time. The processing-time timeout is the simpler of the two to use, so we’ll\\nstart with that.\\nProcessing-time timeouts\\nProcessing-time timeouts are based on the system time (also known as the wall clock\\ntime) of the machine running the streaming query and are defined as follows: if a key\\nlast received data at system timestamp T, and the current timestamp is more than (T\\n+ <timeout duration> ), then the function will be called again with a new empty\\ndata iterator.\\nLet’s investigate how to use timeouts by updating our user example to remove a user’s\\nstate based on one hour of inactivity. We will make three changes:\\nArbitrary Stateful Computations | 257', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 280}), Document(page_content='•In mapGroupsWithState() , we will specify the timeout as GroupStateTime\\nout.ProcessingTimeTimeout .\\n•In the state update function, before updating the state with new data, we have to\\ncheck whether the state has timed out or not. Accordingly, we will update or\\nremove the state.\\n•In addition, every time we update the state with new data, we will set the timeout\\nduration.\\nHere’s the updated code:\\n// In Scala\\ndef updateUserStatus (\\n    userId: String, \\n    newActions : Iterator [UserAction ],\\n    state: GroupState [UserStatus ]): UserStatus  = {\\n  if (!state.hasTimedOut ) {       // Was not called due to timeout\\n    val userStatus  = state.getOption .getOrElse  {\\n      new UserStatus (userId, false)\\n    }\\n    newActions .foreach { action => userStatus .updateWith (action) }\\n    state.update(userStatus ) \\n    state.setTimeoutDuration (\"1 hour\" ) // Set timeout duration\\n    return userStatus\\n    \\n  } else {\\n    val userStatus  = state.get()\\n    state.remove()                  // Remove state when timed out\\n    return userStatus .asInactive ()  // Return inactive user\\'s status\\n  }\\n}\\n \\nval latestStatuses  = userActions\\n  .groupByKey (userAction  => userAction .userId) \\n  .mapGroupsWithState (\\n    GroupStateTimeout .ProcessingTimeTimeout )(\\n    updateUserStatus  _)\\nThis query will automatically clean up the state of users for whom the query has not\\nprocessed any data for more than an hour. However, there are a few points to note\\nabout timeouts:\\n•The timeout set by the last call to the function is automatically cancelled when\\nthe function is called again, either for the new received data or for the timeout.\\nHence, whenever the function is called, the timeout duration or timestamp needs\\nto be explicitly set to enable the timeout.\\n•Since the timeouts are processed during the micro-batches, the timing of\\ntheir execution is imprecise and depends heavily on the trigger interval and\\n258 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 281}), Document(page_content='micro-batch  processing times. Therefore, it is not advised to use timeouts for\\nprecise timing control.\\n•While processing-time timeouts are simple to reason about, they are not robust\\nto slowdowns and downtimes. If the streaming query suffers a downtime of more\\nthan one hour, then after restart, all the keys in the state will be timed out because\\nmore than one hour has passed since each key received data. Similar wide-scale\\ntimeouts can occur if the query processes data slower than it is arriving at the\\nsource (e.g., if data is arriving and getting buffered in Kafka). For example, if the\\ntimeout is five minutes, then a sudden drop in processing rate (or spike in data\\narrival rate) that causes a five-minute lag could produce spurious timeouts. To\\navoid such issues we can use an event-time timeout, which we will discuss next.\\nEvent-time timeouts\\nInstead of the system clock time, an event-time timeout is based on the event time in\\nthe data (similar to time-based aggregations) and a watermark defined on the event\\ntime. If a key is configured with a specific timeout timestamp of T (i.e., not a dura‐\\ntion), then that key will time out when the watermark exceeds T if no new data was\\nreceived for that key since the last time the function was called. Recall that the water‐\\nmark is a moving threshold that lags behind the maximum event time seen while pro‐\\ncessing the data. Hence, unlike system time, the watermark moves forward in time at\\nthe same rate as the data is processed. This means (unlike with processing-time time‐\\nouts) any slowdown or downtime in query processing will not cause spurious\\ntimeouts.\\nLet’s modify our example to use an event-time timeout. In addition to the changes we\\nalready made for using the processing-time timeout, we will make the following\\nchanges:\\n•Define watermarks on the input Dataset (assume that the class UserAction  has\\nan eventTimestamp  field). Recall that the watermark threshold represents the\\nacceptable amount of time by which input data can be late and out of order.\\n•Update mapGroupsWithState()  to use EventTimeTimeout .\\n•Update the function to set the threshold timestamp at which the timeout will\\noccur. Note that event-time timeouts do not allow setting a timeout duration, like\\nprocessing-time timeouts. We will discuss the reason for this later. In this exam‐\\nple, we will calculate this timeout as the current watermark plus one hour.\\nHere is the updated example:\\nArbitrary Stateful Computations | 259', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 282}), Document(page_content='// In Scala\\ndef updateUserStatus (\\n    userId: String, \\n    newActions : Iterator [UserAction ],\\n    state: GroupState [UserStatus ]):UserStatus  = {\\n  if (!state.hasTimedOut ) {  // Was not called due to timeout\\n    val userStatus  = if (state.getOption .getOrElse  {\\n      new UserStatus ()\\n    }\\n    newActions .foreach { action => userStatus .updateWith (action) }\\n    state.update(userStatus )\\n    // Set the timeout timestamp to the current watermark + 1 hour\\n    state.setTimeoutTimestamp (state.getCurrentWatermarkMs , \"1 hour\" ) \\n    return userStatus\\n  } else {\\n    val userStatus  = state.get()\\n    state.remove()\\n    return userStatus .asInactive () }\\n}\\nval latestStatuses  = userActions\\n  .withWatermark (\"eventTimestamp\" , \"10 minutes\" ) \\n  .groupByKey (userAction  => userAction .userId) \\n  .mapGroupsWithState (\\n    GroupStateTimeout .EventTimeTimeout )(\\n    updateUserStatus  _)\\nThis query will be much more robust to spurious timeouts caused by restarts and\\nprocessing delays.\\nHere are a few points to note about event-time timeouts:\\n•Unlike in the previous example with processing-time timeouts, we have used\\nGroupState.setTimeoutTimestamp()  instead of GroupState.setTimeoutDura\\ntion() . This is because with processing-time timeouts the duration is sufficient\\nto calculate the exact future timestamp (i.e., current system time + specified\\nduration) when the timeout would occur, but this is not the case for event-time\\ntimeouts. Different applications may want to use different strategies to calculate\\nthe threshold timestamp. In this example we simply calculate it based on the cur‐\\nrent watermark, but a different application may instead choose to calculate a key’s\\ntimeout timestamp based on the maximum event-time timestamp seen for that\\nkey (tracked and saved as part of the state).\\n•The timeout timestamp must be set to a value larger than the current watermark.\\nThis is because the timeout is expected to happen when the timestamp crosses\\nthe watermark, so it’s illogical to set the timestamp to a value already larger than\\nthe current watermark.\\n260 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 283}), Document(page_content='Before we move on from timeouts, one last thing to remember is that you can use\\nthese timeout mechanisms for more creative processing than fixed-duration time‐\\nouts. For example, you can implement an approximately periodic task (say, every\\nhour) on the state by saving the last task execution timestamp in the state and using\\nthat to set the processing-time timeout duration, as shown in this code snippet:\\n// In Scala\\ntimeoutDurationMs  = lastTaskTimstampMs  + periodIntervalMs  - \\ngroupState .getCurrentProcessingTimeMs ()\\nGeneralization with flatMapGroupsWithState()\\nThere are two key limitations with mapGroupsWithState()  that may limit the flexibil‐\\nity that we want to implement more complex use cases (e.g., chained sessionizations):\\n•Every time mapGroupsWithState()  is called, you have to return one and only one\\nrecord. For some applications, in some triggers, you may not want to output any‐\\nthing at all.\\n•With mapGroupsWithState() , due to the lack of more information about the\\nopaque state update function, the engine assumes that generated records are\\nupdated key/value data pairs. Accordingly, it reasons about downstream opera‐\\ntions and allows or disallows some of them. For example, the DataFrame gener‐\\nated using mapGroupsWithState()  cannot be written out in append mode to files.\\nHowever, some applications may want to generate records that can be considered\\nas appends.\\nflatMapGroupsWithState()  overcomes these limitations, at the cost of slightly more\\ncomplex syntax. It has two differences from mapGroupsWithState() :\\n•The return type is an iterator, instead of a single object. This allows the function\\nto return any number of records, or, if needed, no records at all.\\n•It takes another parameter, called the operator output mode  (not to be confused\\nwith the query output modes we discussed earlier in the chapter), that defines\\nwhether the output records are new records that can be appended ( Output\\nMode.Append ) or updated key/value records ( OutputMode.Update ).\\nTo illustrate the use of this function, let’s extend our user tracking example (we have\\nremoved timeouts to keep the code simple). For example, if we want to generate alerts\\nonly for certain user changes and we want to write the output alerts to files, we can do\\nthe following:\\n// In Scala\\ndef getUserAlerts (\\n    userId: String, \\n    newActions : Iterator [UserAction ],\\nArbitrary Stateful Computations | 261', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 284}), Document(page_content='    state: GroupState [UserStatus ]): Iterator [UserAlert ] = {\\n  val userStatus  = state.getOption .getOrElse  {\\n    new UserStatus (userId, false) \\n  }\\n  newActions .foreach { action => \\n    userStatus .updateWith (action)\\n  } \\n  state.update(userStatus )\\n  // Generate any number of alerts\\n  return userStatus .generateAlerts ().toIterator   \\n}\\nval userAlerts  = userActions\\n  .groupByKey (userAction  => userAction .userId) \\n  .flatMapGroupsWithState (\\n    OutputMode .Append, \\n    GroupStateTimeout .NoTimeout )(\\n    getUserAlerts )\\nPerformance Tuning\\nStructured Streaming uses the Spark SQL engine and therefore can be tuned with the\\nsame parameters as those discussed for Spark SQL in Chapters 5 and 7. However,\\nunlike batch jobs that may process gigabytes to terabytes of data, micro-batch jobs\\nusually process much smaller volumes of data. Hence, a Spark cluster running\\nstreaming queries usually needs to be tuned slightly differently. Here are a few con‐\\nsiderations to keep in mind:\\nCluster resource provisioning\\nSince Spark clusters running streaming queries are going to run 24/7, it is impor‐\\ntant to provision resources appropriately. Underprovisoning the resources can\\ncause the streaming queries to fall behind (with micro-batches taking longer and\\nlonger), while overprovisioning (e.g., allocated but unused cores) can cause\\nunnecessary costs. Furthermore, allocation should be done based on the nature\\nof the streaming queries: stateless queries usually need more cores, and stateful\\nqueries usually need more memory.\\nNumber of partitions for shuffles\\nFor Structured Streaming queries, the number of shuffle partitions usually needs\\nto be set much lower than for most batch queries—dividing the computation too\\nmuch increases overheads and reduces throughput. Furthermore, shuffles due to\\nstateful operations have significantly higher task overheads due to checkpointing.\\nHence, for streaming queries with stateful operations and trigger intervals of a\\nfew seconds to minutes, it is recommended to tune the number of shuffle\\n262 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 285}), Document(page_content='partitions from the default value of 200 to at most two to three times the number\\nof allocated cores.\\nSetting source rate limits for stability\\nAfter the allocated resources and configurations have been optimized for a\\nquery’s expected input data rates, it’s possible that sudden surges in data rates can\\ngenerate unexpectedly large jobs and subsequent instability. Besides the costly\\napproach of overprovisioning, you can safeguard against instability using source\\nrate limits. Setting limits in supported sources (e.g., Kafka and files) prevents a\\nquery from consuming too much data in a single micro-batch. The surge data\\nwill stay buffered in the source, and the query will eventually catch up. However,\\nnote the following:\\n•Setting the limit too low can cause the query to underutilize allocated resour‐\\nces and fall behind the input rate.\\n•Limits do not effectively guard against sustained increases in input rate.\\nWhile stability is maintained, the volume of buffered, unprocessed data will\\ngrow indefinitely at the source and so will the end-to-end latencies.\\nMultiple streaming queries in the same Spark application\\nRunning multiple streaming queries in the same SparkContext  or SparkSession\\ncan lead to fine-grained resource sharing. However:\\n•Executing each query continuously uses resources in the Spark driver (i.e.,\\nthe JVM where it is running). This limits the number of queries that the\\ndriver can execute simultaneously. Hitting those limits can either bottleneck\\nthe task scheduling (i.e., underutilizing the executors) or exceed memory\\nlimits.\\n•Y ou can ensure fairer resource allocation between queries in the same\\ncontext  by setting them to run in separate scheduler pools. Set the\\nSparkContext ’s thread-local property spark.scheduler.pool  to a different\\nstring value for each stream:\\n// In Scala\\n// Run streaming query1 in scheduler pool1\\nspark.sparkContext .setLocalProperty (\"spark.scheduler.pool\" , \"pool1\")\\ndf.writeStream .queryName (\"query1\" ).format(\"parquet\" ).start(path1)\\n// Run streaming query2 in scheduler pool2\\nspark.sparkContext .setLocalProperty (\"spark.scheduler.pool\" , \"pool2\")\\ndf.writeStream .queryName (\"query2\" ).format(\"parquet\" ).start(path2)\\nPerformance Tuning | 263', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 286}), Document(page_content='# In Python\\n# Run streaming query1 in scheduler pool1\\nspark.sparkContext .setLocalProperty (\"spark.scheduler.pool\" , \"pool1\")\\ndf.writeStream .queryName (\"query1\" ).format(\"parquet\" ).start(path1)\\n# Run streaming query2 in scheduler pool2\\nspark.sparkContext .setLocalProperty (\"spark.scheduler.pool\" , \"pool2\")\\ndf.writeStream .queryName (\"query2\" ).format(\"parquet\" ).start(path2)\\nSummary\\nThis chapter explored writing Structured Streaming queries using the DataFrame\\nAPI. Specifically, we discussed:\\n•The central philosophy of Structured Streaming and the processing model of\\ntreating input data streams as unbounded tables\\n•The key steps to define, start, restart, and monitor streaming queries\\n•How to use various built-in streaming sources and sinks and write custom\\nstreaming sinks\\n•How to use and tune managed stateful operations like streaming aggregations\\nand stream–stream joins\\n•Techniques for expressing custom stateful computations\\nBy working through the code snippets in the chapter and the notebooks in the book’s\\nGitHub repo , you will get a feel for how to use Structured Streaming effectively. In the\\nnext chapter, we explore how you can manage structured data read and written\\nsimultaneously from batch and streaming workloads.\\n264 | Chapter 8: Structured Streaming', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 287}), Document(page_content='CHAPTER 9\\nBuilding Reliable Data Lakes\\nwith Apache Spark\\nIn the previous chapters, you learned how to easily and effectively use Apache Spark\\nto build scalable and performant data processing pipelines. However, in practice,\\nexpressing the processing logic only solves half of the end-to-end problem of building\\na pipeline. For a data engineer, data scientist, or data analyst, the ultimate goal of\\nbuilding pipelines is to query the processed data and get insights from it. The choice\\nof storage solution determines the end-to-end (i.e., from raw data to insights) robust‐\\nness and performance of the data pipeline.\\nIn this chapter, we will first discuss the key features of a storage solution that you\\nneed to look out for. Then we will discuss two broad classes of storage solutions, data‐\\nbases and data lakes, and how to use Apache Spark with them. Finally, we will intro‐\\nduce the next wave of storage solution, called lakehouses, and explore some of the\\nnew open source processing engines in this space.\\nThe Importance of an Optimal Storage Solution\\nHere are some of the properties that are desired in a storage solution:\\nScalability and performance\\nThe storage solution should be able to scale to the volume of data and provide\\nthe read/write throughput and latency that the workload requires.\\nTransaction support\\nComplex workloads are often reading and writing data concurrently, so support\\nfor ACID transactions  is essential to ensure the quality of the end results.\\n265', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 288}), Document(page_content='Support for diverse data formats\\nThe storage solution should be able to store unstructured data (e.g., text files like\\nraw logs), semi-structured data (e.g., JSON data), and structured data (e.g., tabu‐\\nlar data).\\nSupport for diverse workloads\\nThe storage solution should be able to support a diverse range of business work‐\\nloads, including:\\n•SQL workloads like traditional BI analytics\\n•Batch workloads like traditional ETL jobs processing raw unstructured data\\n•Streaming workloads like real-time monitoring and alerting\\n•ML and AI workloads like recommendations and churn predictions\\nOpenness\\nSupporting a wide range of workloads often requires the data to be stored in\\nopen data formats. Standard APIs allow the data to be accessed from a variety of\\ntools and engines. This allows the business to use the most optimal tools for each\\ntype of workload and make the best business decisions.\\nOver time, different kinds of storage solutions have been proposed, each with its\\nunique advantages and disadvantages with respect to these properties. In this chapter,\\nwe will explore how the available storage solutions evolved from databases  to data\\nlakes , and how to use Apache Spark with each of them. We’ll then turn our attention\\nto the next generation of storage solutions, often called data lakehouses , that can pro‐\\nvide the best of both worlds: the scalability and flexibility of data lakes with the trans‐\\nactional guarantees of databases.\\nDatabases\\nFor many decades, databases have been the most reliable solution for building data\\nwarehouses to store business-critical data. In this section, we will explore the archi‐\\ntecture of databases and their workloads, and how to use Apache Spark for analytics\\nworkloads on databases. We will end this section with a discussion of the limitations\\nof databases in supporting modern non-SQL workloads.\\nA Brief Introduction to Databases\\nDatabases are designed to store structured data as tables, which can be read using\\nSQL queries. The data must adhere to a strict schema, which allows a database man‐\\nagement system to heavily co-optimize the data storage and processing. That is, they\\ntightly couple their internal layout of the data and indexes in on-disk files with their\\nhighly optimized query processing engines, thus providing very fast computations on\\n266 | Chapter 9: Building Reliable Data Lakes with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 289}), Document(page_content='the stored data along with strong transactional ACID guarantees on all read/write\\noperations.\\nSQL workloads on databases can be broadly classified into two categories, as follows:\\nOnline transaction processing (OLTP) workloads\\nLike bank account transactions, OLTP workloads are typically high-concurrency,\\nlow-latency, simple queries that read or update a few records at a time.\\nOnline analytical processing (OLAP)\\nOLAP workloads, like periodic reporting, are typically complex queries (involv‐\\ning aggregates and joins) that require high-throughput scans over many records.\\nIt is important to note that Apache Spark is a query engine that is primarily designed\\nfor OLAP workloads, not OLTP workloads. Hence, in the rest of the chapter we are\\ngoing to focus our discussion on storage solutions for analytical workloads. Next, let’s\\nsee how Apache Spark can be used to read from and write to databases.\\nReading from and Writing to Databases Using Apache Spark\\nThanks to the ever-growing ecosystem of connectors, Apache Spark can connect to a\\nwide variety of databases for reading and writing data. For databases that have JDBC\\ndrivers (e.g., PostgreSQL, MySQL), you can use the built-in JDBC data source along\\nwith the appropriate JDBC driver jars to access the data. For many other modern\\ndatabases (e.g., Azure Cosmos DB, Snowflake), there are dedicated connectors that\\nyou can invoke using the appropriate format name. Several examples were discussed\\nin detail in Chapter 5 . This makes it very easy to augment your data warehouses and\\ndatabases with workloads and use cases based on Apache Spark.\\nLimitations of Databases\\nSince the last century, databases and SQL queries have been known as great building\\nsolutions for BI workloads. However, the last decade has seen two major new trends\\nin analytical workloads:\\nGrowth in data sizes\\nWith the advent of big data, there has been a global trend in the industry to\\nmeasure and collect everything (page views, clicks, etc.) in order to understand\\ntrends and user behaviors. As a result, the amount of data collected by any com‐\\npany or organization has increased from gigabytes a couple of decades ago to ter‐\\nabytes and petabytes today.\\nGrowth in the diversity of analytics\\nAlong with the increase in data collection, there is a need for deeper insights.\\nThis has led to an explosive growth of complex analytics like machine learning\\nand deep learning.\\nDatabases | 267', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 290}), Document(page_content='Databases have been shown to be rather inadequate at accommodating these new\\ntrends, because of the following limitations:\\nDatabases are extremely expensive to scale out\\nAlthough databases are extremely efficient at processing data on a single\\nmachine, the rate of growth of data volumes has far outpaced the growth in per‐\\nformance capabilities of a single machine. The only way forward for processing\\nengines is to scale out—that is, use multiple machines to process data in parallel.\\nHowever, most databases, especially the open source ones, are not designed for\\nscaling out to perform distributed processing. The few industrial database solu‐\\ntions that can remotely keep up with the processing requirements tend to be pro‐\\nprietary solutions running on specialized hardware, and are therefore very\\nexpensive to acquire and maintain.\\nDatabases do not support non–SQL based analytics very well\\nDatabases store data in complex (often proprietary) formats that are typically\\nhighly optimized for only that database’s SQL processing engine to read. This\\nmeans other processing tools, like machine learning and deep learning systems,\\ncannot efficiently access the data (except by inefficiently reading all the data from\\nthe database). Nor can databases be easily extended to perform non–SQL based\\nanalytics like machine learning.\\nThese limitations of databases led to the development of a completely different\\napproach to storing data, known as data lakes .\\nData Lakes\\nIn contrast to most databases, a data lake is a distributed storage solution that runs on\\ncommodity hardware and easily scales out horizontally. In this section, we will start\\nwith a discussion of how data lakes satisfy the requirements of modern workloads,\\nthen see how Apache Spark integrates with data lakes to make workloads scale to data\\nof any size. Finally, we will explore the impact of the architectural sacrifices made by\\ndata lakes to achieve scalability.\\nA Brief Introduction to Data Lakes\\nThe data lake architecture, unlike that of databases, decouples the distributed storage\\nsystem from the distributed compute system. This allows each system to scale out as\\nneeded by the workload. Furthermore, the data is saved as files with open formats,\\nsuch that any processing engine can read and write them using standard APIs. This\\nidea was popularized in the late 2000s by the Hadoop File System (HDFS) from the\\nApache Hadoop project , which itself was heavily inspired by the research paper “The\\nGoogle File System”  by Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung.\\n268 | Chapter 9: Building Reliable Data Lakes with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 291}), Document(page_content='Organizations build their data lakes by independently choosing the following:\\nStorage system\\nThey choose to either run HDFS on a cluster of machines or use any cloud object\\nstore (e.g., AWS S3, Azure Data Lake Storage, or Google Cloud Storage).\\nFile format\\nDepending on the downstream workloads, the data is stored as files in either\\nstructured (e.g., Parquet, ORC), semi-structured (e.g., JSON), or sometimes even\\nunstructured formats (e.g., text, images, audio, video).\\nProcessing engine(s)\\nAgain, depending on the kinds of analytical workloads to be performed, a pro‐\\ncessing engine is chosen. This can either be a batch processing engine (e.g.,\\nSpark, Presto, Apache Hive), a stream processing engine (e.g., Spark, Apache\\nFlink), or a machine learning library (e.g., Spark MLlib, scikit-learn, R).\\nThis flexibility—the ability to choose the storage system, open data format, and pro‐\\ncessing engine that are best suited to the workload at hand—is the biggest advantage\\nof data lakes over databases. On the whole, for the same performance characteristics,\\ndata lakes often provide a much cheaper solution than databases. This key advantage\\nhas led to the explosive growth of the big data ecosystem. In the next section, we will\\ndiscuss how you can use Apache Spark to read and write common file formats on any\\nstorage system.\\nReading from and Writing to Data Lakes using Apache Spark\\nApache Spark is one of the best processing engines to use when building your own\\ndata lake, because it provides all the key features they require:\\nSupport for diverse workloads\\nSpark provides all the necessary tools to handle a diverse range of workloads,\\nincluding batch processing, ETL operations, SQL workloads using Spark SQL,\\nstream processing using Structured Streaming (discussed in Chapter 8 ), and\\nmachine learning using MLlib (discussed in Chapter 10 ), among many others.\\nSupport for diverse file formats\\nIn Chapter 4 , we explored in detail how Spark has built-in support for unstruc‐\\ntured, semi-structured, and structured file formats.\\nSupport for diverse filesystems\\nSpark supports accessing data from any storage system that supports Hadoop’s\\nFileSystem APIs. Since this API has become the de facto standard in the big data\\necosystem, most cloud and on-premises storage systems provide implementa‐\\ntions for it—which means Spark can read from and write to most storage\\nsystems.\\nData Lakes | 269', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 292}), Document(page_content='However, for many filesystems (especially those based on cloud storage, like AWS S3),\\nyou have to configure Spark such that it can access the filesystem in a secure manner.\\nFurthermore, cloud storage systems often do not have the same file operation seman‐\\ntics expected from a standard filesystem (e.g., eventual consistency in S3), which can\\nlead to inconsistent results if you do not configure Spark accordingly. See the docu‐\\nmentation on cloud integration  for details.\\nLimitations of Data Lakes\\nData lakes are not without their share of flaws, the most egregious of which is the lack\\nof transactional guarantees. Specifically, data lakes fail to provide ACID guarantees\\non:\\nAtomicity and isolation\\nProcessing engines write data in data lakes as many files in a distributed manner.\\nIf the operation fails, there is no mechanism to roll back the files already written,\\nthus leaving behind potentially corrupted data (the problem is exacerbated when\\nconcurrent workloads modify the data because it is very difficult to provide isola‐\\ntion across files without higher-level mechanisms).\\nConsistency\\nLack of atomicity on failed writes further causes readers to get an inconsistent\\nview of the data. In fact, it is hard to ensure data quality even in successfully writ‐\\nten data. For example, a very common issue with data lakes is accidentally writ‐\\ning out data files in a format and schema inconsistent with existing data.\\nTo work around these limitations of data lakes, developers employ all sorts of tricks.\\nHere are a few examples:\\n•Large collections of data files in data lakes are often “partitioned” by subdirecto‐\\nries based on a column’s value (e.g., a large Parquet-formatted Hive table parti‐\\ntioned by date). To achieve atomic modifications of existing data, often entire\\nsubdirectories are rewritten (i.e., written to a temporary directory, then refer‐\\nences swapped) just to update or delete a few records.\\n•The schedules of data update jobs (e.g., daily ETL jobs) and data querying jobs\\n(e.g., daily reporting jobs) are often staggered to avoid concurrent access to the\\ndata and any inconsistencies caused by it.\\nAttempts to eliminate such practical issues have led to the development of new sys‐\\ntems, such as lakehouses.\\n270 | Chapter 9: Building Reliable Data Lakes with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 293}), Document(page_content='Lakehouses: The Next Step in the Evolution of\\nStorage Solutions\\nThe lakehouse  is a new paradigm that combines the best elements of data lakes and\\ndata warehouses for OLAP workloads. Lakehouses are enabled by a new system\\ndesign that provides data management features similar to databases directly on the\\nlow-cost, scalable storage used for data lakes. More specifically, they provide the fol‐\\nlowing features:\\nTransaction support\\nSimilar to databases, lakehouses provide ACID guarantees in the presence of\\nconcurrent workloads.\\nSchema enforcement and governance\\nLakehouses prevent data with an incorrect schema being inserted into a table,\\nand when needed, the table schema can be explicitly evolved to accommodate\\never-changing data. The system should be able to reason about data integrity, and\\nit should have robust governance and auditing mechanisms.\\nSupport for diverse data types in open formats\\nUnlike databases, but similar to data lakes, lakehouses can store, refine, analyze,\\nand access all types of data needed for many new data applications, be it struc‐\\ntured, semi-structured, or unstructured. To enable a wide variety of tools to\\naccess it directly and efficiently, the data must be stored in open formats with\\nstandardized APIs to read and write them.\\nSupport for diverse workloads\\nPowered by the variety of tools reading data using open APIs, lakehouses enable\\ndiverse workloads to operate on data in a single repository. Breaking down iso‐\\nlated data silos (i.e., multiple repositories for different categories of data) enables\\ndevelopers to more easily build diverse and complex data solutions, from tradi‐\\ntional SQL and streaming analytics to machine learning.\\nSupport for upserts and deletes\\nComplex use cases like change-data-capture (CDC)  and slowly changing dimen‐\\nsion (SCD)  operations require data in tables to be continuously updated. Lake‐\\nhouses allow data to be concurrently deleted and updated with transactional\\nguarantees.\\nData governance\\nLakehouses provide the tools with which you can reason about data integrity and\\naudit all the data changes for policy compliance.\\nCurrently, there are a few open source systems, such as Apache Hudi, Apache Iceberg,\\nand Delta Lake, that can be used to build lakehouses with these properties. At a very\\nLakehouses: The Next Step in the Evolution of Storage Solutions | 271', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 294}), Document(page_content='high level, all three projects have a similar architecture inspired by well-known data‐\\nbase principles. They are all open data storage formats that do the following:\\n•Store large volumes of data in structured file formats on scalable filesystems.\\n•Maintain a transaction log to record a timeline of atomic changes to the data\\n(much like databases).\\n•Use the log to define versions of the table data and provide snapshot isolation\\nguarantees between readers and writers.\\n•Support reading and writing to tables using Apache Spark.\\nWithin these broad strokes, each project has unique characteristics in terms of APIs,\\nperformance, and the level of integration with Apache Spark’s data source APIs. We\\nwill explore them next. Note that all of these projects are evolving fast, and therefore\\nsome of the descriptions may be outdated at the time you are reading them. Refer to\\nthe online documentation for each project for the most up-to-date information.\\nApache Hudi\\nInitially built by Uber Engineering , Apache Hudi —an acronym for Hadoop Update\\nDelete and Incremental—is a data storage format that is designed for incremental\\nupserts and deletes over key/value-style data. The data is stored as a combination of\\ncolumnar formats (e.g., Parquet files) and row-based formats (e.g., Avro files for\\nrecording incremental changes over Parquet files). Besides the common features\\nmentioned earlier, it supports:\\n•Upserting with fast, pluggable indexing\\n•Atomic publishing of data with rollback support\\n•Reading incremental changes to a table\\n•Savepoints for data recovery\\n•File size and layout management using statistics\\n•Async compaction of row and columnar data\\nApache Iceberg\\nOriginally built at Netflix , Apache Iceberg  is another open storage format for huge\\ndata sets. However, unlike Hudi, which focuses on upserting key/value data, Iceberg\\nfocuses more on general-purpose data storage that scales to petabytes in a single table\\nand has schema evolution properties. Specifically, it provides the following additional\\nfeatures (besides the common ones):\\n272 | Chapter 9: Building Reliable Data Lakes with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 295}), Document(page_content='•Schema evolution by adding, dropping, updating, renaming, and reordering of\\ncolumns, fields, and/or nested structures\\n•Hidden partitioning, which under the covers creates the partition values for rows\\nin a table\\n•Partition evolution, where it automatically performs a metadata operation to\\nupdate the table layout as data volume or query patterns change\\n•Time travel, which allows you to query a specific table snapshot by ID or by\\ntimestamp\\n•Rollback to previous versions to correct errors\\n•Serializable isolation, even between multiple concurrent writers\\nDelta Lake\\nDelta Lake  is an open source project hosted by the Linux Foundation, built by the\\noriginal creators of Apache Spark. Similar to the others, it is an open data storage for‐\\nmat that provides transactional guarantees and enables schema enforcement and evo‐\\nlution. It also provides several other interesting features, some of which are unique.\\nDelta Lake supports:\\n•Streaming reading from and writing to tables using Structured Streaming sources\\nand sinks\\n•Update, delete, and merge (for upserts) operations, even in Java, Scala, and\\nPython APIs\\n•Schema evolution either by explicitly altering the table schema or by implicitly\\nmerging a DataFrame’s schema to the table’s during the DataFrame’s write. (In\\nfact, the merge operation in Delta Lake supports advanced syntax for conditional\\nupdates/inserts/deletes, updating all columns together, etc., as you’ll see later in\\nthe chapter.)\\n•Time travel, which allows you to query a specific table snapshot by ID or by\\ntimestamp\\n•Rollback to previous versions to correct errors\\n•Serializable isolation between multiple concurrent writers performing any SQL,\\nbatch, or streaming operations\\nIn the rest of this chapter, we are going to explore how such a system, along with\\nApache Spark, can be used to build a lakehouse that provides the aforementioned\\nproperties. Of these three systems, so far Delta Lake has the tightest integration with\\nApache Spark data sources (both for batch and streaming workloads) and SQL\\noperations  (e.g., MERGE ). Hence, we will use Delta Lake as the vehicle for further\\nexploration.\\nLakehouses: The Next Step in the Evolution of Storage Solutions | 273', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 296}), Document(page_content='1A full view of the data is available at this Excel file .\\nThis project is called Delta Lake because of its analogy to stream‐\\ning. Streams flow into the sea to create deltas—this is where all of\\nthe sediments accumulate, and thus where the valuable crops are\\ngrown. Jules S. Damji (one of our coauthors) came up with this!\\nBuilding Lakehouses with Apache Spark and Delta Lake\\nIn this section, we are going to take a quick look at how Delta Lake and Apache Spark\\ncan be used to build lakehouses. Specifically, we will explore the following:\\n•Reading and writing Delta Lake tables using Apache Spark\\n•How Delta Lake allows concurrent batch and streaming writes with ACID\\nguarantees\\n•How Delta Lake ensures better data quality by enforcing schema on all writes,\\nwhile allowing for explicit schema evolution\\n•Building complex data pipelines using update, delete, and merge operations, all\\nof which ensure ACID guarantees\\n•Auditing the history of operations that modified a Delta Lake table and traveling\\nback in time by querying earlier versions of the table\\nThe data we will use in this section is a modified version (a subset of columns in Par‐\\nquet format) of the public Lending Club Loan Data .1 It includes all funded loans from\\n2012 to 2017. Each loan record includes applicant information provided by the appli‐\\ncant as well as the current loan status (current, late, fully paid, etc.) and latest pay‐\\nment information.\\nConfiguring  Apache Spark with Delta Lake\\nY ou can configure Apache Spark to link to the Delta Lake library in one of the follow‐\\ning ways:\\nSet up an interactive shell\\nIf you’re using Apache Spark 3.0, you can start a PySpark or Scala shell with Delta\\nLake by using the following command-line argument:\\n--packages  io.delta:delta-core_2.12:0.7.0\\nFor example:\\npyspark --packages  io.delta:delta-core_2.12:0.7.0\\nIf you are running Spark 2.4, you have to use Delta Lake 0.6.0.\\n274 | Chapter 9: Building Reliable Data Lakes with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 297}), Document(page_content='Set up a standalone Scala/Java project using Maven coordinates\\nIf you want to build a project using Delta Lake binaries from the Maven Central\\nrepository, you can add the following Maven coordinates to the project\\ndependencies:\\n  <dependency >\\n  <groupId>io.delta</groupId>\\n  <artifactId >delta-core_2.12</artifactId >\\n  <version>0.7.0</version>\\n</dependency >\\nAgain, if you are running Spark 2.4 you have to use Delta Lake 0.6.0.\\nSee the Delta Lake documentation  for the most up-to-date infor‐\\nmation.\\nLoading Data into a Delta Lake Table\\nIf you are used to building data lakes with Apache Spark and any of the structured\\ndata formats—say, Parquet—then it is very easy to migrate existing workloads to use\\nthe Delta Lake format. All you have to do is change all the DataFrame read and write\\noperations to use format(\"delta\")  instead of format(\"parquet\") . Let’s try this out\\nwith some of the aforementioned loan data, which is available as a Parquet file . First\\nlet’s read this data and save it as a Delta Lake table:\\n// In Scala\\n// Configure source data path\\nval sourcePath  = \"/databricks-datasets/learning-spark-v2/loans/\\n  loan-risks.snappy.parquet\"\\n// Configure Delta Lake path\\nval deltaPath  = \"/tmp/loans_delta\"\\n// Create the Delta table with the same loans data\\nspark\\n  .read\\n  .format(\"parquet\" )\\n  .load(sourcePath )\\n  .write\\n  .format(\"delta\")\\n  .save(deltaPath )\\n// Create a view on the data called loans_delta\\nspark\\n .read\\n .format(\"delta\")\\nBuilding Lakehouses with Apache Spark and Delta Lake | 275', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 298}), Document(page_content=' .load(deltaPath )\\n .createOrReplaceTempView (\"loans_delta\" )\\n# In Python\\n# Configure source data path\\nsourcePath  = \"/databricks-datasets/learning-spark-v2/loans/\\n  loan-risks.snappy.parquet\"\\n# Configure Delta Lake path\\ndeltaPath  = \"/tmp/loans_delta\"\\n# Create the Delta Lake table with the same loans data\\n(spark.read.format(\"parquet\" ).load(sourcePath ) \\n  .write.format(\"delta\").save(deltaPath ))\\n# Create a view on the data called loans_delta\\nspark.read.format(\"delta\").load(deltaPath ).createOrReplaceTempView (\"loans_delta\" )\\nNow we can read and explore the data as easily as any other table:\\n// In Scala/Python\\n// Loans row count\\nspark.sql(\"SELECT count(*) FROM loans_delta\" ).show()\\n+--------+\\n|count(1)|\\n+--------+\\n|   14705|\\n+--------+\\n// First 5 rows of loans table\\nspark.sql(\"SELECT * FROM loans_delta LIMIT 5\" ).show()\\n+-------+-----------+---------+----------+\\n|loan_id|funded_amnt |paid_amnt |addr_state |\\n+-------+-----------+---------+----------+\\n|      0|       1000|   182.22|        CA|\\n|      1|       1000|   361.19|        WA|\\n|      2|       1000|   176.26|        TX|\\n|      3|       1000|   1000.0|        OK|\\n|      4|       1000|   249.98|        PA|\\n+-------+-----------+---------+----------+\\n276 | Chapter 9: Building Reliable Data Lakes with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 299}), Document(page_content='Loading Data Streams into a Delta Lake Table\\nAs with static DataFrames, you can easily modify your existing Structured Streaming\\njobs to write to and read from a Delta Lake table by setting the format to \"delta\" . Say\\nyou have a stream of new loan data as a DataFrame named newLoanStreamDF , which\\nhas the same schema as the table. Y ou can append to the table as follows:\\n// In Scala\\nimport org.apache.spark.sql.streaming._\\nval newLoanStreamDF  = ...   // Streaming DataFrame with new loans data\\nval checkpointDir  = ...     // Directory for streaming checkpoints\\nval streamingQuery  = newLoanStreamDF .writeStream\\n  .format(\"delta\")\\n  .option(\"checkpointLocation\" , checkpointDir )\\n  .trigger(Trigger.ProcessingTime (\"10 seconds\" ))\\n  .start(deltaPath )\\n# In Python\\nnewLoanStreamDF  = ...   # Streaming DataFrame with new loans data\\ncheckpointDir  = ...     # Directory for streaming checkpoints\\nstreamingQuery  = (newLoanStreamDF .writeStream  \\n    .format(\"delta\") \\n    .option(\"checkpointLocation\" , checkpointDir ) \\n    .trigger(processingTime  = \"10 seconds\" ) \\n    .start(deltaPath ))\\nWith this format, just like any other, Structured Streaming offers end-to-end exactly-\\nonce guarantees. However, Delta Lake has a few additional advantages over tradi‐\\ntional formats like JSON, Parquet, or ORC:\\nIt allows writes from both batch and streaming jobs into the same table\\nWith other formats, data written into a table from a Structured Streaming job\\nwill overwrite any existing data in the table. This is because the metadata main‐\\ntained in the table to ensure exactly-once guarantees for streaming writes does\\nnot account for other nonstreaming writes. Delta Lake’s advanced metadata man‐\\nagement allows both batch and streaming data to be written.\\nIt allows multiple streaming jobs to append data to the same table\\nThe same limitation of metadata with other formats also prevents multiple Struc‐\\ntured Streaming queries from appending to the same table. Delta Lake’s metadata\\nmaintains transaction information for each streaming query, thus enabling any\\nnumber of streaming queries to concurrently write into a table with exactly-once\\nguarantees.\\nIt provides ACID guarantees even under concurrent writes\\nUnlike built-in formats, Delta Lake allows concurrent batch and streaming oper‐\\nations to write data with ACID guarantees.\\nBuilding Lakehouses with Apache Spark and Delta Lake | 277', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 300}), Document(page_content='Enforcing Schema on Write to Prevent Data Corruption\\nA common problem with managing data with Spark using common formats like\\nJSON, Parquet, and ORC is accidental data corruption caused by writing incorrectly\\nformatted data. Since these formats define the data layout of individual files and not\\nof the entire table, there is no mechanism to prevent any Spark job from writing files\\nwith different schemas into existing tables. This means there are no guarantees of\\nconsistency for the entire table of many Parquet files.\\nThe Delta Lake format records the schema as table-level metadata. Hence, all writes\\nto a Delta Lake table can verify whether the data being written has a schema compati‐\\nble with that of the table. If it is not compatible, Spark will throw an error before any\\ndata is written and committed to the table, thus preventing such accidental data cor‐\\nruption. Let’s test this by trying to write some data with an additional column,\\nclosed , that signifies whether the loan has been terminated. Note that this column\\ndoes not exist in the table:\\n// In Scala\\nval loanUpdates  = Seq(\\n    (1111111L , 1000, 1000.0, \"TX\", false), \\n    (2222222L , 2000, 0.0, \"CA\", true))\\n  .toDF(\"loan_id\" , \"funded_amnt\" , \"paid_amnt\" , \"addr_state\" , \"closed\" )\\n  \\nloanUpdates .write.format(\"delta\").mode(\"append\" ).save(deltaPath )\\n# In Python\\nfrom pyspark.sql.functions  import *\\ncols = [\\'loan_id\\' , \\'funded_amnt\\' , \\'paid_amnt\\' , \\'addr_state\\' , \\'closed\\' ]\\nitems = [\\n(1111111, 1000, 1000.0, \\'TX\\', True), \\n(2222222, 2000, 0.0, \\'CA\\', False)\\n]\\nloanUpdates  = (spark.createDataFrame (items, cols)\\n  .withColumn (\"funded_amnt\" , col(\"funded_amnt\" ).cast(\"int\")))\\nloanUpdates .write.format(\"delta\").mode(\"append\" ).save(deltaPath )\\nThis write will fail with the following error message:\\norg.apache.spark.sql.AnalysisException : A schema mismatch  detected  when writing \\n  to the Delta table (Table ID: 48bfa949 -5a09-49ce-96cb-34090ab7d695 ).\\nTo enable schema migration , please set:\\n\\'.option(\"mergeSchema\", \"true\")\\' .\\nTable schema:\\nroot\\n-- loan_id: long (nullable  = true)\\n-- funded_amnt : integer (nullable  = true)\\n-- paid_amnt : double (nullable  = true)\\n-- addr_state : string (nullable  = true)\\n278 | Chapter 9: Building Reliable Data Lakes with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 301}), Document(page_content='Data schema:\\nroot\\n-- loan_id: long (nullable  = true)\\n-- funded_amnt : integer (nullable  = true)\\n-- paid_amnt : double (nullable  = true)\\n-- addr_state : string (nullable  = true)\\n-- closed: boolean (nullable  = true)\\nThis illustrates how Delta Lake blocks writes that do not match the schema of the\\ntable. However, it also gives a hint about how to actually evolve the schema of the\\ntable using the option mergeSchema , as discussed next.\\nEvolving Schemas to Accommodate Changing Data\\nIn our world of ever-changing data, it is possible that we might want to add this new\\ncolumn to the table. This new column can be explicitly added by setting the option\\n\"mergeSchema\"  to \"true\" :\\n// In Scala\\nloanUpdates .write.format(\"delta\").mode(\"append\" )\\n  .option(\"mergeSchema\" , \"true\")\\n  .save(deltaPath )\\n# In Python\\n(loanUpdates .write.format(\"delta\").mode(\"append\" )\\n  .option(\"mergeSchema\" , \"true\")\\n  .save(deltaPath ))\\nWith this, the column closed  will be added to the table schema, and new data will be\\nappended. When existing rows are read, the value of the new column is considered as\\nNULL . In Spark 3.0, you can also use the SQL DDL command ALTER TABLE  to add and\\nmodify columns.\\nTransforming Existing Data\\nDelta Lake supports the DML commands UPDATE , DELETE , and MERGE , which allow\\nyou to build complex data pipelines. These commands can be invoked using Java,\\nScala, Python, and SQL, giving users the flexibility of using the commands with any\\nAPIs they are familiar with, using either DataFrames or tables. Furthermore, each of\\nthese data modification operations ensures ACID guarantees.\\nLet’s explore this with a few examples of real-world use cases.\\nBuilding Lakehouses with Apache Spark and Delta Lake | 279', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 302}), Document(page_content='Updating data to fix errors\\nA common use case when managing data is fixing errors in the data. Suppose, upon\\nreviewing the data, we realized that all of the loans assigned to addr_state = \\'OR\\'\\nshould have been assigned to addr_state = \\'WA\\' . If the loan table were a Parquet\\ntable, then to do such an update we would need to:\\n1.Copy all of the rows that are not affected into a new table.\\n2.Copy all of the rows that are affected into a DataFrame, then perform the data\\nmodification.\\n3.Insert the previously noted DataFrame’s rows into the new table.\\n4.Remove the old table and rename the new table to the old table name.\\nIn Spark 3.0, which added direct support for DML SQL operations like UPDATE ,\\nDELETE , and MERGE , instead of manually performing all these steps you can simply run\\nthe SQL UPDATE  command. However, with a Delta Lake table, users can run this oper‐\\nation too, by using Delta Lake’s programmatic APIs as follows:\\n// In Scala\\nimport io.delta.tables.DeltaTable\\nimport org.apache.spark.sql.functions._\\nval deltaTable  = DeltaTable .forPath(spark, deltaPath )\\ndeltaTable .update(\\n  col(\"addr_state\" ) === \"OR\",\\n  Map(\"addr_state\"  -> lit(\"WA\")))\\n# In Python\\nfrom delta.tables  import *\\ndeltaTable  = DeltaTable .forPath(spark, deltaPath )\\ndeltaTable .update(\"addr_state = \\'OR\\'\" ,  {\"addr_state\" : \"\\'WA\\'\"})\\nDeleting user-related data\\nWith data protection policies like the EU’s General Data Protection Regulation\\n(GDPR)  coming into force, it is more important now than ever to be able to delete\\nuser data from all your tables. Say it is mandated that you have to delete the data on\\nall loans that have been fully paid off. With Delta Lake, you can do the following:\\n// In Scala\\nval deltaTable  = DeltaTable .forPath(spark, deltaPath )\\ndeltaTable .delete(\"funded_amnt >= paid_amnt\" )\\n# In Python\\ndeltaTable  = DeltaTable .forPath(spark, deltaPath )\\ndeltaTable .delete(\"funded_amnt >= paid_amnt\" )\\n280 | Chapter 9: Building Reliable Data Lakes with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 303}), Document(page_content='Similar to updates, with Delta Lake and Apache Spark 3.0 you can directly run the\\nDELETE  SQL command on the table.\\nUpserting change data to a table using merge()\\nA common use case is change data capture, where you have to replicate row changes\\nmade in an OLTP table to another table for OLAP workloads. To continue with our\\nloan data example, say we have another table of new loan information, some of which\\nare new loans and others of which are updates to existing loans. In addition, let’s say\\nthis changes  table has the same schema as the loan_delta  table. Y ou can upsert these\\nchanges into the table using the DeltaTable.merge()  operation, which is based on\\nthe MERGE  SQL command:\\n// In Scala\\ndeltaTable\\n  .alias(\"t\")\\n  .merge(loanUpdates .alias(\"s\"), \"t.loan_id = s.loan_id\" )\\n  .whenMatched .updateAll ()\\n  .whenNotMatched .insertAll ()\\n  .execute()\\n# In Python\\n(deltaTable\\n  .alias(\"t\")\\n  .merge(loanUpdates .alias(\"s\"), \"t.loan_id = s.loan_id\" ) \\n  .whenMatchedUpdateAll () \\n  .whenNotMatchedInsertAll () \\n  .execute())\\nAs a reminder, you can run this as a SQL MERGE  command starting with Spark 3.0.\\nFurthermore, if you have a stream of such captured changes, you can continuously\\napply those changes using a Structured Streaming query. The query can read the\\nchanges in micro-batches (see Chapter 8 ) from any streaming source, and use fore\\nachBatch()  to apply the changes in each micro-batch to the Delta Lake table.\\nDeduplicating data while inserting using insert-only merge\\nThe merge operation in Delta Lake supports an extended syntax beyond that speci‐\\nfied by the ANSI standard, including advanced features like the following:\\nDelete actions\\nFor example, MERGE ... WHEN MATCHED THEN DELETE .\\nClause conditions\\nFor example, MERGE ... WHEN MATCHED AND <condition>  THEN ... .\\nOptional actions\\nAll the MATCHED  and NOT MATCHED  clauses are optional.\\nBuilding Lakehouses with Apache Spark and Delta Lake | 281', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 304}), Document(page_content='Star syntax\\nFor example, UPDATE *  and INSERT *  to update/insert all the columns in the tar‐\\nget table with matching columns from the source data set. The equivalent Delta\\nLake APIs are updateAll()  and insertAll() , which we saw in the previous\\nsection.\\nThis allows you to express many more complex use cases with little code. For exam‐\\nple, say you want to backfill the loan_delta  table with historical data on past loans.\\nBut some of the historical data may already have been inserted in the table, and you\\ndon’t want to update those records because they may contain more up-to-date infor‐\\nmation. Y ou can deduplicate by the loan_id  while inserting by running the following\\nmerge operation with only the INSERT  action (since the UPDATE  action is optional):\\n// In Scala\\ndeltaTable\\n  .alias(\"t\")\\n  .merge(historicalUpdates .alias(\"s\"), \"t.loan_id = s.loan_id\" )\\n  .whenNotMatched .insertAll ()\\n  .execute()\\n# In Python\\n(deltaTable\\n  .alias(\"t\")\\n  .merge(historicalUpdates .alias(\"s\"), \"t.loan_id = s.loan_id\" ) \\n  .whenNotMatchedInsertAll () \\n  .execute())\\nThere are even more complex use cases, like CDC with deletes and SCD tables, that\\nare made simple with the extended merge syntax. Refer to the documentation  for\\nmore details and examples.\\nAuditing Data Changes with Operation History\\nAll of the changes to your Delta Lake table are recorded as commits in the table’s\\ntransaction log. As you write into a Delta Lake table or directory, every operation is\\nautomatically versioned. Y ou can query the table’s operation history as noted in the\\nfollowing code snippet:\\n// In Scala/Python\\ndeltaTable .history().show()\\nBy default this will show a huge table with many versions and a lot of columns. Let’s\\ninstead print some of the key columns of the last three operations:\\n// In Scala\\ndeltaTable\\n  .history(3)\\n  .select(\"version\" , \"timestamp\" , \"operation\" , \"operationParameters\" )\\n  .show(false)\\n282 | Chapter 9: Building Reliable Data Lakes with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 305}), Document(page_content='# In Python\\n(deltaTable\\n  .history(3)\\n  .select(\"version\" , \"timestamp\" , \"operation\" , \"operationParameters\" )\\n  .show(truncate =False))\\nThis will generate the following output:\\n+-------+-----------+---------+-------------------------------------------+\\n|version|timestamp  |operation|operationParameters                        |\\n+-------+-----------+---------+-------------------------------------------+\\n|5      |2020-04-07 |MERGE    |[predicate -> (t.`loan_id` = s.`loan_id`)] |\\n|4      |2020-04-07 |MERGE    |[predicate -> (t.`loan_id` = s.`loan_id`)] |\\n|3      |2020-04-07 |DELETE   |[predicate -> [\"(CAST(`funded_amnt` ...    |\\n+-------+-----------+---------+-------------------------------------------+\\nNote the operation  and operationParameters  that are useful for auditing the\\nchanges.\\nQuerying Previous Snapshots of a Table with Time Travel\\nY ou can query previous versioned snapshots of a table by using the DataFrameReader\\noptions \"versionAsOf\"  and \"timestampAsOf\" . Here are a few examples:\\n// In Scala\\nspark.read\\n  .format(\"delta\")\\n  .option(\"timestampAsOf\" , \"2020-01-01\" )  // timestamp after table creation\\n  .load(deltaPath )\\nspark.read.format(\"delta\")\\n  .option(\"versionAsOf\" , \"4\")\\n  .load(deltaPath )\\n# In Python\\n(spark.read\\n  .format(\"delta\")\\n  .option(\"timestampAsOf\" , \"2020-01-01\" )  # timestamp after table creation\\n  .load(deltaPath ))\\n(spark.read.format(\"delta\")\\n  .option(\"versionAsOf\" , \"4\")\\n  .load(deltaPath ))\\nThis is useful in a variety of situations, such as:\\n•Reproducing machine learning experiments and reports by rerunning the job on\\na specific table version\\n•Comparing the data changes between different versions for auditing\\n•Rolling back incorrect changes by reading a previous snapshot as a DataFrame\\nand overwriting the table with it\\nBuilding Lakehouses with Apache Spark and Delta Lake | 283', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 306}), Document(page_content='Summary\\nThis chapter examined the possibilities for building reliable data lakes using Apache\\nSpark. To recap, databases have solved data problems for a long time, but they fail to\\nfulfill the diverse requirements of modern use cases and workloads. Data lakes were\\nbuilt to alleviate some of the limitations of databases, and Apache Spark is one of the\\nbest tools to build them with. However, data lakes still lack some of the key features\\nprovided by databases (e.g., ACID guarantees). Lakehouses are the next generation of\\ndata solutions, which aim to provide the best features of databases and data lakes and\\nmeet all the requirements of diverse use cases and workloads.\\nWe briefly explored a couple of open source systems (Apache Hudi and Apache Ice‐\\nberg) that can be used to build lakehouses, then took a closer look at Delta Lake, a\\nfile-based open source storage format that, along with Apache Spark, is a great build‐\\ning block for lakehouses. As you saw, it provides the following:\\n•Transactional guarantees and schema management, like databases\\n•Scalability and openness, like data lakes\\n•Support for concurrent batch and streaming workloads with ACID guarantees\\n•Support for transformation of existing data using update, delete, and merge oper‐\\nations that ensure ACID guarantees\\n•Support for versioning, auditing of operation history, and querying of previous\\nversions\\nIn the next chapter, we’ll explore how to begin building ML models using Spark’s\\nMLlib.\\n284 | Chapter 9: Building Reliable Data Lakes with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 307}), Document(page_content='CHAPTER 10\\nMachine Learning with MLlib\\nUp until this point, we have focused on data engineering workloads with Apache\\nSpark. Data engineering is often a precursory step to preparing your data for machine\\nlearning (ML) tasks, which will be the focus of this chapter. We live in an era in which\\nmachine learning and artificial intelligence applications are an integral part of our\\nlives. Chances are that whether we realize it or not, every day we come into contact\\nwith ML models for purposes such as online shopping recommendations and adver‐\\ntisements, fraud detection, classification, image recognition, pattern matching, and\\nmore. These ML models drive important business decisions for many companies.\\nAccording to this McKinsey study , 35% of what consumers purchase on Amazon and\\n75% of what they watch on Netflix is driven by machine learning–based product rec‐\\nommendations. Building a model that performs well can make or break companies.\\nIn this chapter we will get you started building ML models using MLlib , the de facto\\nmachine learning library in Apache Spark. We’ll begin with a brief introduction to\\nmachine learning, then cover best practices for distributed ML and feature engineer‐\\ning at scale (if you’re already familiar with machine learning fundamentals, you can\\nskip straight to “Designing Machine Learning Pipelines” on page 289). Through the\\nshort code snippets presented here and the notebooks available in the book’s GitHub\\nrepo , you’ll learn how to build basic ML models and use MLlib.\\nThis chapter covers the Scala and Python APIs; if you’re interested\\nin using R ( sparklyr ) with Spark for machine learning, we invite\\nyou to check out Mastering Spark with R  by Javier Luraschi, Kevin\\nKuo, and Edgar Ruiz (O’Reilly).\\n285', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 308}), Document(page_content='What Is Machine Learning?\\nMachine learning is getting a lot of hype these days—but what is it, exactly? Broadly\\nspeaking, machine learning is a process for extracting patterns from your data, using\\nstatistics, linear algebra, and numerical optimization. Machine learning can be\\napplied to problems such as predicting power consumption, determining whether or\\nnot there is a cat in your video, or clustering items with similar characteristics.\\nThere are a few types of machine learning, including supervised, semi-supervised,\\nunsupervised, and reinforcement learning. This chapter will mainly focus on super‐\\nvised machine learning and just touch upon unsupervised learning. Before we dive\\nin, let’s briefly discuss the differences between supervised and unsupervised ML.\\nSupervised Learning\\nIn supervised machine learning , your data consists of a set of input records, each of\\nwhich has associated labels, and the goal is to predict the output label(s) given a new\\nunlabeled input. These output labels can either be discrete  or continuous , which\\nbrings us to the two types of supervised machine learning: classification  and regres‐\\nsion.\\nIn a classification problem, the aim is to separate the inputs into a discrete set of\\nclasses or labels. With binary  classification, there are two discrete labels you want to\\npredict, such as “dog” or “not dog, ” as Figure 10-1  depicts.\\nFigure 10-1. Binary classification  example: dog or not dog\\nWith multiclass , also known as multinomial , classification, there can be three or more\\ndiscrete labels, such as predicting the breed of a dog (e.g., Australian shepherd,\\ngolden retriever, or poodle, as shown in Figure 10-2 ).\\n286 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 309}), Document(page_content='Figure 10-2. Multinomial classification  example: Australian shepherd, golden retriever,\\nor poodle\\nIn regression problems, the value to predict is a continuous number, not a label. This\\nmeans you might predict values that your model hasn’t seen during training, as\\nshown in Figure 10-3 . For example, you might build a model to predict the daily ice\\ncream sales given the temperature. Y our model might predict the value $77.67, even if\\nnone of the input/output pairs it was trained on contained that value.\\nFigure 10-3. Regression example: predicting ice cream sales based on temperature\\nTable 10-1  lists some commonly used supervised ML algorithms that are available in\\nSpark MLlib , with a note as to whether they can be used for regression, classification,\\nor both.\\nWhat Is Machine Learning? | 287', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 310}), Document(page_content='Table 10-1. Popular classification  and regression algorithms\\nAlgorithm Typical usage\\nLinear regression Regression\\nLogistic regression Classification  (we know, it has regression in the name!)\\nDecision trees Both\\nGradient boosted trees Both\\nRandom forests Both\\nNaive Bayes Classification\\nSupport vector machines (SVMs) Classification\\nUnsupervised Learning\\nObtaining the labeled data required by supervised machine learning can be very\\nexpensive and/or infeasible. This is where unsupervised machine learning  comes into\\nplay. Instead of predicting a label, unsupervised ML helps you to better understand\\nthe structure of your data.\\nAs an example, consider the original unclustered data on the left in Figure 10-4 .\\nThere is no known true label for each of these data points ( x1, x2), but by applying\\nunsupervised machine learning to our data we can find the clusters that naturally\\nform, as shown on the right.\\nFigure 10-4. Clustering example\\nUnsupervised machine learning can be used for outlier detection or as a preprocess‐\\ning step for supervised machine learning—for example, to reduce the dimensionality\\n288 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 311}), Document(page_content='(i.e., number of dimensions per datum) of the data set, which is useful for reducing\\nstorage requirements or simplifying downstream tasks. Some unsupervised machine\\nlearning algorithms in MLlib  include k-means, Latent Dirichlet Allocation (LDA),\\nand Gaussian mixture models.\\nWhy Spark for Machine Learning?\\nSpark is a unified analytics engine that provides an ecosystem for data ingestion, fea‐\\nture engineering, model training, and deployment. Without Spark, developers would\\nneed many disparate tools to accomplish this set of tasks, and might still struggle with\\nscalability.\\nSpark has two machine learning packages: spark.mllib  and spark.ml . spark.mllib\\nis the original machine learning API, based on the RDD API (which has been in\\nmaintenance mode since Spark 2.0), while spark.ml  is the newer API, based on Data‐\\nFrames. The rest of this chapter will focus on using the spark.ml  package and how to\\ndesign machine learning pipelines in Spark. However, we use “MLlib” as an umbrella\\nterm to refer to both machine learning library packages in Apache Spark.\\nWith spark.ml , data scientists can use one ecosystem for their data preparation and\\nmodel building, without the need to downsample their data to fit on a single\\nmachine. spark.ml  focuses on O( n) scale-out, where the model scales linearly with\\nthe number of data points you have, so it can scale to massive amounts of data. In the\\nfollowing chapter, we will discuss some of the trade-offs involved in choosing\\nbetween a distributed framework such as spark.ml  and a single-node framework like\\nscikit-learn  (sklearn ). If you have previously used scikit-learn , many of the\\nAPIs in spark.ml  will feel quite familiar, but there are some subtle differences that we\\nwill discuss.\\nDesigning Machine Learning Pipelines\\nIn this section, we will cover how to create and tune ML pipelines. The concept of\\npipelines is common across many ML frameworks as a way to organize a series of\\noperations to apply to your data. In MLlib, the Pipeline API  provides a high-level API\\nbuilt on top of DataFrames to organize your machine learning workflow. The Pipe‐\\nline API is composed of a series of transformers and estimators, which we will discuss\\nin-depth later.\\nThroughout this chapter, we will use the San Francisco housing data set from Inside\\nAirbnb . It contains information about Airbnb rentals in San Francisco, such as the\\nnumber of bedrooms, location, review scores, etc., and our goal is to build a model to\\npredict the nightly rental prices for listings in that city. This is a regression problem,\\nbecause price is a continuous variable. We will guide you through the workflow a data\\nscientist would go through to approach this problem, including feature engineering,\\nDesigning Machine Learning Pipelines | 289', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 312}), Document(page_content='building models, hyperparameter tuning, and evaluating model performance. This\\ndata set is quite messy and can be difficult to model (like most real-world data sets!),\\nso if you are experimenting on your own, don’t feel bad if your early models aren’t\\ngreat.\\nThe intent of this chapter is not to show you every API in MLlib, but rather to equip\\nyou with the skills and knowledge to get started with using MLlib to build end-to-end\\npipelines. Before going into the details, let’s define some MLlib terminology:\\nTransformer\\nAccepts a DataFrame as input, and returns a new DataFrame with one or more\\ncolumns appended to it. Transformers do not learn any parameters from your\\ndata and simply apply rule-based transformations to either prepare data for\\nmodel training or generate predictions using a trained MLlib model. They have\\na .transform()  method.\\nEstimator\\nLearns (or “fits”) parameters from your DataFrame via a .fit()  method and\\nreturns a Model , which is a transformer.\\nPipeline\\nOrganizes a series of transformers and estimators into a single model. While\\npipelines themselves are estimators, the output of pipeline.fit()  returns a Pipe\\nlineModel , a transformer.\\nWhile these concepts may seem rather abstract right now, the code snippets and\\nexamples in this chapter will help you understand how they all come together. But\\nbefore we can build our ML models and use transformers, estimators, and pipelines,\\nwe need to load in our data and perform some data preparation.\\nData Ingestion and Exploration\\nWe have slightly preprocessed the data in our example data set to remove outliers\\n(e.g., Airbnbs posted for $0/night), converted all integers to doubles, and selected an\\ninformative subset of the more than one hundred fields. Further, for any missing\\nnumerical values in our data columns, we have imputed the median value and added\\nan indicator column (the column name followed by _na, such as bedrooms_na ). This\\nway the ML model or human analyst can interpret any value in that column as an\\nimputed value, not a true value. Y ou can see the data preparation notebook in the\\nbook’s GitHub repo . Note there are many other ways to handle missing values, which\\nare outside the scope of this book.\\nLet’s take a quick peek at the data set and the corresponding schema (with the output\\nshowing just a subset of the columns):\\n290 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 313}), Document(page_content='# In Python\\nfilePath  = \"\"\"/databricks-datasets/learning-spark-v2/sf-airbnb/\\nsf-airbnb-clean.parquet/\"\"\"\\nairbnbDF  = spark.read.parquet(filePath )\\nairbnbDF .select(\"neighbourhood_cleansed\" , \"room_type\" , \"bedrooms\" , \"bathrooms\" , \\n                \"number_of_reviews\" , \"price\").show(5)\\n// In Scala\\nval filePath  = \\n  \"/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet/\"\\nval airbnbDF  = spark.read.parquet(filePath )\\nairbnbDF .select(\"neighbourhood_cleansed\" , \"room_type\" , \"bedrooms\" , \"bathrooms\" , \\n                \"number_of_reviews\" , \"price\").show(5)\\n+----------------------+---------------+--------+---------+----------+-----+\\n|neighbourhood_cleansed |      room_type |bedrooms |bathrooms |number_... |price|\\n+----------------------+---------------+--------+---------+----------+-----+\\n|      Western Addition |Entire home/apt|     1.0|      1.0|     180.0|170.0|\\n|        Bernal Heights|Entire home/apt|     2.0|      1.0|     111.0|235.0|\\n|        Haight Ashbury|   Private room|     1.0|      4.0|      17.0| 65.0|\\n|        Haight Ashbury|   Private room|     1.0|      4.0|       8.0| 65.0|\\n|      Western Addition |Entire home/apt|     2.0|      1.5|      27.0|785.0|\\n+----------------------+---------------+--------+---------+----------+-----+\\nOur goal is to predict the price per night for a rental property, given our features.\\nBefore data scientists can get to model building, they need to\\nexplore and understand their data. They will often use Spark to\\ngroup their data, then use data visualization libraries such as mat‐\\nplotlib  to visualize the data. We will leave data exploration as an\\nexercise for the reader.\\nCreating Training and Test Data Sets\\nBefore we begin feature engineering and modeling, we will divide our data set into\\ntwo groups: train  and test. Depending on the size of your data set, your train/test ratio\\nmay vary, but many data scientists use 80/20 as a standard train/test split. Y ou might\\nbe wondering, “Why not use the entire data set to train the model?” The problem is\\nthat if we built a model on the entire data set, it’s possible that the model would mem‐\\norize or “overfit” to the training data we provided, and we would have no more data\\nwith which to evaluate how well it generalizes to previously unseen data. The model’s\\nperformance on the test set is a proxy for how well it will perform on unseen data\\n(i.e., in the wild or in production), assuming that data follows similar distributions.\\nThis split is depicted in Figure 10-5 .\\nDesigning Machine Learning Pipelines | 291', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 314}), Document(page_content='Figure 10-5. Train/test split\\nOur training set consists of a set of features, X, and a label, y. Here we use capital X to\\ndenote a matrix with dimensions n x d, where n is the number of data points (or\\nexamples) and d is the number of features (this is what we call the fields or columns\\nin our DataFrame). We use lowercase y to denote a vector, with dimensions n x 1; for\\nevery example, there is one label.\\nDifferent metrics are used to measure the performance of the model. For classifica‐\\ntion problems, a standard metric is the accuracy , or percentage, of correct predic‐\\ntions. Once the model has satisfactory performance on the training set using that\\nmetric, we will apply the model to our test set. If it performs well on our test set\\naccording to our evaluation metrics, then we can feel confident that we have built a\\nmodel that will “generalize” to unseen data.\\nFor our Airbnb data set, we will keep 80% for the training set and set aside 20% of our\\ndata for the test set. Further, we will set a random seed for reproducibility, such that if\\nwe rerun this code we will get the same data points going to our train and test data\\nsets, respectively. The value of the seed itself shouldn’t  matter, but data scientists often\\nlike setting it to 42 as that is the answer to the Ultimate Question of Life :\\n# In Python\\ntrainDF, testDF = airbnbDF .randomSplit ([.8, .2], seed=42)\\nprint(f\"\"\"There are {trainDF.count()} rows in the training set, \\nand {testDF.count()} in the test set\"\"\" )\\n// In Scala\\nval Array(trainDF, testDF) = airbnbDF .randomSplit (Array(.8, .2), seed=42)\\nprintln(f\"\"\"There are ${trainDF.count} rows in the training set, and \\n${testDF.count} in the test set\"\"\" )\\nThis produces the following output:\\nThere are 5780 rows in the training set, and 1366 in the test set\\n292 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 315}), Document(page_content='But what happens if we change the number of executors in our Spark cluster? The\\nCatalyst optimizer determines the optimal way to partition your data  as a function of\\nyour cluster resources and size of your data set. Given that data in a Spark DataFrame\\nis row-partitioned and each worker performs its split independently of the other\\nworkers, if the data in the partitions changes, then the result of the split (by random\\nSplit() ) won’t be the same.\\nWhile you could fix your cluster configuration and your seed to ensure that you get\\nconsistent results, our recommendation is to split your data once, then write it out to\\nits own train/test folder so you don’t have these reproducibility issues.\\nDuring your exploratory analysis, you should cache the training\\ndata set because you will be accessing it many times throughout the\\nmachine learning process. Please reference the section on “Caching\\nand Persistence of Data” on page 183  in Chapter 7 .\\nPreparing Features with Transformers\\nNow that we have split our data into training and test sets, let’s prepare the data to\\nbuild a linear regression model predicting price given the number of bedrooms. In a\\nlater example, we will include all of the relevant features, but for now let’s make sure\\nwe have the mechanics in place. Linear regression (like many other algorithms in\\nSpark) requires that all the input features are contained within a single vector in your\\nDataFrame. Thus, we need to transform  our data.\\nTransformers in Spark accept a DataFrame as input and return a new DataFrame\\nwith one or more columns appended to it. They do not learn from your data, but\\napply rule-based transformations using the transform()  method.\\nFor the task of putting all of our features into a single vector, we will use the VectorAs\\nsembler  transformer . VectorAssembler  takes a list of input columns and creates a\\nnew DataFrame with an additional column, which we will call features . It combines\\nthe values of those input columns into a single vector:\\n# In Python\\nfrom pyspark.ml.feature  import VectorAssembler\\nvecAssembler  = VectorAssembler (inputCols =[\"bedrooms\" ], outputCol =\"features\" )\\nvecTrainDF  = vecAssembler .transform (trainDF)\\nvecTrainDF .select(\"bedrooms\" , \"features\" , \"price\").show(10)\\n// In Scala\\nimport org.apache.spark.ml.feature.VectorAssembler\\nval vecAssembler  = new VectorAssembler ()\\n  .setInputCols (Array(\"bedrooms\" ))\\n  .setOutputCol (\"features\" )\\nval vecTrainDF  = vecAssembler .transform (trainDF)\\nvecTrainDF .select(\"bedrooms\" , \"features\" , \"price\").show(10)\\nDesigning Machine Learning Pipelines | 293', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 316}), Document(page_content='+--------+--------+-----+\\n|bedrooms |features |price|\\n+--------+--------+-----+\\n|     1.0|   [1.0]|200.0|\\n|     1.0|   [1.0]|130.0|\\n|     1.0|   [1.0]| 95.0|\\n|     1.0|   [1.0]|250.0|\\n|     3.0|   [3.0]|250.0|\\n|     1.0|   [1.0]|115.0|\\n|     1.0|   [1.0]|105.0|\\n|     1.0|   [1.0]| 86.0|\\n|     1.0|   [1.0]|100.0|\\n|     2.0|   [2.0]|220.0|\\n+--------+--------+-----+\\nY ou’ll notice that in the Scala code, we had to instantiate the new VectorAssembler\\nobject as well as using setter methods to change the input and output columns. In\\nPython, you have the option to pass the parameters directly to the constructor of Vec\\ntorAssembler  or to use the setter methods, but in Scala you can only use the setter\\nmethods.\\nWe cover the fundamentals of linear regression next, but if you are already familiar\\nwith the algorithm, please skip to “Using Estimators to Build Models” on page 295 .\\nUnderstanding Linear Regression\\nLinear regression  models a linear relationship between your dependent variable (or\\nlabel) and one or more independent variables (or features). In our case, we want to fit\\na linear regression model to predict the price of an Airbnb rental given the number of\\nbedrooms.\\nIn Figure 10-6 , we have a single feature x and an output y (this is our dependent vari‐\\nable). Linear regression seeks to fit an equation for a line to x and y, which for scalar\\nvariables can be expressed as y = mx + b , where m is the slope and b is the offset or\\nintercept.\\nThe dots indicate the true ( x, y) pairs from our data set, and the solid line indicates\\nthe line of best fit for this data set. The data points do not perfectly line up, so we\\nusually think of linear regression as fitting a model to y ≈ mx + b + ε, where ε (epsi‐\\nlon) is an error drawn independently per record x from some distribution. These are\\nthe errors between our model predictions and the true values. Often we think of ε as\\nbeing Gaussian, or normally distributed. The vertical lines above the regression line\\nindicate positive ε (or residuals), where your true values are above the predicted val‐\\nues, and the vertical lines below the regression line indicate negative residuals. The\\ngoal of linear regression is to find a line that minimizes the square of these residuals.\\nY ou’ll notice that the line can extrapolate predictions for data points it hasn’t seen.\\n294 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 317}), Document(page_content='Figure 10-6. Univariate linear regression\\nLinear regression can also be extended to handle multiple independent variables. If\\nwe had three features as input, x = [x1, x2, x3], then we could model y as y ≈ w0 + w1x1\\n+ w2x2 + w3x3 + ε. In this case, there is a separate coefficient (or weight) for each fea‐\\nture and a single intercept ( w0 instead of b here). The process of estimating the coeffi‐\\ncients and intercept for our model is called learning  (or fitting ) the parameters for the\\nmodel. For right now, we’ll focus on the univariate regression example of predicting\\nprice given the number of bedrooms, and we’ll get back to multivariate linear regres‐\\nsion in a bit.\\nUsing Estimators to Build Models\\nAfter setting up our vectorAssembler , we have our data prepared and transformed\\ninto a format that our linear regression model expects. In Spark, LinearRegression  is\\na type of estimator—it takes in a DataFrame and returns a Model . Estimators learn\\nparameters from your data, have an estimator_name.fit()  method, and are eagerly\\nevaluated (i.e., kick off Spark jobs), whereas transformers are lazily evaluated. Some\\nother examples of estimators include Imputer , DecisionTreeClassifier , and Random\\nForestRegressor .\\nY ou’ll notice that our input column for linear regression ( features ) is the output\\nfrom our vectorAssembler :\\n# In Python\\nfrom pyspark.ml.regression  import LinearRegression\\nlr = LinearRegression (featuresCol =\"features\" , labelCol =\"price\")\\nlrModel = lr.fit(vecTrainDF )\\n// In Scala\\nimport org.apache.spark.ml.regression.LinearRegression\\nval lr = new LinearRegression ()\\n  .setFeaturesCol (\"features\" )\\nDesigning Machine Learning Pipelines | 295', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 318}), Document(page_content='  .setLabelCol (\"price\")\\nval lrModel = lr.fit(vecTrainDF )\\nlr.fit()  returns a LinearRegressionModel  (lrModel ), which is a transformer. In\\nother words, the output of an estimator’s fit()  method is a transformer. Once the\\nestimator has learned the parameters, the transformer can apply these parameters to\\nnew data points to generate predictions. Let’s inspect the parameters it learned:\\n# In Python\\nm = round(lrModel.coefficients [0], 2)\\nb = round(lrModel.intercept , 2)\\nprint(f\"\"\"The formula for the linear regression line is \\nprice = {m}*bedrooms + {b}\"\"\" )\\n// In Scala\\nval m = lrModel.coefficients (0)\\nval b = lrModel.intercept\\nprintln(f\"\"\"The formula for the linear regression line is \\nprice = $m%1.2f*bedrooms + $b%1.2f\"\"\" )\\nThis prints:\\nThe formula for the linear regression line is price = 123.68*bedrooms + 47.51\\nCreating a Pipeline\\nIf we want to apply our model to our test set, then we need to prepare that data in the\\nsame way as the training set (i.e., pass it through the vector assembler). Oftentimes\\ndata preparation pipelines will have multiple steps, and it becomes cumbersome to\\nremember not only which steps to apply, but also the ordering of the steps. This is the\\nmotivation for the Pipeline API : you simply specify the stages you want your data to\\npass through, in order, and Spark takes care of the processing for you. They provide\\nthe user with better code reusability and organization. In Spark, Pipeline s are esti‐\\nmators, whereas PipelineModel s—fitted Pipeline s—are transformers.\\nLet’s build our pipeline now:\\n# In Python\\nfrom pyspark.ml  import Pipeline\\npipeline  = Pipeline (stages=[vecAssembler , lr])\\npipelineModel  = pipeline .fit(trainDF)\\n// In Scala\\nimport org.apache.spark.ml.Pipeline\\nval pipeline  = new Pipeline ().setStages (Array(vecAssembler , lr))\\nval pipelineModel  = pipeline .fit(trainDF)\\nAnother advantage of using the Pipeline API is that it determines which stages are\\nestimators/transformers for you, so you don’t have to worry about specifying\\nname.fit()  versus name.transform()  for each of the stages.\\n296 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 319}), Document(page_content='Since pipelineModel  is a transformer, it is straightforward to apply it to our test data\\nset too:\\n# In Python\\npredDF = pipelineModel .transform (testDF)\\npredDF.select(\"bedrooms\" , \"features\" , \"price\", \"prediction\" ).show(10)\\n// In Scala\\nval predDF = pipelineModel .transform (testDF)\\npredDF.select(\"bedrooms\" , \"features\" , \"price\", \"prediction\" ).show(10)\\n+--------+--------+------+------------------+\\n|bedrooms |features | price|        prediction |\\n+--------+--------+------+------------------+\\n|     1.0|   [1.0]|  85.0|171.18598011578285 |\\n|     1.0|   [1.0]|  45.0|171.18598011578285 |\\n|     1.0|   [1.0]|  70.0|171.18598011578285 |\\n|     1.0|   [1.0]| 128.0|171.18598011578285 |\\n|     1.0|   [1.0]| 159.0|171.18598011578285 |\\n|     2.0|   [2.0]| 250.0|294.86172649777757 |\\n|     1.0|   [1.0]|  99.0|171.18598011578285 |\\n|     1.0|   [1.0]|  95.0|171.18598011578285 |\\n|     1.0|   [1.0]| 100.0|171.18598011578285 |\\n|     1.0|   [1.0]|2010.0|171.18598011578285 |\\n+--------+--------+------+------------------+\\nIn this code we built a model using only a single feature, bedrooms  (you can find the\\nnotebook for this chapter in the book’s GitHub repo ). However, you may want to\\nbuild a model using all of your features, some of which may be categorical, such as\\nhost_is_superhost . Categorical features take on discrete values and have no intrin‐\\nsic ordering—examples include occupations or country names. In the next section\\nwe’ll consider a solution for how to treat these kinds of variables, known as one-hot\\nencoding .\\nOne-hot encoding\\nIn the pipeline we just created, we only had two stages, and our linear regression\\nmodel only used one feature. Let’s take a look at how to build a slightly more complex\\npipeline that incorporates all of our numeric and categorical features.\\nMost machine learning models in MLlib expect numerical values as input, repre‐\\nsented as vectors. To convert categorical values into numeric values, we can use a\\ntechnique called one-hot encoding (OHE). Suppose we have a column called Animal\\nand we have three types of animals: Dog, Cat, and Fish . We can’t pass the string types\\ninto our ML model directly, so we need to assign a numeric mapping, such as this:\\nAnimal = {\"Dog\", \"Cat\", \"Fish\"}\\n\"Dog\" = 1, \"Cat\" = 2, \"Fish\" = 3\\nDesigning Machine Learning Pipelines | 297', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 320}), Document(page_content='However, using this approach we’ve introduced some spurious relationships into our\\ndata set that weren’t there before. For example, why did we assign Cat twice the value\\nof Dog? The numeric values we use should not introduce any relationships into our\\ndata set. Instead, we want to create a separate column for every distinct value in our\\nAnimal  column:\\n\"Dog\" = [ 1, 0, 0]\\n\"Cat\" = [ 0, 1, 0]\\n\"Fish\" = [0, 0, 1]\\nIf the animal is a dog, it has a one in the first column and zeros elsewhere. If it is a cat,\\nit has a one in the second column and zeros elsewhere. The ordering of the columns\\nis irrelevant. If you’ve used pandas before, you’ll note that this does the same thing as\\npandas.get_dummies() .\\nIf we had a zoo of 300 animals, would OHE massively increase consumption of mem‐\\nory/compute resources? Not with Spark! Spark internally uses a SparseVector  when\\nthe majority of the entries are 0, as is often the case after OHE, so it does not waste\\nspace storing 0 values. Let’s take a look at an example to better understand how\\nSparseVector s work:\\nDenseVector(0, 0, 0, 7, 0, 2, 0, 0, 0, 0)\\nSparseVector(10, [3, 5], [7, 2])\\nThe DenseVector  in this example contains 10 values, all but 2 of which are 0. To cre‐\\nate a SparseVector , we need to keep track of the size of the vector, the indices of the\\nnonzero elements, and the corresponding values at those indices. In this example the\\nsize of the vector is 10, there are two nonzero values at indices 3 and 5, and the corre‐\\nsponding values at those indices are 7 and 2.\\nThere are a few ways to one-hot encode your data with Spark. A common approach is\\nto use the StringIndexer  and OneHotEncoder . With this approach, the first step is to\\napply the StringIndexer  estimator to convert categorical values into category indi‐\\nces. These category indices are ordered by label frequencies, so the most frequent\\nlabel gets index 0, which provides us with reproducible results across various runs of\\nthe same data.\\nOnce you have created your category indices, you can pass those as input to the\\nOneHotEncoder  (OneHotEncoderEstimator  if using Spark 2.3/2.4). The OneHotEn\\ncoder  maps a column of category indices to a column of binary vectors. Take a look\\nat Table 10-2  to see the differences in the StringIndexer  and OneHotEncoder  APIs\\nfrom Spark 2.3/2.4 to 3.0.\\n298 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 321}), Document(page_content='Table 10-2. StringIndexer and OneHotEncoder changes in Spark 3.0\\n Spark 2.3 and 2.4 Spark 3.0\\nStringIndexer Single column as input/output Multiple columns as input/output\\nOneHotEncoder Deprecated Multiple columns as input/output\\nOneHotEncoderEstimator Multiple columns as input/output N/A\\nThe following code demonstrates how to one-hot encode our categorical features. In\\nour data set, any column of type string  is treated as a categorical feature, but some‐\\ntimes you might have numeric features you want treated as categorical or vice versa.\\nY ou’ll need to carefully identify which columns are numeric and which are\\ncategorical:\\n# In Python\\nfrom pyspark.ml.feature  import OneHotEncoder , StringIndexer\\ncategoricalCols  = [field for (field, dataType ) in trainDF.dtypes \\n                   if dataType  == \"string\" ]\\nindexOutputCols  = [x + \"Index\" for x in categoricalCols ]\\noheOutputCols  = [x + \"OHE\" for x in categoricalCols ]\\nstringIndexer  = StringIndexer (inputCols =categoricalCols , \\n                              outputCols =indexOutputCols , \\n                              handleInvalid =\"skip\")\\noheEncoder  = OneHotEncoder (inputCols =indexOutputCols , \\n                           outputCols =oheOutputCols )\\nnumericCols  = [field for (field, dataType ) in trainDF.dtypes \\n               if ((dataType  == \"double\" ) & (field != \"price\"))]\\nassemblerInputs  = oheOutputCols  + numericCols\\nvecAssembler  = VectorAssembler (inputCols =assemblerInputs , \\n                               outputCol =\"features\" )\\n// In Scala\\nimport org.apache.spark.ml.feature. {OneHotEncoder , StringIndexer }\\nval categoricalCols  = trainDF.dtypes.filter(_._2 == \"StringType\" ).map(_._1)\\nval indexOutputCols  = categoricalCols .map(_ + \"Index\")\\nval oheOutputCols  = categoricalCols .map(_ + \"OHE\")\\nval stringIndexer  = new StringIndexer ()\\n  .setInputCols (categoricalCols )\\n  .setOutputCols (indexOutputCols )\\n  .setHandleInvalid (\"skip\")\\nval oheEncoder  = new OneHotEncoder ()\\n  .setInputCols (indexOutputCols )\\n  .setOutputCols (oheOutputCols )\\nval numericCols  = trainDF.dtypes.filter{ case (field, dataType ) => \\n  dataType  == \"DoubleType\"  && field != \"price\"}.map(_._1)\\nDesigning Machine Learning Pipelines | 299', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 322}), Document(page_content='val assemblerInputs  = oheOutputCols  ++ numericCols\\nval vecAssembler  = new VectorAssembler ()\\n  .setInputCols (assemblerInputs )\\n  .setOutputCol (\"features\" )\\nNow you might be wondering, “How does the StringIndexer  handle new categories\\nthat appear in the test data set, but not in the training data set?” There is a\\nhandleInvalid  parameter that specifies how you want to handle them. The options\\nare skip  (filter out rows with invalid data), error  (throw an error), or keep  (put inva‐\\nlid data in a special additional bucket, at index numLabels ). For this example, we just\\nskipped the invalid records.\\nOne difficulty with this approach is that you need to tell StringIndexer  explicitly\\nwhich features should be treated as categorical features. Y ou could use\\nVectorIndexer  to automatically detect all the categorical variables, but it is computa‐\\ntionally expensive as it has to iterate over every single column and detect if it has\\nfewer than maxCategories  distinct values. maxCategories  is a parameter the user\\nspecifies, and determining this value can also be difficult.\\nAnother approach is to use RFormula . The syntax for this is inspired by the R pro‐\\ngramming language. With RFormula , you provide your label and which features you\\nwant to include. It supports a limited subset of the R operators, including ~, ., :, +,\\nand -. For example, you might specify formula = \"y ~ bedrooms + bathrooms\" ,\\nwhich means to predict y given just bedrooms  and bathrooms , or formula = \"y\\n~ .\" , which means to use all of the available features (and automatically excludes y\\nfrom the features). RFormula  will automatically StringIndex  and OHE all of your\\nstring  columns, convert your numeric columns to double  type, and combine all of\\nthese into a single vector using VectorAssembler  under the hood. Thus, we can\\nreplace all of the preceding code with a single line, and we will get the same result:\\n# In Python\\nfrom pyspark.ml.feature  import RFormula\\nrFormula  = RFormula (formula=\"price ~ .\" , \\n                    featuresCol =\"features\" , \\n                    labelCol =\"price\", \\n                    handleInvalid =\"skip\")\\n// In Scala\\nimport org.apache.spark.ml.feature.RFormula\\nval rFormula  = new RFormula ()\\n  .setFormula (\"price ~ .\" )\\n  .setFeaturesCol (\"features\" )\\n  .setLabelCol (\"price\")\\n  .setHandleInvalid (\"skip\")\\n300 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 323}), Document(page_content='The downside of RFormula  automatically combining the StringIndexer  and\\nOneHotEncoder  is that one-hot encoding is not required or recommended for all algo‐\\nrithms. For example, tree-based algorithms can handle categorical variables directly if\\nyou just use the StringIndexer  for the categorical features. Y ou do not need to one-\\nhot encode categorical features for tree-based methods, and it will often make your\\ntree-based models worse . Unfortunately, there is no one-size-fits-all solution for fea‐\\nture engineering, and the ideal approach is closely related to the downstream algo‐\\nrithms you plan to apply to your data set.\\nIf someone else performs the feature engineering for you, make\\nsure they document how they generated those features.\\nOnce you’ve written the code to transform your data set, you can add a linear regres‐\\nsion model using all of the features as input.\\nHere, we put all the feature preparation and model building into the pipeline, and\\napply it to our data set:\\n# In Python\\nlr = LinearRegression (labelCol =\"price\", featuresCol =\"features\" )\\npipeline  = Pipeline (stages = [stringIndexer , oheEncoder , vecAssembler , lr])\\n# Or use RFormula\\n# pipeline = Pipeline(stages = [rFormula, lr])\\npipelineModel  = pipeline .fit(trainDF)\\npredDF = pipelineModel .transform (testDF)\\npredDF.select(\"features\" , \"price\", \"prediction\" ).show(5)\\n// In Scala\\nval lr = new LinearRegression ()\\n  .setLabelCol (\"price\")\\n  .setFeaturesCol (\"features\" )\\nval pipeline  = new Pipeline ()\\n  .setStages (Array(stringIndexer , oheEncoder , vecAssembler , lr))\\n// Or use RFormula\\n// val pipeline = new Pipeline().setStages(Array(rFormula, lr))\\nval pipelineModel  = pipeline .fit(trainDF)\\nval predDF = pipelineModel .transform (testDF)\\npredDF.select(\"features\" , \"price\", \"prediction\" ).show(5)\\n+--------------------+-----+------------------+\\n|            features|price|         prediction|\\n+--------------------+-----+------------------+\\n|(98,[0,3,6,7,23,4...| 85.0| 55.80250714362137 |\\n|(98,[0,3,6,7,23,4...| 45.0| 22.74720286761658 |\\n|(98,[0,3,6,7,23,4...| 70.0|27.115811183814913 |\\nDesigning Machine Learning Pipelines | 301', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 324}), Document(page_content='|(98,[0,3,6,7,13,4...|128.0|-91.60763412465076 |\\n|(98,[0,3,6,7,13,4...|159.0| 94.70374072351933 |\\n+--------------------+-----+------------------+\\nAs you can see, the features column is represented as a SparseVector . There are 98\\nfeatures after one-hot encoding, followed by the nonzero indices and then the values\\nthemselves. Y ou can see the whole output if you pass in truncate=False  to show() .\\nHow is our model performing? Y ou can see that while some of the predictions might\\nbe considered “close, ” others are far off (a negative price for a rental!?). Next, we’ll\\nnumerically evaluate how well our model performs across our entire test set.\\nEvaluating Models\\nNow that we have built a model, we need to evaluate how well it performs. In\\nspark.ml  there are classification, regression, clustering, and ranking evaluators\\n(introduced in Spark 3.0). Given that this is a regression problem, we will use root-\\nmean-square error (RMSE)  and R2 (pronounced “R-squared”) to evaluate our model’s\\nperformance.\\nRMSE\\nRMSE is a metric that ranges from zero to infinity. The closer it is to zero, the better.\\nLet’s walk through the mathematical formula step by step:\\n1.Compute the difference (or error) between the true value yi and the predicted\\nvalue ŷi (pronounced y-hat, where the “hat” indicates that it is a predicted value\\nof the quantity under the hat):\\nError = yi−yi\\n2.Square the difference between yi and ŷi so that our positive and negative residuals\\ndo not cancel out. This is known as the squared error:\\nSquare Error (SE) = yi−yi2\\n3.Then we sum up the squared error for all n of our data points, known as the sum\\nof squared errors (SSE) or sum of squared residuals:\\nSum of Squared Errors (SSE) = ∑\\ni= 1n\\nyi−yi2\\n302 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 325}), Document(page_content='4.However, the SSE grows with the number of records n in the data set, so we want\\nto normalize it by the number of records. The gives us the mean-squared error\\n(MSE), a very commonly used regression metric:\\nMean Squared Error (MSE) =1\\nn∑\\ni= 1n\\nyi−yi2\\n5.If we stop at MSE, then our error term is on the scale of unit2. We’ll often take the\\nsquare root of the MSE to get the error back on the scale of the original unit,\\nwhich gives us the root-mean-square error (RMSE):\\nRoot Mean Squared Error (RMSE) =1\\nn∑\\ni= 1n\\nyi−yi2\\nLet’s evaluate our model using RMSE:\\n# In Python\\nfrom pyspark.ml.evaluation  import RegressionEvaluator\\nregressionEvaluator  = RegressionEvaluator (\\n  predictionCol =\"prediction\" , \\n  labelCol =\"price\", \\n  metricName =\"rmse\")\\nrmse = regressionEvaluator .evaluate (predDF)\\nprint(f\"RMSE is {rmse:.1f}\" )\\n// In Scala\\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\\nval regressionEvaluator  = new RegressionEvaluator ()\\n  .setPredictionCol (\"prediction\" )\\n  .setLabelCol (\"price\")\\n  .setMetricName (\"rmse\")\\nval rmse = regressionEvaluator .evaluate (predDF)\\nprintln(f\"RMSE is $rmse%.1f\")\\nThis produces the following output:\\nRMSE is 220.6\\nInterpreting the value of RMSE.    So how do we know if 220.6 is a good value for the\\nRMSE? There are various ways to interpret this value, one of which is to build a sim‐\\nple baseline model and compute its RMSE to compare against. A common baseline\\nmodel for regression tasks is to compute the average value of the label on the training\\nset ȳ (pronounced y-bar), then predict ȳ for every record in the test data set and com‐\\npute the resulting RMSE (example code is available in the book’s GitHub repo ). If you\\ntry this, you will see that our baseline model has an RMSE of 240.7, so we beat our\\nDesigning Machine Learning Pipelines | 303', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 326}), Document(page_content='baseline. If you don’t beat the baseline, then something probably went wrong in your\\nmodel building process.\\nIf this were a classification problem, you might want to predict the\\nmost prevalent class as your baseline model.\\nKeep in mind that the unit of your label directly impacts your RMSE. For example, if\\nyour label is height, then your RMSE will be higher if you use centimeters rather than\\nmeters as your unit of measurement. Y ou could arbitrarily decrease the RMSE by\\nusing a different unit, which is why it is important to compare your RMSE against a\\nbaseline.\\nThere are also some metrics that naturally give you an intuition of how you are per‐\\nforming against a baseline, such as R2, which we discuss next.\\nR2\\nDespite the name R2 containing “squared, ” R2 values range from negative infinity to 1.\\nLet’s take a look at the math behind this metric. R2 is computed as follows:\\nR2= 1 −SSres\\nSStot\\nwhere SStot is the total sum of squares if you always predict ȳ:\\nSStot=∑\\ni= 1n\\nyi−y2\\nand SSres is the sum of residuals squared from your model predictions (also known as\\nthe sum of squared errors, which we used to compute the RMSE):\\nSSres=∑\\ni= 1n\\nyi−yi2\\nIf your model perfectly predicts every data point, then your SSres = 0, making your R2\\n= 1. And if your SSres = SStot, then the fraction is 1/1, so your R2 is 0. This is what hap‐\\npens if your model performs the same as always predicting the average value, ȳ.\\nBut what if your model performs worse than always predicting ȳ and your SSres is\\nreally large? Then your R2 can actually be negative! If your R2 is negative, you should\\n304 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 327}), Document(page_content='reevaluate your modeling process. The nice thing about using R2 is that you don’t\\nnecessarily need to define a baseline model to compare against.\\nIf we want to change our regression evaluator to use R2, instead of redefining the\\nregression evaluator, we can set the metric name using the setter property:\\n# In Python\\nr2 = regressionEvaluator .setMetricName (\"r2\").evaluate (predDF)\\nprint(f\"R2 is {r2}\" )\\n// In Scala\\nval r2 = regressionEvaluator .setMetricName (\"r2\").evaluate (predDF)\\nprintln(s\"R2 is $r2\")\\nThe output is:\\nR2 is 0.159854\\nOur R2 is positive, but it’s very close to 0. One of the reasons why our model is not\\nperforming too well is because our label, price , appears to be log-normally dis‐\\ntributed . If a distribution is log-normal, it means that if we take the logarithm of the\\nvalue, the result looks like a normal distribution. Price is often log-normally dis‐\\ntributed. If you think about rental prices in San Francisco, most cost around $200 per\\nnight, but there are some that rent for thousands of dollars a night! Y ou can see the\\ndistribution of our Airbnb prices for our training Dataset in Figure 10-7 .\\nFigure 10-7. San Francisco housing price distribution\\nLet’s take a look at the resulting distribution if we instead look at the log of the price\\n(Figure 10-8 ).\\nDesigning Machine Learning Pipelines | 305', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 328}), Document(page_content='Figure 10-8. San Francisco housing log-price distribution\\nY ou can see here that our log-price distribution looks a bit more like a normal distri‐\\nbution. As an exercise, try building a model to predict price on the log scale, then\\nexponentiate the prediction to get it out of log scale and evaluate your model. The\\ncode can also be found in this chapter’s notebook in the book’s GitHub repo . Y ou\\nshould see that your RMSE decreases and your R2 increases for this data set.\\nSaving and Loading Models\\nNow that we have built and evaluated a model, let’s save it to persistent storage for\\nreuse later (or in the event that our cluster goes down, we don’t have to recompute the\\nmodel). Saving models is very similar to writing DataFrames—the API is\\nmodel.write().save( path). Y ou can optionally provide the overwrite()  command\\nto overwrite any data contained in that path:\\n# In Python\\npipelinePath  = \"/tmp/lr-pipeline-model\"\\npipelineModel .write().overwrite ().save(pipelinePath )\\n// In Scala\\nval pipelinePath  = \"/tmp/lr-pipeline-model\"\\npipelineModel .write.overwrite ().save(pipelinePath )\\nWhen you load your saved models, you need to specify the type of model you are\\nloading back in (e.g., was it a LinearRegressionModel  or a LogisticRegressionMo\\ndel?). For this reason, we recommend you always put your transformers/estimators\\ninto a Pipeline , so that for all your models you load a PipelineModel  and only need\\nto change the file path to the model:\\n306 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 329}), Document(page_content='# In Python\\nfrom pyspark.ml  import PipelineModel\\nsavedPipelineModel  = PipelineModel .load(pipelinePath )\\n// In Scala\\nimport org.apache.spark.ml.PipelineModel\\nval savedPipelineModel  = PipelineModel .load(pipelinePath )\\nAfter loading, you can apply it to new data points. However, you can’t use the weights\\nfrom this model as initialization parameters for training a new model (as opposed to\\nstarting with random weights), as Spark has no concept of “warm starts. ” If your data\\nset changes slightly, you’ll have to retrain the entire linear regression model from\\nscratch.\\nWith our linear regression model built and evaluated, let’s explore how a few other\\nmodels perform on our data set. In the next section, we will explore tree-based mod‐\\nels and look at some common hyperparameters to tune in order to improve model\\nperformance.\\nHyperparameter Tuning\\nWhen data scientists talk about tuning their models, they often discuss tuning hyper‐\\nparameters to improve the model’s predictive power. A hyperparameter  is an attribute\\nthat you define about the model prior to training, and it is not learned during the\\ntraining process (not to be confused with parameters, which are learned in the train‐\\ning process). The number of trees in your random forest is an example of a\\nhyperparameter.\\nIn this section, we will focus on using tree-based models as an example for hyper‐\\nparameter tuning procedures, but the same concepts apply to other models as well.\\nOnce we set up the mechanics to do hyperparameter tuning with spark.ml , we will\\ndiscuss ways to optimize the pipeline. Let’s get started with a brief introduction to\\ndecision trees, followed by how we can use them in spark.ml .\\nTree-Based Models\\nTree-based models such as decision trees, gradient boosted trees, and random forests\\nare relatively simple yet powerful models that are easy to interpret (meaning, it is easy\\nto explain the predictions they make). Hence, they’re quite popular for machine\\nlearning tasks. We’ll get to random forests shortly, but first we need to cover the fun‐\\ndamentals of decision trees.\\nHyperparameter Tuning | 307', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 330}), Document(page_content='Decision trees\\nAs an off-the-shelf solution, decision trees are well suited to data mining. They are\\nrelatively fast to build, highly interpretable, and scale-invariant (i.e., standardizing or\\nscaling the numeric features does not change the performance of the tree). So what is\\na decision tree?\\nA decision tree is a series of if-then-else rules learned from your data for classification\\nor regression tasks. Suppose we are trying to build a model to predict whether or not\\nsomeone will accept a job offer, and the features comprise salary, commute time, free\\ncoffee, etc. If we fit a decision tree to this data set, we might get a model that looks\\nlike Figure 10-9 .\\nFigure 10-9. Decision tree example\\nThe node at the top of the tree is called the “root” of the tree because it’s the first fea‐\\nture that we “split” on. This feature should give the most informative split—in this\\ncase, if the salary is less than $50,000, then the majority of candidates will decline the\\njob offer. The “Decline offer” node is known as a “leaf node” as there are no other\\nsplits coming out of that node; it’s at the end of a branch. (Y es, it’s a bit funny that we\\ncall it a decision “tree” but draw the root of the tree at the top and the leaves at the\\nbottom!)\\nHowever, if the salary offered is greater than $50,000, we proceed to the next most\\ninformative feature in the decision tree, which in this case is the commute time. Even\\nif the salary is over $50,000, if the commute is longer than one hour, then the major‐\\nity of people will decline the job offer.\\n308 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 331}), Document(page_content='We won’t get into the details of how to determine which features\\nwill give you the highest information gain here, but if you’re inter‐\\nested, check out Chapter 9 of The Elements of Statistical Learning ,\\nby Trevor Hastie, Robert Tibshirani, and Jerome Friedman\\n(Springer).\\nThe final feature in our model is free coffee. In this case the decision tree shows that if\\nthe salary is greater than $50,000, the commute is less than an hour, and there is free\\ncoffee, then the majority of people will accept our job offer (if only it were that sim‐\\nple!). As a follow-up resource, R2D3  has a great visualization of how decision trees\\nwork.\\nIt is possible to split on the same feature multiple times in a single\\ndecision tree, but each split will occur at a different value.\\nThe depth  of a decision tree is the longest path from the root node to any given leaf\\nnode. In Figure 10-9 , the depth is three. Trees that are very deep are prone to overfit‐\\nting, or memorizing noise in your training data set, but trees that are too shallow will\\nunderfit to your data set (i.e., could have picked up more signal from the data).\\nWith the essence of a decision tree explained, let’s resume the topic of feature prepa‐\\nration for decision trees. For decision trees, you don’t have to worry about standard‐\\nizing or scaling your input features, because this has no impact on the splits—but you\\ndo have to be careful about how you prepare your categorical features.\\nTree-based methods can naturally handle categorical variables. In spark.ml , you just\\nneed to pass the categorical columns to the StringIndexer , and the decision tree can\\ntake care of the rest. Let’s fit a decision tree to our data set:\\n# In Python\\nfrom pyspark.ml.regression  import DecisionTreeRegressor\\ndt = DecisionTreeRegressor (labelCol =\"price\")\\n# Filter for just numeric columns (and exclude price, our label)\\nnumericCols  = [field for (field, dataType ) in trainDF.dtypes \\n               if ((dataType  == \"double\" ) & (field != \"price\"))]\\n# Combine output of StringIndexer defined above and numeric columns\\nassemblerInputs  = indexOutputCols  + numericCols\\nvecAssembler  = VectorAssembler (inputCols =assemblerInputs , outputCol =\"features\" )\\n# Combine stages into pipeline\\nstages = [stringIndexer , vecAssembler , dt]\\nHyperparameter Tuning | 309', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 332}), Document(page_content='pipeline  = Pipeline (stages=stages)\\npipelineModel  = pipeline .fit(trainDF) # This line should error\\n// In Scala\\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\\nval dt = new DecisionTreeRegressor ()\\n  .setLabelCol (\"price\")\\n// Filter for just numeric columns (and exclude price, our label)\\nval numericCols  = trainDF.dtypes.filter{ case (field, dataType ) => \\n  dataType  == \"DoubleType\"  && field != \"price\"}.map(_._1)\\n// Combine output of StringIndexer defined above and numeric columns\\nval assemblerInputs  = indexOutputCols  ++ numericCols\\nval vecAssembler  = new VectorAssembler ()\\n  .setInputCols (assemblerInputs )\\n  .setOutputCol (\"features\" )\\n// Combine stages into pipeline\\nval stages = Array(stringIndexer , vecAssembler , dt)\\nval pipeline  = new Pipeline ()\\n  .setStages (stages)\\nval pipelineModel  = pipeline .fit(trainDF) // This line should error\\nThis produces the following error:\\njava.lang.IllegalArgumentException : requirement  failed: DecisionTree  requires\\nmaxBins (= 32) to be at least as large as the number of values in each \\ncategorical  feature, but categorical  feature 3 has 36 values. Consider  removing  \\nthis and other categorical  features  with a large number of values, or add more \\ntraining  examples .\\nWe can see that there is an issue with the maxBins  parameter. What does that parame‐\\nter do? maxBins  determines the number of bins into which your continuous features\\nare discretized, or split. This discretization step is crucial for performing distributed\\ntraining. There is no maxBins  parameter in scikit-learn  because all of the data and\\nthe model reside on a single machine. In Spark, however, workers have all the col‐\\numns of the data, but only a subset of the rows. Thus, when communicating about\\nwhich features and values to split on, we need to be sure they’re all talking about the\\nsame split values, which we get from the common discretization set up at training\\ntime. Let’s take a look at Figure 10-10 , which shows the PLANET  implementation of\\ndistributed decision trees, to get a better understanding of distributed machine learn‐\\ning and illustrate the maxBins  parameter.\\n310 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 333}), Document(page_content='Figure 10-10. PLANET implementation of distributed decision trees (source: https://\\noreil.ly/RAvvP )\\nEvery worker has to compute summary statistics for every feature and every possible\\nsplit point, and those statistics will be aggregated across the workers. MLlib requires\\nmaxBins  to be large enough to handle the discretization of the categorical columns.\\nThe default value for maxBins  is 32, and we had a categorical column with 36 distinct\\nvalues, which is why we got the error earlier. While we could increase maxBins  to 64\\nto more accurately represent our continuous features, that would double the number\\nof possible splits for continuous variables, greatly increasing our computation time.\\nLet’s instead set maxBins  to be 40 and retrain the pipeline. Y ou’ll notice here that we\\nare using the setter method setMaxBins()  to modify the decision tree rather than\\nredefining it completely:\\n# In Python\\ndt.setMaxBins (40)\\npipelineModel  = pipeline .fit(trainDF)\\n// In Scala\\ndt.setMaxBins (40)\\nval pipelineModel  = pipeline .fit(trainDF)\\nHyperparameter Tuning | 311', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 334}), Document(page_content='Due to differences in implementation, oftentimes you won’t get\\nexactly the same results when building a model with scikit-learn\\nversus MLlib. However, that’s OK. The key is to understand why\\nthey are different, and to see what parameters are in your control to\\nget them to perform the way you need them to. If you are porting\\nworkloads over from scikit-learn  to MLlib, we encourage you to\\ntake a look at the spark.ml  and scikit-learn  documentation to\\nsee what parameters differ, and to tweak those parameters to get\\ncomparable results for the same data. Once the values are close\\nenough, you can scale up your MLlib model to larger data sizes that\\nscikit-learn  can’t handle.\\nNow that we have successfully built our model, we can extract the if-then-else rules\\nlearned by the decision tree:\\n# In Python\\ndtModel = pipelineModel .stages[-1]\\nprint(dtModel.toDebugString )\\n// In Scala\\nval dtModel = pipelineModel .stages.last\\n  .asInstanceOf [org.apache.spark.ml.regression.DecisionTreeRegressionModel ]\\nprintln(dtModel.toDebugString )\\nDecisionTreeRegressionModel : uid=dtr_005040f1efac , depth=5, numNodes =47,...\\n  If (feature 12 <= 2.5)\\n   If (feature 12 <= 1.5)\\n    If (feature 5 in {1.0,2.0})\\n     If (feature 4 in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\\n      If (feature 3 in\\n{0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,...})\\n       Predict: 104.23992784125075\\n      Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,...})\\n       Predict: 250.7111111111111\\n...\\nThis is just a subset of the printout, but you’ll notice that it’s possible to split on the\\nsame feature more than once (e.g., feature 12), but at different split values. Also\\nnotice the difference between how the decision tree splits on numeric features versus\\ncategorical features: for numeric features it checks if the value is less than or equal to\\nthe threshold, and for categorical features it checks if the value is in that set or not.\\nWe can also extract the feature importance scores from our model to see the most\\nimportant features:\\n# In Python\\nimport pandas as pd\\nfeatureImp  = pd.DataFrame (\\n  list(zip(vecAssembler .getInputCols (), dtModel.featureImportances )),\\n312 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 335}), Document(page_content='  columns=[\"feature\" , \"importance\" ])\\nfeatureImp .sort_values (by=\"importance\" , ascending =False)\\n// In Scala\\nval featureImp  = vecAssembler\\n  .getInputCols .zip(dtModel.featureImportances .toArray)\\nval columns = Array(\"feature\" , \"Importance\" )\\nval featureImpDF  = spark.createDataFrame (featureImp ).toDF(columns: _*)\\nfeatureImpDF .orderBy($\"Importance\" .desc).show()\\nFeature Importance\\nbedrooms 0.283406\\ncancellation_policyIndex 0.167893\\ninstant_bookableIndex 0.140081\\nproperty_typeIndex 0.128179\\nnumber_of_reviews 0.126233\\nneighbourhood_cleansedIndex 0.056200\\nlongitude 0.038810\\nminimum_nights 0.029473\\nbeds 0.015218\\nroom_typeIndex 0.010905\\naccommodates 0.003603\\nWhile decision trees are very flexible and easy to use, they are not always the most\\naccurate model. If we were to compute our R2 on the test data set, we would actually\\nget a negative score! That’s worse than just predicting the average. (Y ou can see this in\\nthis chapter’s notebook in the book’s GitHub repo .)\\nLet’s look at improving this model by using an ensemble  approach that combines dif‐\\nferent models to achieve a better result: random forests.\\nRandom forests\\nEnsembles  work by taking a democratic approach. Imagine there are many M&Ms in\\na jar. Y ou ask one hundred people to guess the number of M&Ms, and then take the\\naverage of all the guesses. The average is probably closer to the true value than most\\nof the individual guesses. That same concept applies to machine learning models. If\\nyou build many models and combine/average their predictions, they will be more\\nrobust than those produced by any individual model.\\nRandom forests  are an ensemble of decision trees with two key tweaks:\\nHyperparameter Tuning | 313', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 336}), Document(page_content='Bootstrapping samples by rows\\nBootstrapping  is a technique for simulating new data by sampling with replace‐\\nment from your original data. Each decision tree is trained on a different boot‐\\nstrap sample of your data set, which produces slightly different decision trees,\\nand then you aggregate their predictions. This technique is known as bootstrap\\naggregating , or bagging . In a typical random forest implementation, each tree\\nsamples the same number of data points with replacement from the original data\\nset, and that number can be controlled through the subsamplingRate  parameter.\\nRandom feature selection by columns\\nThe main drawback with bagging is that the trees are all highly correlated, and\\nthus learn similar patterns in your data. To mitigate this problem, each time you\\nwant to make a split you only consider a random subset of the columns (1/3 of\\nthe features for RandomForestRegressor  and #features  for RandomForestClas\\nsifier ). Due to this randomness you introduce, you typically want each tree to\\nbe quite shallow. Y ou might be thinking: each of these trees will perform worse\\nthan any single decision tree, so how could this approach possibly be better? It\\nturns out that each of the trees learns something different about your data set,\\nand combining this collection of “weak” learners into an ensemble makes the for‐\\nest much more robust than a single decision tree.\\nFigure 10-11  illustrates a random forest at training time. At each split, it considers 3\\nof the 10 original features to split on; finally, it picks the best from among those.\\nFigure 10-11. Random forest training\\n314 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 337}), Document(page_content='The APIs for random forests and decision trees are similar, and both can be applied\\nto regression or classification tasks:\\n# In Python\\nfrom pyspark.ml.regression  import RandomForestRegressor\\nrf = RandomForestRegressor (labelCol =\"price\", maxBins=40, seed=42)\\n// In Scala\\nimport org.apache.spark.ml.regression.RandomForestRegressor\\nval rf = new RandomForestRegressor ()\\n  .setLabelCol (\"price\")\\n  .setMaxBins (40)\\n  .setSeed(42)\\nOnce you’ve trained your random forest, you can pass new data points through the\\ndifferent trees trained in the ensemble.\\nAs Figure 10-12  shows, if you build a random forest for classification, it passes the\\ntest point through each of the trees in the forest and takes a majority vote among the\\npredictions of the individual trees. (By contrast, in regression, the random forest sim‐\\nply averages those predictions.) Even though each of these trees is less performant\\nthan any individual decision tree, the collection (or ensemble) actually provides a\\nmore robust model.\\nFigure 10-12. Random forest predictions\\nRandom forests truly demonstrate the power of distributed machine learning with\\nSpark, as each tree can be built independently of the other trees (e.g., you do not need\\nto build tree 3 before you build tree 10). Furthermore, within each level of the tree,\\nyou can parallelize the work to find the optimal splits.\\nHyperparameter Tuning | 315', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 338}), Document(page_content='So how do we determine what the optimal number of trees in our random forest or\\nthe max depth of those trees should be? This process is called hyperparameter tuning .\\nIn contrast to a parameter, a hyperparameter is a value that controls the learning pro‐\\ncess or structure of your model, and it is not learned during training. Both the num‐\\nber of trees and the max depth are examples of hyperparameters you can tune for\\nrandom forests. Let’s now shift our focus to how we can discover and evaluate the\\nbest random forest model by tuning some hyperparameters.\\nk-Fold Cross-Validation\\nWhich data set should we use to determine the optimal hyperparameter values? If we\\nuse the training set, then the model is likely to overfit, or memorize the nuances of\\nour training data. This means it will be less likely to generalize to unseen data. But if\\nwe use the test set, then that will no longer represent “unseen” data, so we won’t be\\nable to use it to verify how well our model generalizes. Thus, we need another data set\\nto help us determine the optimal hyperparameters: the validation  data set.\\nFor example, instead of splitting our data into an 80/20 train/test split, as we did ear‐\\nlier, we can do a 60/20/20 split to generate training, validation, and test data sets,\\nrespectively. We can then build our model on the training set, evaluate performance\\non the validation set to select the best hyperparameter configuration, and apply the\\nmodel to the test set to see how well it performs on new data. However, one of the\\ndownsides of this approach is that we lose 25% of our training data (80% -> 60%),\\nwhich could have been used to help improve the model. This motivates the use of the\\nk-fold cross-validation  technique to solve this problem.\\nWith this approach, instead of splitting the data set into separate training, validation,\\nand test sets, we split it into training and test sets as before—but we use the training\\ndata for both training and validation. To accomplish this, we split our training data\\ninto k subsets, or “folds” (e.g., three). Then, for a given hyperparameter configuration,\\nwe train our model on k–1 folds and evaluate on the remaining fold, repeating this\\nprocess k times. Figure 10-13  illustrates this approach.\\nFigure 10-13. k-fold cross-validation\\nAs this figure shows, if we split our data into three folds, our model is first trained on\\nthe first and second folds (or splits) of the data, and evaluated on the third fold. We\\nthen build the same model with the same hyperparameters on the first and third folds\\n316 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 339}), Document(page_content='of the data, and evaluate its performance on the second fold. Lastly, we build the\\nmodel on the second and third folds and evaluate it on the first fold. We then average\\nthe performance of those three (or k) validation data sets as a proxy of how well this\\nmodel will perform on unseen data, as every data point had the chance to be part of\\nthe validation data set exactly once. Next, we repeat this process for all of our differ‐\\nent hyperparameter configurations to identify the optimal one.\\nDetermining the search space of your hyperparameters can be difficult, and often\\ndoing a random search of hyperparameters outperforms a structured grid search .\\nThere are specialized libraries, such as Hyperopt , to help you identify the optimal\\nhyperparameter configurations , which we touch upon in Chapter 11 .\\nTo perform a hyperparameter search in Spark, take the following steps :\\n1.Define the estimator  you want to evaluate.\\n2.Specify which hyperparameters you want to vary, as well as their respective val‐\\nues, using the ParamGridBuilder .\\n3.Define an evaluator  to specify which metric to use to compare the various\\nmodels.\\n4.Use the CrossValidator  to perform cross-validation, evaluating each of the vari‐\\nous models.\\nLet’s start by defining our pipeline estimator:\\n# In Python\\npipeline  = Pipeline (stages = [stringIndexer , vecAssembler , rf])\\n// In Scala\\nval pipeline  = new Pipeline ()\\n  .setStages (Array(stringIndexer , vecAssembler , rf))\\nFor our ParamGridBuilder , we’ll vary our maxDepth  to be 2, 4, or 6 and numTrees  (the\\nnumber of trees in our random forest) to be 10 or 100. This will give us a grid of 6 (3\\nx 2) different hyperparameter configurations in total:\\n(maxDepth=2, numTrees=10)\\n(maxDepth=2, numTrees=100)\\n(maxDepth=4, numTrees=10)\\n(maxDepth=4, numTrees=100)\\n(maxDepth=6, numTrees=10)\\n(maxDepth=6, numTrees=100)\\n# In Python\\nfrom pyspark.ml.tuning  import ParamGridBuilder\\nparamGrid  = (ParamGridBuilder ()\\n            .addGrid(rf.maxDepth , [2, 4, 6])\\n            .addGrid(rf.numTrees , [10, 100])\\n            .build())\\nHyperparameter Tuning | 317', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 340}), Document(page_content='// In Scala\\nimport org.apache.spark.ml.tuning.ParamGridBuilder\\nval paramGrid  = new ParamGridBuilder ()\\n  .addGrid(rf.maxDepth , Array(2, 4, 6))\\n  .addGrid(rf.numTrees , Array(10, 100))\\n  .build()\\nNow that we have set up our hyperparameter grid, we need to define how to evaluate\\neach of the models to determine which one performed best. For this task we will use\\nthe RegressionEvaluator , and we’ll use RMSE as our metric of interest:\\n# In Python\\nevaluator  = RegressionEvaluator (labelCol =\"price\", \\n                                predictionCol =\"prediction\" , \\n                                metricName =\"rmse\")\\n// In Scala\\nval evaluator  = new RegressionEvaluator ()\\n  .setLabelCol (\"price\")\\n  .setPredictionCol (\"prediction\" )\\n  .setMetricName (\"rmse\")\\nWe will perform our k-fold cross-validation using the CrossValidator , which accepts\\nan estimator , evaluator , and estimatorParamMaps  so that it knows which model to\\nuse, how to evaluate the model, and which hyperparameters to set for the model. We\\ncan also set the number of folds we want to split our data into ( numFolds=3 ), as well\\nas setting a seed so we have reproducible splits across the folds ( seed=42 ). Let’s then\\nfit this cross-validator to our training data set:\\n# In Python\\nfrom pyspark.ml.tuning  import CrossValidator\\ncv = CrossValidator (estimator =pipeline , \\n                    evaluator =evaluator , \\n                    estimatorParamMaps =paramGrid , \\n                    numFolds =3, \\n                    seed=42)\\ncvModel = cv.fit(trainDF)\\n// In Scala\\nimport org.apache.spark.ml.tuning.CrossValidator\\nval cv = new CrossValidator ()\\n .setEstimator (pipeline )\\n .setEvaluator (evaluator )\\n .setEstimatorParamMaps (paramGrid )\\n .setNumFolds (3)\\n .setSeed(42)\\nval cvModel = cv.fit(trainDF)\\n318 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 341}), Document(page_content='The output tells us how long the operation took:\\nCommand took 1.07 minutes\\nSo, how many models did we just train? If you answered 18 (6 hyperparameter con‐\\nfigurations x 3-fold cross-validation), you’re close. Once you’ve identified the optimal\\nhyperparameter configuration, how do you combine those three (or k) models\\ntogether? While some models might be easy enough to average together, some are\\nnot. Therefore, Spark retrains your model on the entire training data set once it has\\nidentified the optimal hyperparameter configuration, so in the end we trained 19\\nmodels. If you want to retain the intermediate models trained, you can set collect\\nSubModels=True  in the CrossValidator .\\nTo inspect the results of the cross-validator, you can take a look at the avgMetrics :\\n# In Python\\nlist(zip(cvModel.getEstimatorParamMaps (), cvModel.avgMetrics ))\\n// In Scala\\ncvModel.getEstimatorParamMaps .zip(cvModel.avgMetrics )\\nHere’s the output:\\nres1: Array[(org.apache.spark.ml.param.ParamMap, Double)] =\\nArray(({\\n    rfr_a132fb1ab6c8-maxDepth: 2,\\n    rfr_a132fb1ab6c8-numTrees: 10\\n},303.99522869739343), ({\\n    rfr_a132fb1ab6c8-maxDepth: 2,\\n    rfr_a132fb1ab6c8-numTrees: 100\\n},299.56501993529474), ({\\n    rfr_a132fb1ab6c8-maxDepth: 4,\\n    rfr_a132fb1ab6c8-numTrees: 10\\n},310.63687030886894), ({\\n    rfr_a132fb1ab6c8-maxDepth: 4,\\n    rfr_a132fb1ab6c8-numTrees: 100\\n},294.7369599168999), ({\\n    rfr_a132fb1ab6c8-maxDepth: 6,\\n    rfr_a132fb1ab6c8-numTrees: 10\\n},312.6678169109293), ({\\n    rfr_a132fb1ab6c8-maxDepth: 6,\\n    rfr_a132fb1ab6c8-numTrees: 100\\n},292.101039874209))\\nWe can see that the best model from our CrossValidator  (the one with the lowest\\nRMSE) had maxDepth=6  and numTrees=100 . However, this took a long time to run. In\\nthe next section, we will look at how we can decrease the time to train our model\\nwhile maintaining the same model performance.\\nHyperparameter Tuning | 319', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 342}), Document(page_content='Optimizing Pipelines\\nIf your code takes long enough for you to think about improving it, then you should\\noptimize it. In the preceding code, even though each of the models in the cross-\\nvalidator is technically independent, spark.ml  actually trains the collection of models\\nsequentially rather than in parallel. In Spark 2.3, a parallelism  parameter was intro‐\\nduced to solve this problem. This parameter determines the number of models to\\ntrain in parallel, which themselves are fit in parallel. From the Spark Tuning Guide :\\nThe value of parallelism  should be chosen carefully to maximize parallelism without\\nexceeding cluster resources, and larger values may not always lead to improved perfor‐\\nmance. Generally speaking, a value up to 10 should be sufficient for most clusters.\\nLet’s set this value to 4 and see if we can train any faster:\\n# In Python\\ncvModel = cv.setParallelism (4).fit(trainDF)\\n// In Scala\\nval cvModel = cv.setParallelism (4).fit(trainDF)\\nThe answer is yes:\\nCommand took 31.45 seconds\\nWe’ve cut the training time in half (from 1.07 minutes to 31.45 seconds), but we can\\nstill improve it further! There’s another trick we can use to speed up model training:\\nputting the cross-validator inside the pipeline (e.g., Pipeline(stages=[..., cv])\\ninstead of putting the pipeline inside the cross-validator (e.g., CrossValidator(esti\\nmator=pipeline, ...) ). Every time the cross-validator evaluates the pipeline, it runs\\nthrough every step of the pipeline for each model, even if some of the steps don’t\\nchange, such as the StringIndexer . By reevaluating every step in the pipeline, we are\\nlearning the same StringIndexer  mapping over and over again, even though it’s not\\nchanging.\\nIf instead we put our cross-validator inside our pipeline, then we won’t be reevaluat‐\\ning the StringIndexer  (or any other estimator) each time we try a different model:\\n# In Python\\ncv = CrossValidator (estimator =rf, \\n                    evaluator =evaluator , \\n                    estimatorParamMaps =paramGrid , \\n                    numFolds =3, \\n                    parallelism =4, \\n                    seed=42)\\npipeline  = Pipeline (stages=[stringIndexer , vecAssembler , cv])\\npipelineModel  = pipeline .fit(trainDF)\\n// In Scala\\nval cv = new CrossValidator ()\\n320 | Chapter 10: Machine Learning with MLlib', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 343}), Document(page_content='  .setEstimator (rf)\\n  .setEvaluator (evaluator )\\n  .setEstimatorParamMaps (paramGrid )\\n  .setNumFolds (3)\\n  .setParallelism (4)\\n  .setSeed(42)\\nval pipeline  = new Pipeline ()\\n                   .setStages (Array(stringIndexer , vecAssembler , cv))\\nval pipelineModel  = pipeline .fit(trainDF)\\nThis trims five seconds off our training time:\\nCommand took 26.21 seconds\\nThanks to the parallelism  parameter and rearranging the ordering of our pipeline,\\nthat last run was the fastest—and if you apply it to the test data set you’ll see that you\\nget the same results. Although these gains were on the order of seconds, the same\\ntechniques apply to much larger data sets and models, with correspondingly larger\\ntime savings. Y ou can try running this code yourself by accessing the notebook in the\\nbook’s GitHub repo .\\nSummary\\nIn this chapter we covered how to build pipelines using Spark MLlib—in particular,\\nits DataFrame-based API package, spark.ml . We discussed the differences between\\ntransformers and estimators, how to compose them using the Pipeline API, and some\\ndifferent metrics for evaluating models. We then explored how to use cross-validation\\nto perform hyperparameter tuning to deliver the best model, as well as tips for opti‐\\nmizing cross-validation and model training in Spark.\\nAll this sets the context for the next chapter, in which we will discuss deployment\\nstrategies and ways to manage and scale machine learning pipelines with Spark.\\nSummary | 321', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 344}), Document(page_content='', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 345}), Document(page_content='CHAPTER 11\\nManaging, Deploying, and Scaling Machine\\nLearning Pipelines with Apache Spark\\nIn the previous chapter, we covered how to build machine learning pipelines with\\nMLlib. This chapter will focus on how to manage and deploy the models you train. By\\nthe end of this chapter, you will be able to use MLflow to track, reproduce, and\\ndeploy your MLlib models, discuss the difficulties of and trade-offs among various\\nmodel deployment scenarios, and architect scalable machine learning solutions. But\\nbefore we discuss deploying models, let’s first discuss some best practices for model\\nmanagement to get your models ready for deployment.\\nModel Management\\nBefore you deploy your machine learning model, you should ensure that you can\\nreproduce and track the model’s performance. For us, end-to-end reproducibility of\\nmachine learning solutions means that we need to be able to reproduce the code that\\ngenerated a model, the environment used in training, the data it was trained on, and\\nthe model itself. Every data scientist loves to remind you to set your seeds so you can\\nreproduce your experiments (e.g., for the train/test split, when using models with\\ninherent randomness such as random forests). However, there are many more aspects\\nthat contribute to reproducibility than just setting seeds, and some of them are much\\nmore subtle. Here are a few examples:\\nLibrary versioning\\nWhen a data scientist hands you their code, they may or may not mention the\\ndependent libraries. While you are able to figure out which libraries are required\\nby going through the error messages, you won’t be certain which library versions\\nthey used, so you’ll likely install the latest ones. But if their code was built on a\\nprevious version of a library, which may be taking advantage of some default\\n323', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 346}), Document(page_content='behavior that differs from the version you installed, using the latest version can\\ncause the code to break or the results to differ (for example, consider how\\nXGBoost  changed how it handles missing values  in v0.90).\\nData evolution\\nSuppose you build a model on June 1, 2020, and keep track of all your hyperpara‐\\nmeters, libraries, etc. Y ou then try to reproduce the same model on July 1, 2020—\\nbut the pipeline breaks or the results differ because the underlying data has\\nchanged, which could happen if someone added an extra column or an order of\\nmagnitude more data after the initial build.\\nOrder of execution\\nIf a data scientist hands you their code, you should be able to run it top-to-\\nbottom without error. However, data scientists are notorious for running things\\nout of order, or running the same stateful cell multiple times, making their results\\nvery difficult to reproduce. (They might also check in a copy of the code with dif‐\\nferent hyperparameters than those used to train the final model!)\\nParallel operations\\nTo maximize throughput, GPUs will run many operations in parallel. However,\\nthe order of execution is not always guaranteed, which can lead to nondetermin‐\\nistic outputs. This is a known problem with functions like tf.reduce_sum()  and\\nwhen aggregating floating-point numbers (which have limited precision): the\\norder in which you add them may generate slightly different results, which can be\\nexacerbated across many iterations.\\nAn inability to reproduce your experiments can often be a blocker in getting business\\nunits to adopt your model or put it into production. While you could build your own\\nin-house tools for tracking your models, data, dependency versions, etc., they may\\nbecome obsolete, brittle, and take significant development effort to maintain. Equally\\nimportant is having industry-wide standards for managing models so that they can be\\neasily shared with partners. There are both open source and proprietary tools that\\ncan help us with reproducing our machine learning experiments by abstracting away\\nmany of these common difficulties. This section will focus on MLflow, as it has the\\ntightest integration with MLlib of the currently available open source model manage‐\\nment tools.\\nMLflow\\nMLflow  is an open source platform that helps developers reproduce and share experi‐\\nments, manage models, and much more. It provides interfaces in Python, R, and Java/\\nScala, as well as a REST API. As shown in Figure 11-1 , MLflow has four main\\ncomponents:\\n324 | Chapter 11: Managing, Deploying, and Scaling Machine Learning Pipelines with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 347}), Document(page_content='Tracking\\nProvides APIs to record parameters, metrics, code versions, models, and artifacts\\nsuch as plots, and text.\\nProjects\\nA standardized format to package your data science projects and their dependen‐\\ncies to run on other platforms. It helps you manage the model training process.\\nModels\\nA standardized format to package models to deploy to diverse execution environ‐\\nments. It provides a consistent API for loading and applying models, regardless\\nof the algorithm or library used to build the model.\\nRegistry\\nA repository to keep track of model lineage, model versions, stage transitions,\\nand annotations.\\nFigure 11-1. MLflow  components\\nLet’s track the MLlib model experiments we ran in Chapter 10  for reproducibility. We\\nwill then see how the other components of MLflow come into play when we discuss\\nmodel deployment. To get started with MLflow, simply run pip install mlflow  on\\nyour local host.\\nTracking\\nMLflow Tracking is a logging API that is agnostic to the libraries and environments\\nthat actually do the training. It is organized around the concept of runs , which are\\nexecutions of data science code. Runs are aggregated into experiments , such that\\nmany runs can be part of a given experiment.\\nThe MLflow tracking server can host many experiments. Y ou can log to the tracking\\nserver using a notebook, local app, or cloud job, as shown in Figure 11-2 .\\nModel Management | 325', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 348}), Document(page_content='Figure 11-2. MLflow  tracking server\\nLet’s examine a few things that can be logged to the tracking server:\\nParameters\\nKey/value inputs to your code—e.g., hyperparameters like num_trees  or\\nmax_depth  in your random forest\\nMetrics\\nNumeric values (can update over time)—e.g., RMSE or accuracy values\\nArtifacts\\nFiles, data, and models—e.g., matplotlib  images, or Parquet files\\nMetadata\\nInformation about the run, such as the source code that executed the run or the\\nversion of the code (e.g., the Git commit hash string for the code version)\\nModels\\nThe model(s) you trained\\nBy default, the tracking server records everything to the filesystem, but you can spec‐\\nify a database  for faster querying, such as for the parameters and metrics. Let’s add\\nMLflow tracking to our random forest code from Chapter 10 :\\n326 | Chapter 11: Managing, Deploying, and Scaling Machine Learning Pipelines with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 349}), Document(page_content='# In Python\\nfrom pyspark.ml  import Pipeline\\nfrom pyspark.ml.feature  import StringIndexer , VectorAssembler\\nfrom pyspark.ml.regression  import RandomForestRegressor\\nfrom pyspark.ml.evaluation  import RegressionEvaluator\\nfilePath  = \"\"\"/databricks-datasets/learning-spark-v2/sf-airbnb/\\nsf-airbnb-clean.parquet\"\"\"\\nairbnbDF  = spark.read.parquet(filePath )\\n(trainDF, testDF) = airbnbDF .randomSplit ([.8, .2], seed=42)\\ncategoricalCols  = [field for (field, dataType ) in trainDF.dtypes \\n                   if dataType  == \"string\" ]\\nindexOutputCols  = [x + \"Index\" for x in categoricalCols ]\\nstringIndexer  = StringIndexer (inputCols =categoricalCols , \\n                              outputCols =indexOutputCols , \\n                              handleInvalid =\"skip\")\\nnumericCols  = [field for (field, dataType ) in trainDF.dtypes \\n               if ((dataType  == \"double\" ) & (field != \"price\"))]\\nassemblerInputs  = indexOutputCols  + numericCols\\nvecAssembler  = VectorAssembler (inputCols =assemblerInputs , \\n                               outputCol =\"features\" )\\nrf = RandomForestRegressor (labelCol =\"price\", maxBins=40, maxDepth =5, \\n                           numTrees =100, seed=42)\\npipeline  = Pipeline (stages=[stringIndexer , vecAssembler , rf])\\nTo start logging with MLflow, you will need to start a run using mlflow.start_run() .\\nInstead of explicitly calling mlflow.end_run() , the examples in this chapter will use a\\nwith  clause to automatically end the run at the end of the with  block:\\n# In Python \\nimport mlflow\\nimport mlflow.spark\\nimport pandas as pd\\nwith mlflow.start_run (run_name =\"random-forest\" ) as run:\\n  # Log params: num_trees and max_depth\\n  mlflow.log_param (\"num_trees\" , rf.getNumTrees ())\\n  mlflow.log_param (\"max_depth\" , rf.getMaxDepth ())\\n \\n  # Log model\\n  pipelineModel  = pipeline .fit(trainDF)\\n  mlflow.spark.log_model (pipelineModel , \"model\")\\n  # Log metrics: RMSE and R2\\n  predDF = pipelineModel .transform (testDF)\\n  regressionEvaluator  = RegressionEvaluator (predictionCol =\"prediction\" , \\n                                            labelCol =\"price\")\\n  rmse = regressionEvaluator .setMetricName (\"rmse\").evaluate (predDF)\\nModel Management | 327', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 350}), Document(page_content='  r2 = regressionEvaluator .setMetricName (\"r2\").evaluate (predDF)\\n  mlflow.log_metrics ({\"rmse\": rmse, \"r2\": r2})\\n  # Log artifact: feature importance scores\\n  rfModel = pipelineModel .stages[-1]\\n  pandasDF  = (pd.DataFrame (list(zip(vecAssembler .getInputCols (), \\n                                    rfModel.featureImportances )), \\n                           columns=[\"feature\" , \"importance\" ])\\n              .sort_values (by=\"importance\" , ascending =False))\\n  # First write to local filesystem, then tell MLflow where to find that file\\n  pandasDF .to_csv(\"feature-importance.csv\" , index=False)\\n  mlflow.log_artifact (\"feature-importance.csv\" )\\nLet’s examine the MLflow UI, which you can access by running mlflow ui  in your\\nterminal and navigating to http://localhost:5000/ . Figure 11-3  shows a screenshot of\\nthe UI.\\nFigure 11-3. The MLflow  UI\\nThe UI stores all the runs for a given experiment. Y ou can search across all the runs,\\nfilter for those that meet particular criteria, compare runs side by side, etc. If you\\nwish, you can also export the contents as a CSV file to analyze locally. Click on the\\nrun in the UI named \"random-forest\" . Y ou should see a screen like Figure 11-4 .\\n328 | Chapter 11: Managing, Deploying, and Scaling Machine Learning Pipelines with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 351}), Document(page_content='Figure 11-4. Random forest run\\nY ou’ll notice that it keeps track of the source code used for this MLflow run, as well as\\nstoring all the corresponding parameters, metrics, etc. Y ou can add notes about this\\nrun in free text, as well as tags. Y ou cannot modify the parameters or metrics after the\\nrun has finished.\\nY ou can also query the tracking server using the MlflowClient  or REST API:\\n# In Python\\nfrom mlflow.tracking  import MlflowClient\\nclient = MlflowClient ()\\nruns = client.search_runs (run.info.experiment_id , \\n                          order_by =[\"attributes.start_time desc\" ], \\n                          max_results =1)\\nrun_id = runs[0].info.run_id\\nruns[0].data.metrics\\nModel Management | 329', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 352}), Document(page_content='This produces the following output:\\n{\\'r2\\': 0.22794251914574226, \\'rmse\\': 211.5096898777315}\\nWe have hosted this code as an MLflow project  in the GitHub repo  for this book, so\\nyou can experiment running it with different hyperparameter values for max_depth\\nand num_trees . The YAML file inside the MLflow project specifies the library depen‐\\ndencies so this code can be run in other environments:\\n# In Python\\nmlflow.run(\\n  \"https://github.com/databricks/LearningSparkV2/#mlflow-project-example\" , \\n  parameters ={\"max_depth\" : 5, \"num_trees\" : 100})\\n# Or on the command line\\nmlflow run https://github.com/databricks /LearningSparkV2 /#mlflow-project-example\\n-P max_depth =5 -P num_trees =100\\nNow that you have tracked and reproduced your experiments, let’s discuss the various\\ndeployment options available for your MLlib models.\\nModel Deployment Options with MLlib\\nDeploying machine learning models means something different for every organiza‐\\ntion and use case. Business constraints will impose different requirements for latency,\\nthroughput, cost, etc., which dictate which mode of model deployment is suitable for\\nthe task at hand—be it batch, streaming, real-time, or mobile/embedded. Deploying\\nmodels on mobile/embedded systems is outside the scope of this book, so we will\\nfocus primarily on the other options. Table 11-1  shows the throughput  and latency\\ntrade-offs for these three deployment options for generating predictions. We care\\nabout both the number of concurrent requests and the size of those requests, and the\\nresulting solutions will look quite different.\\nTable 11-1. Batch, streaming, and real-time comparison\\nThroughput Latency Example application\\nBatch High High (hours to days) Customer churn prediction\\nStreaming Medium Medium (seconds to minutes) Dynamic pricing\\nReal-time Low Low (milliseconds) Online ad bidding\\nBatch processing generates predictions on a regular schedule and writes the results\\nout to persistent storage to be served elsewhere. It is typically the cheapest and easiest\\ndeployment option as you only need to pay for compute during your scheduled run.\\nBatch processing is much more efficient per data point because you accumulate less\\noverhead when amortized across all predictions made. This is particularly the case\\nwith Spark, because of the overhead of communicating back and forth between the\\ndriver and the executors—you wouldn’t want to make predictions one data point at a\\n330 | Chapter 11: Managing, Deploying, and Scaling Machine Learning Pipelines with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 353}), Document(page_content='time! However, its main drawback is latency, as it is typically scheduled with a period\\nof hours or days to generate the next batch of predictions.\\nStreaming provides a nice trade-off between throughput and latency. Y ou will contin‐\\nuously make predictions on micro-batches of data and get your predictions in sec‐\\nonds to minutes. If you are using Structured Streaming, almost all of your code will\\nlook identical to the batch use case, making it easy to go back and forth between these\\ntwo options. With streaming, you will have to pay for the VMs or computing resour‐\\nces you use to continually stay up and running, and ensure that you have configured\\nthe stream properly to be fault tolerant and provide buffering if there are spikes in the\\nincoming data.\\nReal-time deployment prioritizes latency over throughput and generates predictions\\nin a few milliseconds. Y our infrastructure will need to support load balancing and be\\nable to scale to many concurrent requests if there is a large spike in demand (e.g., for\\nonline retailers around the holidays). Sometimes when people say “real-time deploy‐\\nment” they mean extracting precomputed predictions in real time, but here we’re\\nreferring to generating  model predictions in real time. Real-time deployment is the\\nonly option that Spark cannot meet the latency requirements for, so to use it you will\\nneed to export your model outside of Spark. For example, if you intend to use a REST\\nendpoint for real-time model inference (say, computing predictions in under 50 ms),\\nMLlib does not meet the latency requirements necessary for this application, as\\nshown in Figure 11-5 . Y ou will need to get your feature preparation and model out of\\nSpark, which can be time-consuming and difficult.\\nFigure 11-5. Deployment options for MLlib\\nBefore you begin the modeling process, you need to define your model deployment\\nrequirements. MLlib and Spark are just a few tools in your toolbox, and you need to\\nunderstand when and where they should be applied. The remainder of this section\\nModel Deployment Options with MLlib | 331', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 354}), Document(page_content='discusses the deployment options for MLlib in more depth, and then we’ll consider\\nthe deployment options with Spark for non-MLlib models.\\nBatch\\nBatch deployments represent the majority of use cases for deploying machine learn‐\\ning models, and this is arguably the easiest option to implement. Y ou will run a regu‐\\nlar job to generate predictions, and save the results to a table, database, data lake, etc.\\nfor downstream consumption. In fact, you have already seen how to generate batch\\npredictions in Chapter 10  with MLlib. MLlib’s model.transform()  will apply the\\nmodel in parallel to all partitions of your DataFrame:\\n# In Python\\n# Load saved model with MLflow\\nimport mlflow.spark\\npipelineModel  = mlflow.spark.load_model (f\"runs:/{run_id}/model\" )\\n# Generate predictions\\ninputDF = spark.read.parquet(\"/databricks-datasets/learning-spark-v2/\\n  sf-airbnb/sf-airbnb-clean.parquet \")\\npredDF = pipelineModel .transform (inputDF)\\nA few things to keep in mind with batch deployments are:\\nHow frequently will you generate predictions?\\nThere is a trade-off between latency and throughput. Y ou will get higher\\nthroughput batching many predictions together, but then the time it takes to\\nreceive any individual predictions will be much longer, delaying your ability to\\nact on these predictions.\\nHow often  will you retrain the model?\\nUnlike libraries like sklearn or TensorFlow, MLlib does not support online\\nupdates or warm starts. If you’ d like to retrain your model to incorporate the lat‐\\nest data, you’ll have to retrain the entire model from scratch, rather than getting\\nto leverage the existing parameters. In terms of the frequency of retraining, some\\npeople will set up a regular job to retrain the model (e.g., once a month), while\\nothers will actively monitor the model drift  to identify when they need to retrain.\\nHow will you version the model?\\nY ou can use the MLflow Model Registry  to keep track of the models you are\\nusing and control how they are transitioned to/from staging, production, and\\narchived. Y ou can see a screenshot of the Model Registry in Figure 11-6 . Y ou can\\nuse the Model Registry with the other deployment options too.\\n332 | Chapter 11: Managing, Deploying, and Scaling Machine Learning Pipelines with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 355}), Document(page_content='Figure 11-6. MLflow  Model Registry\\nIn addition to using the MLflow UI to manage your models, you can also manage\\nthem programmatically. For example, once you have registered your production\\nmodel, it has a consistent URI that you can use to retrieve the latest version:\\n# Retrieve latest production model\\nmodel_production_uri  = f\"models:/{model_name}/production\"\\nmodel_production  = mlflow.spark.load_model (model_production_uri )\\nStreaming\\nInstead of waiting for an hourly or nightly job to process your data and generate pre‐\\ndictions, Structured Streaming can continuously perform inference on incoming\\ndata. While this approach is more costly than a batch solution as you have to continu‐\\nally pay for compute time (and get lower throughput), you get the added benefit of\\ngenerating predictions more frequently so you can act on them sooner. Streaming\\nsolutions in general are more complicated to maintain and monitor than batch solu‐\\ntions, but they offer lower latency.\\nWith Spark it’s very easy to convert your batch predictions to streaming predictions,\\nand practically all of the code is the same. The only difference is that when you read\\nin the data, you need to use spark.readStream()  rather than spark.read()  and\\nchange the source of the data. In the following example we are going to simulate\\nreading in streaming data by streaming in a directory of Parquet files. Y ou’ll notice\\nthat we are specifying a schema  even though we are working with Parquet files. This is\\nbecause we need to define the schema a priori when working with streaming data. In\\nthis example, we will use the random forest model trained on our Airbnb data set\\nfrom the previous chapter to perform these streaming predictions. We will load in the\\nsaved model using MLflow. We have partitioned the source file into one hundred\\nsmall Parquet files so you can see the output changing at every trigger interval:\\n# In Python\\n# Load saved model with MLflow\\npipelineModel  = mlflow.spark.load_model (f\"runs:/{run_id}/model\" )\\nModel Deployment Options with MLlib | 333', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 356}), Document(page_content='# Set up simulated streaming data\\nrepartitionedPath  = \"/databricks-datasets/learning-spark-v2/sf-airbnb/\\n  sf-airbnb-clean-100p.parquet \"\\nschema = spark.read.parquet(repartitionedPath ).schema\\nstreamingData  = (spark\\n                 .readStream\\n                 .schema(schema) # Can set the schema this way\\n                 .option(\"maxFilesPerTrigger\" , 1)\\n                 .parquet(repartitionedPath ))\\n# Generate predictions\\nstreamPred  = pipelineModel .transform (streamingData )\\nAfter you generate these predictions, you can write them out to any target location\\nfor retrieval later (refer to Chapter 8  for Structured Streaming tips). As you can see,\\nthe code is virtually unchanged between the batch and streaming scenarios, making\\nMLlib a great solution for both. However, depending on the latency demands of your\\ntask, MLlib may not be the best choice. With Spark there is significant overhead\\ninvolved in generating the query plan and communicating the task and results\\nbetween the driver and the worker. Thus, if you need really low-latency predictions,\\nyou’ll need to export your model out of Spark.\\nNear Real-Time\\nIf your use case requires predictions on the order of hundreds of milliseconds to sec‐\\nonds, you could build a prediction server that uses MLlib to generate the predictions.\\nWhile this is not an ideal use case for Spark because you are processing very small\\namounts of data, you’ll get lower latency than with streaming or batch solutions.\\nModel Export Patterns for Real-Time Inference\\nThere are some domains where real-time inference is required, including fraud detec‐\\ntion, ad recommendation, and the like. While making predictions with a small num‐\\nber of records may achieve the low latency required for real-time inference, you will\\nneed to contend with load balancing (handling many concurrent requests) as well as\\ngeolocation in latency-critical tasks. There are popular managed solutions, such as\\nAWS SageMaker  and Azure ML , that provide low-latency model serving solutions. In\\nthis section we’ll show you how to export your MLlib models so they can be deployed\\nto those services.\\nOne way to export your model out of Spark is to reimplement the model natively in\\nPython, C, etc. While it may seem simple to extract the coefficients of the model,\\nexporting all the feature engineering and preprocessing steps along with them ( OneHo\\ntEncoder , VectorAssembler , etc.) quickly gets troublesome and is very error-prone. \\n334 | Chapter 11: Managing, Deploying, and Scaling Machine Learning Pipelines with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 357}), Document(page_content='There are a few open source libraries, such as MLeap  and ONNX , that can help you\\nautomatically export a supported subset of the MLlib models to remove their depend‐\\nency on Spark. However, as of the time of this writing the company that developed\\nMLeap is no longer supporting it. Nor does MLeap yet support Scala 2.12/Spark 3.0.\\nONNX (Open Neural Network Exchange), on the other hand, has become the de\\nfacto open standard for machine learning interoperability. Some of you might recall\\nother ML interoperability formats, like PMML (Predictive Model Markup Language),\\nbut those never gained quite the same traction as ONNX has now. ONNX is very\\npopular in the deep learning community as a tool that allows developers to easily\\nswitch between libraries and languages, and at the time of this writing it has experi‐\\nmental support for MLlib.\\nInstead of exporting MLlib models, there are other third-party libraries that integrate\\nwith Spark that are convenient to deploy in real-time scenarios, such as XGBoost  and\\nH2O.ai’s Sparkling Water  (whose name is derived from a combination of H2O and\\nSpark).\\nXGBoost is one of the most successful algorithms  in Kaggle competitions  for struc‐\\ntured data problems, and it’s a very popular library among data scientists. Although\\nXGBoost is not technically part of MLlib, the XGBoost4J-Spark library  allows you to\\nintegrate distributed XGBoost into your MLlib pipelines. A benefit of XGBoost is the\\nease of deployment: after you train your MLlib pipeline, you can extract the XGBoost\\nmodel and save it as a non-Spark model for serving in Python, as demonstrated here:\\n// In Scala\\nval xgboostModel  = \\n  xgboostPipelineModel .stages.last.asInstanceOf [XGBoostRegressionModel ]\\nxgboostModel .nativeBooster .saveModel (nativeModelPath )\\n# In Python\\nimport xgboost as xgb\\nbst = xgb.Booster({\\'nthread\\' : 4})\\nbst.load_model (\"xgboost_native_model\" )\\nAt the time of this writing, the distributed XGBoost API is only\\navailable in Java/Scala. A full example is included in the book’s Git‐\\nHub repo .\\nNow that you have learned about the different ways of exporting MLlib models for\\nuse in real-time serving environments, let’s discuss how we can leverage Spark for\\nnon-MLlib models.\\nModel Deployment Options with MLlib | 335', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 358}), Document(page_content='Leveraging Spark for Non-MLlib Models\\nAs mentioned previously, MLlib isn’t always the best solution for your machine learn‐\\ning needs. It may not meet super low-latency inference requirements or have built-in\\nsupport for the algorithm you’ d like to use. For these cases, you can still leverage\\nSpark, but not MLlib. In this section, we will discuss how you can use Spark to per‐\\nform distributed inference of single-node models using Pandas UDFs, perform\\nhyperparameter tuning, and scale feature engineering.\\nPandas UDFs\\nWhile MLlib is fantastic for distributed training of models, you are not limited to just\\nusing MLlib for making batch or streaming predictions with Spark—you can create\\ncustom functions to apply your pretrained models at scale, known as user-defined\\nfunctions (UDFs, covered in Chapter 5 ). A common use case is to build a scikit-learn\\nor TensorFlow model on a single machine, perhaps on a subset of your data, but per‐\\nform distributed inference on the entire data set using Spark.\\nIf you define your own UDF to apply a model to each record of your DataFrame in\\nPython, opt for pandas UDFs  for optimized serialization and deserialization, as dis‐\\ncussed in Chapter 5 . However, if your model is very large, then there is high overhead\\nfor the Pandas UDF to repeatedly load the same model for every batch in the same\\nPython worker process. In Spark 3.0, Pandas UDFs can accept an iterator of pan\\ndas.Series  or pandas.DataFrame  so that you can load the model only once instead\\nof loading it for every series in the iterator. For more details on what’s new in Apache\\nSpark 3.0 with Pandas UDFs, see Chapter 12 .\\nIf the workers cached the model weights after loading it for the first\\ntime, subsequent calls of the same UDF with the same model load‐\\ning will become significantly faster.\\nIn the following example, we will use mapInPandas() , introduced in Spark 3.0, to\\napply a scikit-learn  model to our Airbnb data set. mapInPandas()  takes an iterator\\nof pandas.DataFrame  as input, and outputs another iterator of pandas.DataFrame . It’s\\nflexible and easy to use if your model requires all of your columns as input, but it\\nrequires serialization/deserialization of the whole DataFrame (as it is passed to its\\ninput). Y ou can control the size of each pandas.DataFrame  with the spark.sql.execu\\ntion.arrow.maxRecordsPerBatch  config. A full copy of the code to generate the\\nmodel is available in this book’s GitHub repo , but here we will just focus on loading\\nthe saved scikit-learn  model from MLflow and applying it to our Spark\\nDataFrame:\\n336 | Chapter 11: Managing, Deploying, and Scaling Machine Learning Pipelines with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 359}), Document(page_content='# In Python\\nimport mlflow.sklearn\\nimport pandas as pd\\ndef predict(iterator ):\\n  model_path  = f\"runs:/{run_id}/random-forest-model\"\\n  model = mlflow.sklearn.load_model (model_path ) # Load model\\n  for features  in iterator :\\n    yield pd.DataFrame (model.predict(features ))\\n    \\ndf.mapInPandas (predict, \"prediction double\" ).show(3)\\n+-----------------+\\n|       prediction |\\n+-----------------+\\n| 90.4355866254844 |\\n|255.3459534312323 |\\n| 499.625544914651 |\\n+-----------------+\\nIn addition to applying models at scale using a Pandas UDF, you can also use them to\\nparallelize the process of building many models. For example, you might want to\\nbuild a model for each IoT device type to predict time to failure. Y ou can use\\npyspark.sql.GroupedData.applyInPandas()  (introduced in Spark 3.0) for this task.\\nThe function takes a pandas.DataFrame  and returns another pandas.DataFrame . The\\nbook’s GitHub repo contains a full example of the code to build a model per IoT\\ndevice type and track the individual models with MLflow; just a snippet is included\\nhere for brevity:\\n# In Python\\ndf.groupBy(\"device_id\" ).applyInPandas (build_model , schema=trainReturnSchema )\\nThe groupBy()  will cause a full shuffle of your data set, and you need to ensure\\nthat your model and the data for each group can fit on a single machine. Some\\nof you might be familiar with pyspark.sql.GroupedData.apply()  (e.g.,\\ndf.groupBy(\"device_id\").apply(build_model )), but that API will be deprecated in\\nfuture releases of Spark in favor of pyspark.sql.GroupedData.applyInPandas() .\\nNow that you have seen how to apply UDFs to perform distributed inference and\\nparallelize model building, let’s look at how to use Spark for distributed hyperpara‐\\nmeter tuning.\\nSpark for Distributed Hyperparameter Tuning\\nEven if you do not intend to do distributed inference or do not need MLlib’s dis‐\\ntributed training capabilities, you can still leverage Spark for distributed hyperpara‐\\nmeter tuning. This section will cover two open source libraries in particular: Joblib\\nand Hyperopt.\\nLeveraging Spark for Non-MLlib Models | 337', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 360}), Document(page_content='Joblib\\nAccording to its documentation, Joblib  is “a set of tools to provide lightweight pipe‐\\nlining in Python. ” It has a Spark backend to distribute tasks on a Spark cluster. Joblib\\ncan be used for hyperparameter tuning as it automatically broadcasts a copy of your\\ndata to all of your workers, which then create their own models with different hyper‐\\nparameters on their copies of the data. This allows you to train and evaluate multiple\\nmodels in parallel. Y ou still have the fundamental limitation that a single model and\\nall the data have to fit on a single machine, but you can trivially parallelize the hyper‐\\nparameter search, as shown in Figure 11-7 .\\nFigure 11-7. Distributed hyperparameter search\\nTo use Joblib, install it via pip install joblibspark . Ensure you are using scikit-\\nlearn  version 0.21 or later and pyspark  version 2.4.4 or later. An example of how to\\ndo distributed cross-validation is shown here, and the same approach will work for\\ndistributed hyperparameter tuning as well:\\n# In Python\\nfrom sklearn.utils  import parallel_backend\\nfrom sklearn.ensemble  import RandomForestRegressor\\nfrom sklearn.model_selection  import train_test_split\\nfrom sklearn.model_selection  import GridSearchCV\\nimport pandas as pd\\nfrom joblibspark  import register_spark\\nregister_spark () # Register Spark backend\\ndf = pd.read_csv (\"/dbfs/databricks-datasets/learning-spark-v2/sf-airbnb/\\n  sf-airbnb-numeric.csv \")\\nX_train, X_test, y_train, y_test = train_test_split (df.drop([\"price\"], axis=1), \\n  df[[\"price\"]].values.ravel(), random_state =42)\\nrf = RandomForestRegressor (random_state =42)\\nparam_grid  = {\"max_depth\" : [2, 5, 10], \"n_estimators\" : [20, 50, 100]}\\n338 | Chapter 11: Managing, Deploying, and Scaling Machine Learning Pipelines with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 361}), Document(page_content='gscv = GridSearchCV (rf, param_grid , cv=3)\\nwith parallel_backend (\"spark\", n_jobs=3):\\n  gscv.fit(X_train, y_train)\\n  \\nprint(gscv.cv_results_ )\\nSee the scikit-learn  GridSearchCV documentation  for an explanation of the\\nparameters returned from the cross-validator.\\nHyperopt\\nHyperopt  is a Python library for “serial and parallel optimization over awkward\\nsearch spaces, which may include real-valued, discrete, and conditional dimensions. ”\\nY ou can install it via pip install hyperopt . There are two main ways to scale\\nHyperopt with Apache Spark :\\n•Using single-machine Hyperopt with a distributed training algorithm (e.g.,\\nMLlib)\\n•Using distributed Hyperopt with single-machine training algorithms with the\\nSparkTrials  class\\nFor the former case, there is nothing special you need to configure to use MLlib with\\nHyperopt versus any other library. So, let’s take a look at the latter case: distributed\\nHyperopt with single-node models. Unfortunately, you can’t combine distributed\\nhyperparameter evaluation with distributed training models at the time of this writ‐\\ning. The full code example for parallelizing the hyperparameter search for a Keras\\nmodel can be found in the book’s GitHub repo ; just a snippet is included here to illus‐\\ntrate the key components of Hyperopt:\\n# In Python\\nimport hyperopt\\nbest_hyperparameters  = hyperopt .fmin(\\n  fn = training_function ,\\n  space = search_space ,\\n  algo = hyperopt .tpe.suggest,\\n  max_evals  = 64,\\n  trials = hyperopt .SparkTrials (parallelism =4))\\nfmin()  generates new hyperparameter configurations to use for your training_func\\ntion  and passes them to SparkTrials . SparkTrials  runs batches of these training\\ntasks in parallel as a single-task Spark job on each Spark executor. When the Spark\\ntask is done, it returns the results and the corresponding loss to the driver. Hyperopt\\nuses these new results to compute better hyperparameter configurations for future\\ntasks. This allows for massive scale-out of hyperparameter tuning. MLflow also\\nLeveraging Spark for Non-MLlib Models | 339', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 362}), Document(page_content='integrates with Hyperopt, so you can track the results of all the models you’ve trained\\nas part of your hyperparameter tuning.\\nAn important parameter for SparkTrials  is parallelism . This determines the maxi‐\\nmum number of trials to evaluate concurrently. If parallelism=1 , then you are train‐\\ning each model sequentially, but you might get better models by making full use of\\nadaptive algorithms. If you set parallelism=max_evals  (the total number of models\\nto train), then you are just doing a random search. Any number between 1 and\\nmax_evals  allows you to have a trade-off between scalability and adaptiveness. By\\ndefault, parallelism  is set to the number of Spark executors. Y ou can also specify a\\ntimeout  to limit the maximum number of seconds that fmin()  is allowed to take.\\nEven if MLlib isn’t suitable for your problem, hopefully you can see the value of using\\nSpark in any of your machine learning tasks.\\nKoalas\\nPandas  is a very popular data analysis and manipulation library in Python, but it is\\nlimited to running on a single machine. Koalas  is an open source library that imple‐\\nments the Pandas DataFrame API on top of Apache Spark, easing the transition from\\nPandas to Spark. Y ou can install it with pip install koalas , and then simply replace\\nany pd (Pandas) logic in your code with ks (Koalas). This way, you can scale up your\\nanalyses with Pandas without needing to entirely rewrite your codebase in PySpark.\\nHere is an example of how to change your Pandas code to Koalas (you’ll need to have\\nPySpark already installed):\\n# In pandas\\nimport pandas as pd\\npdf = pd.read_csv(csv_path, header=0, sep=\";\", quotechar=\\'\"\\')\\npdf[\"duration_new\"] = pdf[\"duration\"] + 100\\n# In koalas\\nimport databricks.koalas as ks\\nkdf = ks.read_csv(file_path, header=0, sep=\";\", quotechar=\\'\"\\')\\nkdf[\"duration_new\"] = kdf[\"duration\"] + 100\\nWhile Koalas aims to implement all Pandas features eventually, not all of them are\\nimplemented yet. If there is functionality that you need that Koalas does not provide,\\nyou can always switch to using the Spark APIs by calling kdf.to_spark() . Alterna‐\\ntively, you can bring the data to the driver by calling kdf.to_pandas()  and use the\\nPandas API (be careful the data set isn’t too large or you will crash the driver!).\\n340 | Chapter 11: Managing, Deploying, and Scaling Machine Learning Pipelines with Apache Spark', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 363}), Document(page_content='Summary\\nIn this chapter, we covered a variety of best practices for managing and deploying\\nmachine learning pipelines. Y ou saw how MLflow can help you track and reproduce\\nexperiments and package your code and its dependencies to deploy elsewhere. We\\nalso discussed the main deployment options—batch, streaming, and real-time—and\\ntheir associated trade-offs. MLlib is a fantastic solution for large-scale model training\\nand batch/streaming use cases, but it won’t beat a single-node model for real-time\\ninference on small data sets. Y our deployment requirements directly impact the types\\nof models and frameworks that you can use, and it is critical to discuss these require‐\\nments before you begin your model building process.\\nIn the next chapter, we will highlight a handful of key new features in Spark 3.0 and\\nhow you can incorporate them into your Spark workloads.\\nSummary | 341', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 364}), Document(page_content='', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 365}), Document(page_content='CHAPTER 12\\nEpilogue: Apache Spark 3.0\\nAt the time we were writing this book, Apache Spark 3.0 had not yet been officially\\nreleased; it was still under development, and we got to work with Spark 3.0.0-\\npreview2. All the code samples in this book have been tested against Spark 3.0.0-\\npreview2, and they should work no differently with the official Spark 3.0 release.\\nWhenever possible in the chapters, where relevant, we mentioned when features were\\nnew additions or behaviors in Spark 3.0. In this chapter, we survey the changes.\\nThe bug fixes and feature enhancements are numerous, so for brevity, we highlight\\njust a selection of the notable changes and features pertaining to Spark components.\\nSome of the new features are, under the hood, advanced and beyond the scope of this\\nbook, but we mention them here so you can explore them when the release is gener‐\\nally available.\\nSpark Core and Spark SQL\\nLet’s first consider what’s new under the covers. A number of changes have been\\nintroduced in Spark Core and the Spark SQL engine to help speed up queries. One\\nway to expedite queries is to read less data using dynamic partition pruning. Another\\nis to adapt and optimize query plans during execution.\\nDynamic Partition Pruning\\nThe idea behind dynamic partition pruning (DPP)  is to skip over the data you don’t\\nneed in a query’s results. The typical scenario where DPP is optimal is when you are\\njoining two tables: a fact table (partitioned over multiple columns) and a dimension\\ntable (nonpartitioned), as shown in Figure 12-1 . Normally, the filter is on the nonpar‐\\ntitioned side of the table ( Date , in our case). For example, consider this common\\nquery over two tables, Sales  and Date :\\n343', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 366}), Document(page_content='-- In SQL\\nSELECT * FROM Sales JOIN ON Sales.date  = Date.date\\nFigure 12-1. Dynamic filter  is injected from the dimension table into the fact table\\nThe key optimization technique in DPP is to take the result of the filter from the\\ndimension table and inject it into the fact table as part of the scan operation to limit\\nthe data read, as shown in Figure 12-1 .\\nConsider a case where the dimension table is smaller than the fact table and we per‐\\nform a join, as shown in Figure 12-2 . In this case, Spark most likely will do a broad‐\\ncast join (discussed in Chapter 7 ). During this join, Spark will conduct the following\\nsteps to minimize the amount of data scanned from the larger fact table:\\n1.On the dimension side of the join, Spark will build a hash table from the dimen‐\\nsion table, also known as the build relation, as part of this filter query.\\n2.Spark will plug the result of this query into the hash table and assign it to a\\nbroadcast variable, which is distributed to all executors involved in this join\\noperation.\\n3.On each executor, Spark will probe the broadcasted hash table to determine what\\ncorresponding rows to read from the fact table.\\n4.Finally, Spark will inject this filter dynamically into the file scan operation of the\\nfact table and reuse the results from the broadcast variable. This way, as part of\\nthe file scan operation on the fact table, only the partitions that match the filter\\nare scanned and only the data needed is read.\\n344 | Chapter 12: Epilogue: Apache Spark 3.0', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 367}), Document(page_content='Figure 12-2. Spark injects a dimension table filter  into the fact table during a broadcast\\njoin\\nEnabled by default so that you don’t have to explicitly configure it, all this happens\\ndynamically when you perform joins between two tables. With the DPP optimization,\\nSpark 3.0 can work much better with star-schema queries.\\nAdaptive Query Execution\\nAnother way Spark 3.0 optimizes query performance is by adapting its physical exe‐\\ncution plan at runtime. Adaptive Query Execution (AQE)  reoptimizes and adjusts\\nquery plans based on runtime statistics collected in the process of query execution. It\\nattempts to to do the following at runtime:\\n•Reduce the number of reducers in the shuffle stage by decreasing the number of\\nshuffle partitions.\\n•Optimize the physical execution plan of the query, for example by converting a\\nSortMergeJoin  into a BroadcastHashJoin  where appropriate.\\n•Handle data skew during a join.\\nAll these adaptive measures take place during the execution of the plan at runtime, as\\nshown in Figure 12-3 . To use AQE in Spark 3.0, set the configuration\\nspark.sql.adaptive.enabled  to true .\\nSpark Core and Spark SQL | 345', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 368}), Document(page_content='Figure 12-3. AQE reexamines and reoptimizes the execution plan at runtime\\nThe AQE framework\\nSpark operations in a query are pipelined and executed in parallel processes, but a\\nshuffle or broadcast exchange breaks this pipeline, because the output of one stage is\\nneeded as input to the next stage (see “Step 3: Understanding Spark Application Con‐\\ncepts”  on page 25 in Chapter 2 ). These breaking points are called materialization\\npoints  in a query stage, and they present an opportunity to reoptimize and reexamine\\nthe query, as illustrated in Figure 12-4 .\\n346 | Chapter 12: Epilogue: Apache Spark 3.0', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 369}), Document(page_content='Figure 12-4. A query plan reoptimized in the AQE framework\\nSpark Core and Spark SQL | 347', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 370}), Document(page_content='Here are the conceptual steps the AQE framework iterates over, as depicted in this\\nfigure:\\n1.All the leaf nodes, such as scan operations, of each stage are executed.\\n2.Once the materialization point finishes executing, it’s marked as complete, and all\\nthe relevant statistics garnered during execution are updated in its logical plan.\\n3.Based on these statistics, such as number of partitions read, bytes of data read,\\netc., the framework runs the Catalyst optimizer again to understand whether it\\ncan:\\na.Coalesce the number of partitions to reduce the number of reducers to read\\nshuffle data.\\nb.Replace a sort merge join, based on the size of tables read, with a broadcast\\njoin.\\nc.Try to remedy a skew join.\\nd.Create a new optimized logical plan, followed by a new optimized physical\\nplan.\\nThis process is repeated until all the stages of the query plan are executed.\\nIn short, this reoptimization is done dynamically, as shown in Figure 12-3 , and the\\nobjective is to dynamically coalesce the shuffle partitions, decrease the number of\\nreducers needed to read the shuffle output data, switch join strategies if appropriate,\\nand remedy any skew joins.\\nTwo Spark SQL configurations dictate how AQE will reduce the number of reducers:\\n•spark.sql.adaptive.coalescePartitions.enabled  (set to true )\\n•spark.sql.adaptive.skewJoin.enabled  (set to true )\\nAt the time of writing, the Spark 3.0 community blog, documentation, and examples\\nhad not been published publicly, but by the time of publication they should have\\nbeen. These resources will enable you to get more detailed information if you wish to\\nsee how these features work under the hood—including on how you can inject SQL\\njoin hints, discussed next.\\nSQL Join Hints\\nAdding to the existing BROADCAST  hints for joins, Spark 3.0 adds join hints for all\\nSpark join strategies  (see “ A Family of Spark Joins”  on page 187 in Chapter 7 ). Exam‐\\nples are provided here for each type of join.\\n348 | Chapter 12: Epilogue: Apache Spark 3.0', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 371}), Document(page_content='Shuffle  sort merge join (SMJ)\\nWith these new hints, you can suggest to Spark that it perform a SortMergeJoin\\nwhen joining tables a and b or customers  and orders , as shown in the following\\nexamples. Y ou can add one or more hints to a SELECT  statement inside /*+ ... */\\ncomment blocks:\\nSELECT /*+ MERGE(a, b) */ id FROM a JOIN b ON a.key = b.key\\nSELECT /*+ MERGE(customers, orders) */ * FROM customers, orders WHERE \\n    orders.custId = customers.custId\\nBroadcast hash join (BHJ)\\nSimilarly, for a broadcast hash join, you can provide a hint to Spark that you prefer a\\nbroadcast join. For example, here we broadcast table a to join with table b and table\\ncustomers  to join with table orders :\\nSELECT /*+ BROADCAST(a) */ id FROM a JOIN b ON a.key = b.key\\nSELECT /*+ BROADCAST(customers) */ * FROM customers, orders WHERE \\n    orders.custId = customers.custId\\nShuffle  hash join (SHJ)\\nY ou can offer hints in a similar way to perform shuffle hash joins, though this is less\\ncommonly encountered than the previous two supported join strategies:\\nSELECT /*+ SHUFFLE_HASH(a, b) */ id FROM a JOIN b ON a.key = b.key\\nSELECT /*+ SHUFFLE_HASH(customers, orders) */ * FROM customers, orders WHERE \\n    orders.custId = customers.custId\\nShuffle-and-replicate  nested loop join (SNLJ)\\nFinally, the shuffle-and-replicate nested loop join adheres to a similar form and\\nsyntax:\\nSELECT /*+ SHUFFLE_REPLICATE_NL(a, b) */ id FROM a JOIN b\\nCatalog Plugin API and DataSourceV2\\nNot to be confined only to the Hive metastore and catalog, Spark 3.0’s experimental\\nDataSourceV2 API extends the Spark ecosystem and affords developers three core\\ncapabilities. Specifically, it:\\n•Enables plugging in an external data source for catalog and table management\\n•Supports predicate pushdown to additional data sources with supported file for‐\\nmats like ORC, Parquet, Kafka, Cassandra, Delta Lake, and Apache Iceberg.\\n•Provides unified APIs for streaming and batch processing of data sources for\\nsinks and sources\\nSpark Core and Spark SQL | 349', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 372}), Document(page_content='Aimed at developers who want to extend Spark’s ability to use external sources and\\nsinks, the Catalog API provides both SQL and programmatic APIs to create, alter,\\nload, and drop tables from the specified pluggable catalog. The catalog provides a\\nhierarchical abstraction of functionalities and operations performed at different lev‐\\nels, as shown in Figure 12-5 .\\nFigure 12-5. Catalog plugin API’s hierarchical level of functionality\\nThe initial interaction between Spark and a specific connector is to resolve a relation\\nto its actual Table  object. Catalog  defines how to look up tables in this connector.\\nAdditionally, Catalog  can define how to modify its own metadata, thus enabling\\noperations  like CREATE TABLE , ALTER TABLE , etc.\\nFor example, in SQL you can now issue commands to create namespaces for your cat‐\\nalog. To use a pluggable catalog, enable the following configs in your spark-\\ndefaults.conf  file:\\nspark.sql.catalog.ndb_catalog com.ndb.ConnectorImpl # connector implementation\\nspark.sql.catalog.ndb_catalog.option1  value1\\nspark.sql.catalog.ndb_catalog.option2  value2\\nHere, the connector to the data source catalog has two options: option1->value1  and\\noption2->value2 . Once they’ve been defined, application users in Spark or SQL can\\nuse the DataFrameReader  and DataFrameWriter  API methods or Spark SQL com‐\\nmands with these defined options as methods for data source manipulation. For\\nexample:\\n-- In SQL\\nSHOW TABLES ndb_catalog ;\\nCREATE TABLE ndb_catalog .table_1;\\nSELECT * from ndb_catalog .table_1;\\nALTER TABLE ndb_catalog .table_1\\n// In Scala \\ndf.writeTo(\"ndb_catalog.table_1\" )\\nval dfNBD = spark.read.table(\"ndb_catalog.table_1\" )\\n  .option(\"option1\" , \"value1\" )\\n  .option(\"option2\" , \"value2\" )\\n350 | Chapter 12: Epilogue: Apache Spark 3.0', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 373}), Document(page_content='While these catalog plugin APIs extend Spark’s ability to utilize external data sources\\nas sinks and sources, they are still experimental and should not be used in produc‐\\ntion. A detailed guide to their use is beyond the scope of this book, but we encourage\\nyou to check the release documentation for additional information if you want to\\nwrite a custom connector to an external data source as a catalog to manage your\\nexternal tables and their associated metadata.\\nThe preceding code snippets are examples of what your code may\\nlook like after you have defined and implemented your catalog\\nconnectors and populated them with data.\\nAccelerator-Aware Scheduler\\nProject Hydrogen , a community initiative to bring AI and big data together, has three\\nmajor goals: implementing barrier execution mode, accelerator-aware scheduling,\\nand optimized data exchange. A basic implementation of barrier execution mode  was\\nintroduced in Apache Spark 2.4.0. In Spark 3.0, a basic scheduler  has been imple‐\\nmented to take advantage of hardware accelerators such as GPUs on target platforms\\nwhere Spark is deployed in standalone mode, YARN, or Kubernetes.\\nFor Spark to take advantage of these GPUs in an organized way for specialized work‐\\nloads that use them, you have to specify the hardware resources available via configs.\\nY our application can then discover them with the help of a discovery script. Enabling\\nGPU use is a three-step process in your Spark application:\\n1.Write a discovery script that discovers the addresses of the underlying GPUs\\navailable on each Spark executor. This script is set in the following Spark\\nconfiguration:\\nspark.worker.resource.gpu.discoveryScript= /path/to/script.sh\\n2.Set up configuration for your Spark executors to use these discovered GPUs:\\nspark.executor.resource.gpu.amount=2\\nspark.task.resource.gpu.amount=1\\n3.Write RDD code to leverage these GPUs for your task:\\nimport org.apache.spark.BarrierTaskContext\\nval rdd = ...\\nrdd.barrier.mapPartitions  { it =>\\n  val context = BarrierTaskContext .getcontext .barrier()\\n  val gpus = context.resources ().get(\"gpu\").get.addresses\\n  // launch external process that leverages GPU\\n  launchProcess (gpus)\\n}\\nSpark Core and Spark SQL | 351', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 374}), Document(page_content='These steps are still experimental, and further development will\\ncontinue in future Spark 3.x releases to support seamless discovery\\nof GPU resources, both at the command line (with spark-submit )\\nand at the Spark task level.\\nStructured Streaming\\nTo inspect how your Structured Streaming jobs fare with the ebb and flow of data\\nduring the course of execution, the Spark 3.0 UI has a new Structured Streaming tab\\nalongside the other tabs we explored in Chapter 7 . This tab offers two sets of statis‐\\ntics: aggregate information about completed streaming query jobs ( Figure 12-6 ) and\\ndetailed statistics about the streaming queries, including the input rate, process rate,\\nnumber of input rows, batch duration, and operation duration ( Figure 12-7 ).\\nFigure 12-6. Structured Streaming tab showing aggregate statistics of a completed\\nstreaming job\\nThe Figure 12-7  screenshot was taken with Spark 3.0.0-preview2;\\nwith the final release, you should see the query name and ID in the\\nname identifier on the UI page.\\n352 | Chapter 12: Epilogue: Apache Spark 3.0', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 375}), Document(page_content='Figure 12-7. Showing detailed statistics of a completed streaming job\\nNo configuration is required ; all configurations works straight out of the Spark 3.0\\ninstallation, with the following defaults:\\n•spark.sql.streaming.ui.enabled=true\\n•spark.sql.streaming.ui.retainedProgressUpdates=100\\nStructured Streaming | 353', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 376}), Document(page_content='•spark.sql.streaming.ui.retainedQueries=100\\nPySpark, Pandas UDFs, and Pandas Function APIs\\nSpark 3.0 requires pandas version 0.23.2 or higher to employ any pandas-related\\nmethods, such as DataFrame.toPandas()  or SparkSession.createDataFrame(pan\\ndas.DataFrame) .\\nFurthermore, it requires PyArrow version 0.12.1 or later to use PyArrow functional‐\\nity such as pandas_udf() , DataFrame.toPandas() , and SparkSession.createData\\nFrame(pandas.DataFrame)  with the spark.sql.execution.arrow.enabled\\nconfiguration set to true . The next section will introduce new features in Pandas\\nUDFs.\\nRedesigned Pandas UDFs with Python Type Hints\\nThe Pandas UDFs in Spark 3.0 were redesigned by leveraging Python type hints . This\\nenables you to naturally express UDFs without requiring the evaluation type. Pandas\\nUDFs are now more “Pythonic” and can themselves define what the UDF is supposed\\nto input and output, rather than you specifying it via, for example, @pan\\ndas_udf(\"long\", PandasUDFType.SCALAR)  as you did in Spark 2.4.\\nHere’s an example:\\n# Pandas UDFs in Spark 3.0\\nimport pandas as pd\\nfrom pyspark.sql.functions import pandas_udf\\n@pandas_udf(\"long\")\\ndef pandas_plus_one(v: pd.Series) -> pd.Series:\\n  return v + 1\\nThis new format provides several benefits, such as easier static analysis. Y ou can apply\\nthe new UDFs in the same way as before:\\n354 | Chapter 12: Epilogue: Apache Spark 3.0', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 377}), Document(page_content='df = spark.range(3)\\ndf.withColumn(\"plus_one\", pandas_plus_one(\"id\")).show()\\n+---+--------+\\n| id|plus_one|\\n+---+--------+\\n|  0|       1|\\n|  1|       2|\\n|  2|       3|\\n+---+--------+\\nIterator Support in Pandas UDFs\\nPandas UDFs are very commonly used to load a model and perform distributed\\ninference for single-node machine learning and deep learning models. However, if a\\nmodel is very large, then there is high overhead for the Pandas UDF to repeatedly\\nload the same model for every batch in the same Python worker process.\\nIn Spark 3.0, Pandas UDFs can accept an iterator  of pandas.Series  or pandas.Data\\nFrame , as shown here:\\nfrom typing import Iterator      \\n@pandas_udf(\\'long\\')\\ndef pandas_plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\\n    return map(lambda s: s + 1, iterator)\\ndf.withColumn(\"plus_one\", pandas_plus_one(\"id\")).show()\\n+---+--------+\\n| id|plus_one|\\n+---+--------+\\n|  0|       1|\\n|  1|       2|\\n|  2|       3|\\n+---+--------+\\nWith this support, you can load the model only once instead of loading it for every\\nseries in the iterator. The following pseudocode illustrates how to do this:\\n@pandas_udf(...)\\ndef predict(iterator):\\n  model = ... # load model\\n  for features in iterator:\\n    yield model.predict(features)\\nPySpark, Pandas UDFs, and Pandas Function APIs | 355', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 378}), Document(page_content='New Pandas Function APIs\\nSpark 3.0 introduces a few new types of Pandas UDFs that are useful when you want\\nto apply a function against an entire DataFrame instead of column-wise, such as\\nmapInPandas() , introduced in Chapter 11 . These take an iterator of pandas.Data\\nFrame  as input and output another iterator of pandas.DataFrame :\\ndef pandas_filter(\\n    iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\\n  for pdf in iterator:\\n    yield pdf[pdf.id == 1]\\ndf.mapInPandas(pandas_filter, schema=df.schema).show()\\n+---+\\n| id|\\n+---+\\n|  1|\\n+---+\\nY ou can control the size of the pandas.DataFrame  by specifying it in the\\nspark.sql.execution.arrow.maxRecordsPerBatch  configuration. Note that the\\ninput size and output size do not have to match, unlike with most Pandas UDFs.\\nAll the data of a cogroup will be loaded into memory, which means\\nif there is data skew or certain groups are too big to fit in memory\\nyou could run into OOM issues.\\nSpark 3.0 also introduces cogrouped map Pandas UDFs. The applyInPandas()  func‐\\ntion takes two pandas.DataFrame s that share a common key and applies a function to\\neach cogroup. The returned pandas.DataFrame s are then combined as a single Data‐\\nFrame. As with mapInPandas() , there is no restriction on the length of the returned\\npandas.DataFrame . Here’s an example:\\ndf1 = spark.createDataFrame(\\n    [(1201, 1, 1.0), (1201, 2, 2.0), (1202, 1, 3.0), (1202, 2, 4.0)],\\n    (\"time\", \"id\", \"v1\"))\\ndf2 = spark.createDataFrame(\\n    [(1201, 1, \"x\"), (1201, 2, \"y\")], (\"time\", \"id\", \"v2\"))\\ndef asof_join(left: pd.DataFrame, right: pd.DataFrame) -> pd.DataFrame:\\n    return pd.merge_asof(left, right, on=\"time\", by=\"id\")\\ndf1.groupby(\"id\").cogroup(\\n    df2.groupby(\"id\")\\n).applyInPandas(asof_join, \"time int, id int, v1 double, v2 string\").show()\\n356 | Chapter 12: Epilogue: Apache Spark 3.0', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 379}), Document(page_content='+----+---+---+---+\\n|time| id| v1| v2|\\n+----+---+---+---+\\n|1201|  1|1.0|  x|\\n|1202|  1|3.0|  x|\\n|1201|  2|2.0|  y|\\n|1202|  2|4.0|  y|\\n+----+---+---+---+\\nChanged Functionality\\nListing all the functionality changes in Spark 3.0 would transform this book into a\\nbrick several inches thick. So, in the interest of brevity, we will mention a few notable\\nones here, and leave you to consult the release notes for Spark 3.0 for full details and\\nall the nuances as soon as they are available.\\nLanguages Supported and Deprecated\\nSpark 3.0 supports Python 3 and JDK 11, and Scala version 2.12 is required. All\\nPython versions earlier than 3.6 and Java 8 are deprecated. If you use these depre‐\\ncated versions you will get warning messages.\\nChanges to the DataFrame and Dataset APIs\\nIn previous versions of Spark, the Dataset and DataFrame APs had deprecated the\\nunionAll()  method. In Spark 3.0 this has been reversed, and unionAll()  is now an\\nalias to the union()  method.\\nAlso, earlier versions of Spark’s Dataset.groupByKey()  resulted in a grouped Dataset\\nwith the key spuriously named as value  when the key was a non-struct type ( int,\\nstring , array , etc.). As such, aggregation results from ds.groupByKey().count()  in\\nthe query when displayed looked, counterintuitively, like (value, count) . This has\\nbeen rectified to result in (key, count) , which is more intuitive. For example:\\n//  In Scala\\nval ds = spark.createDataset (Seq(20, 3, 3, 2, 4, 8, 1, 1, 3))\\nds.show(5)\\n+-----+\\n|value|\\n+-----+\\n|   20|\\n|    3|\\n|    3|\\n|    2|\\n|    4|\\n+-----+\\nChanged Functionality | 357', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 380}), Document(page_content='ds.groupByKey (k=> k).count.show(5)\\n+---+--------+\\n|key|count(1)|\\n+---+--------+\\n|  1|       2|\\n|  3|       3|\\n| 20|       1|\\n|  4|       1|\\n|  8|       1|\\n+---+--------+\\nonly showing top 5 rows\\nHowever, you can preserve the old format if you prefer by setting spark.sql.leg\\nacy.dataset.nameNonStructGroupingKeyAsValue  to true .\\nDataFrame and SQL Explain Commands\\nFor better readability and formatting, Spark 3.0 introduces the Data\\nFrame.explain( FORMAT_MODE ) capability to display different views of the plans the\\nCatalyst optimizer generates. The FORMAT_MODE  options include \"simple\"  (the\\ndefault), \"extended\" , \"cost\" , \"codegen\" , and \"formatted\" . Here’s a simple\\nillustration:\\n// In Scala\\nval strings = spark\\n .read.text(\"/databricks-datasets/learning-spark-v2/SPARK_README.md\" )\\nval filtered  = strings.filter($\"value\".contains (\"Spark\"))\\nfiltered .count()\\n# In Python\\nstrings = spark\\n .read.text(\"/databricks-datasets/learning-spark-v2/SPARK_README.md\" )\\nfiltered  = strings.filter(strings.value.contains (\"Spark\"))\\nfiltered .count()\\n// In Scala\\nfiltered .explain(\"simple\" )\\n# In Python\\nfiltered .explain(mode=\"simple\" )\\n== Physical  Plan ==\\n*(1) Project [value#72]\\n+- *(1) Filter (isnotnull (value#72) AND Contains(value#72, Spark))\\n   +- FileScan  text [value#72] Batched: false, DataFilters: [isnotnull(value#72),\\nContains (value#72, Spark)], Format: Text, Location:\\nInMemoryFileIndex [dbfs:/databricks -datasets /learning -spark-v2/SPARK_README .md],\\nPartitionFilters : [], PushedFilters : [IsNotNull (value),\\nStringContains (value,Spark)], ReadSchema : struct<value:string>\\n// In Scala\\nfiltered .explain(\"formatted\" )\\n358 | Chapter 12: Epilogue: Apache Spark 3.0', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 381}), Document(page_content='# In Python\\nfiltered .explain(mode=\"formatted\" )\\n== Physical  Plan ==\\n* Project (3)\\n+- * Filter (2)\\n   +- Scan text  (1)\\n(1) Scan text  \\nOutput [1]: [value#72]\\nBatched: false\\nLocation : InMemoryFileIndex  [dbfs:/databricks -datasets /learning -spark-v2/...\\nPushedFilters : [IsNotNull (value), StringContains (value,Spark)]\\nReadSchema : struct<value:string>\\n     \\n(2) Filter [codegen id : 1]\\nInput [1]: [value#72]\\nCondition  : (isnotnull (value#72) AND Contains(value#72, Spark))\\n     \\n(3) Project [codegen id : 1]\\nOutput [1]: [value#72]\\nInput [1]: [value#72]\\n-- In SQL\\nEXPLAIN FORMATTED  \\nSELECT * \\nFROM tmp_spark_readme  \\nWHERE value like \"%Spark%\"\\n== Physical  Plan ==\\n* Project (3)\\n+- * Filter (2)\\n   +- Scan text  (1)\\n(1) Scan text \\nOutput [1]: [value#2016]\\nBatched: false\\nLocation : InMemoryFileIndex  [dbfs:/databricks -datasets /\\nlearning -spark-v2/SPARK_README .md]\\nPushedFilters : [IsNotNull (value), StringContains (value,Spark)]\\nReadSchema : struct<value:string>\\n(2) Filter [codegen id : 1]\\nInput [1]: [value#2016]\\nCondition  : (isnotnull (value#2016) AND Contains (value#2016, Spark))\\n(3) Project [codegen id : 1]\\nOutput [1]: [value#2016]\\nInput [1]: [value#2016]\\nTo see the rest of the format modes in action, you can try the notebook in the book’s\\nGitHub repo . Also check out the migration guides  from Spark 2.x to Spark 3.0.\\nChanged Functionality | 359', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 382}), Document(page_content='Summary\\nThis chapter provided a cursory highlight of new features in Spark 3.0. We took the\\nliberty of mentioning a few advanced features that are worthy of note. They operate\\nunder the hood and not at the API level. In particular, we took a look at dynamic par‐\\ntition pruning (DPP) and adaptive query execution (AQE), two optimizations that\\nenhance Spark’s performance at execution time. We also explored how the experi‐\\nmental Catalog API extends the Spark ecosystem to custom data stores for sources\\nand sinks for both batch and streaming data, and looked at the new scheduler in\\nSpark 3.0 that enables it to take advantage of GPUs in executors.\\nComplementing our discussion of the Spark UI in Chapter 7 , we also showed you the\\nnew Structured Streaming tab, providing accumulated statistics on streaming jobs,\\nadditional visualizations, and detailed metrics on each query.\\nPython versions below 3.6 are deprecated in Spark 3.0, and Pandas UDFs have been\\nredesigned to support Python type hints and iterators as arguments. There are Pandas\\nUDFs that enable transforming an entire DataFrame, as well as combining two\\ncogrouped DataFrames into a new DataFrame.\\nFor better readability of query plans, DataFrame.explain( FORMAT_MODE ) and\\nEXPLAIN FORMAT_MODE  in SQL display different levels and details of logical and physi‐\\ncal plans. Additionally, SQL commands can now take join hints for Spark’s entire sup‐\\nported family of joins.\\nWhile we were unable to enumerate all the changes in the latest version of Spark in\\nthis short chapter, we urge that you explore the release notes when Spark 3.0 is\\nreleased to find out more. Also, for a quick summary of the user-facing changes and\\ndetails on how to migrate to Spark 3.0, we encourage you to check out the migration\\nguides.\\nAs a reminder, all the code in this book has been tested on Spark 3.0.0-preview2 and\\nshould work with Spark 3.0 when it is officially released. We hope you’ve enjoyed\\nreading this book and learned from this journey with us. We thank you for your\\nattention!\\n360 | Chapter 12: Epilogue: Apache Spark 3.0', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 383}), Document(page_content='Index\\nA\\nAccelerator-Aware Scheduler, 351\\nACM (Association for Computing Machinery),\\n6\\nactions, 28-30, 61\\nadding columns, 63, 152\\nagg(), 187\\naggregate(), 162\\naggregations, 66, 239-246\\nallowUnquotedFieldNames property, 101\\nAmazon S3, 89\\nAMPLab, 3\\nAnalysis phase (Spark SQL), 81\\nanalytic functions, 149\\nApache Arrow format, 115\\nApache Cassandra, 89, 137, 231\\nApache Hive, 89, 113-155\\nApache Hudi, 272\\nApache Iceberg, 272\\nApache Kafka\\nabout, 8, 15\\nreading from, 228\\nStructured Streaming and, 228-230\\nwriting to, 229\\nApache Mesos, 12, 177\\nApache Spark Meetup groups, 16\\nAppend mode (Structured Streaming), 212,\\n215, 245\\napplications, Spark\\nabout, 26\\nconcepts of, 25-28\\ndebugging, 204\\ndriver and executors, 10, 12\\ndriver programs, 10optimizing and tuning, 173-205\\nusing Spark SQL in, 84-89\\napproxQuantile() method, 68\\napprox_count_distinct(), 239\\nAQE (Adaptive Query Execution), 345-348\\narbitrary stateful computations, 253-261\\narray type functions, 139-141\\narrays_overlap() function, 139\\narray_distinct() function, 139\\narray_except() function, 139\\narray_intersect() function, 139\\narray_join() function, 139\\narray_max() function, 139\\narray_min() function, 139\\narray_position() function, 139\\narray_remove() function, 139\\narray_repeat() function, 139\\narray_sort() function, 139\\narray_union() function, 139\\narray_zip() function, 139\\nartifacts, 326\\nASF (Apache Software Foundation), 2\\nAST (abstract syntax tree), 81\\natomicity, of data lakes, 270\\nauditing data changes with operation history,\\n282\\navg() method, 67\\navro Schema property, 106\\nAvro, as a data source for DataFrames and SQL\\ntables, 104\\nAWS SageMaker, 334\\nAzure Cosmos DB, 134\\nAzure ML, 334\\n361', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 384}), Document(page_content='B\\nbagging, 313\\nbarrier execution mode, 351\\nbatch deployment, 332\\nBeeline, querying with, 119\\nBHJ (broadcast hash join), 188, 349\\nbig data, 1\\nBig table, 1\\nbin directory, 21, 21\\nbinary files, as a data source for DataFrames\\nand SQL tables, 110\\nbootstrapping samples, 313\\nbroadcast variables, 188, 344\\nbucketBy() method, 96\\nbuilt-in data sources, 83-112, 94\\nbuilt-in functions, 139-141, 239\\nbytecode, 7, 23, 25\\nC\\ncache(), 183-187\\ncaching, 93, 183-187\\ncardinality() function, 139\\ncase class, 71, 158\\nCASE statement, 152\\nCassandra, 89, 137, 231\\nCatalog API, 93, 349-351\\nCatalyst optimizer, xvi, 16, 77-82, 170\\nCBO (cost-based optimizer), 81\\nCDC (change-data-capture), 271\\ncheckpointing, 217, 262\\nclassification, 286-287, 292, 304\\nclause conditions, 281\\nclient mode, 12\\nclose() method, 233\\ncluster managers, 10, 12, 176, 178\\ncluster resource provisioning, 262\\nclustering, 286, 288, 302\\ncode examples, using, xviii\\nCode generation phase (Spark SQL), 81\\ncodegen, enabling in Spark SQL, 189\\ncogroup(), 356\\ncollect() method, 67\\ncollect_list(), 138\\ncollect_set(), 239\\nColumn object, 54\\ncolumns\\nadding, 63, 152\\ndropping, 63, 152\\nin DataFrames, 54random feature selection by, 313\\nrenaming, 63, 153\\ncomma-separated value files (CSV files), 102\\ncommunity adoption/expansion, of Spark, 16\\nComplete mode (Structured Streaming), 212,\\n215, 245\\ncomplex data types, 49, 139-141\\ncompression property, 101, 103, 106\\ncompute function, 44\\nconcat() function, 139\\nconf.spark-defaults.conf file, 173\\nconfigurations\\nsetting, 173-176\\nused in this book, xviii\\nviewing, 173-176\\nconfiguring Spark, with Delta Lake, 274\\nconsistency, of data lakes, 270\\ncontinuous applications, 15\\nContinuous mode (Structured Streaming), 217\\nContinuous Streaming model, 8\\nContinuous trigger mode, 219\\ncorrelation() method, 68\\ncosts\\nmitigating, 170\\nof databases, 267\\nof latency, 209\\ncount(), 29, 66, 183, 215\\ncountDistinct(), 239\\nCounting M&Ms example, 35-39\\ncovariance() method, 68\\nCrossValidator, 317\\nCSV files, as a data source for DataFrames and\\nSQL tables, 102\\ncube() function, 235\\ncubed() function, 116\\ncustomStateUpdateFunction(), 254\\nD\\nDAG (directed acyclic graph), 4, 27\\ndata\\naccommodating changing, 279\\nauditing changes with operation history,\\n282\\ndeduplicating, 281\\ndiversity of formats for storage solutions,\\n265\\ngovernance of, as a feature of lakehouses,\\n271\\ngrowth in size of, 267\\n362 | Index', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 385}), Document(page_content='loading into Delta Lake tables, 275\\ntransforming, 214, 279-282\\nupdating, 280\\ndata corruption, enforcing schema on write to\\nprevent, 278-279\\ndata directory, 21\\ndata engineering tasks, 15\\ndata engineers, xv, 15\\ndata evolution, 323\\ndata ingestion, 290\\ndata lakes, 265-284\\nabout, 268\\ndatabases, 266-268\\nlakehouses, 271\\nlimitations of, 270\\noptimal storage solutions, 265\\nreading from, 269\\nwriting to, 269\\ndata science tasks, 14\\ndata scientists, xv\\ndata sources\\nabout, 89\\nbuilt-in, 94\\nfor DataFrames, 94-112\\nfor SQL tables, 94-112\\nstreaming, 226-234\\nData Sources API, 94\\ndata type\\nabout, 48\\ncomplex, 49\\nstructured, 49\\nsupport for diverse, as a feature of lake‐\\nhouses, 271\\ndatabases\\nabout, 266\\nlimitations of, 267\\nreading from, 267\\nwriting from, 267\\nDatabricks Community Edition, xviii , 34\\nDataFrame API\\nabout, 16, 47\\ncolumns, 54\\ncommon operations, 58-68\\ncreating DataFrames, 50-54\\ndata types, 48\\nexample of, 68\\nexpressions, 54\\nrows, 57\\nschemas, 50-54structured data types, 49\\nDataFrame.cache(), 183\\nDataFrame.persist(), 184\\nDataFrameReader\\nabout, 5\\nas a data source for DataFrames and SQL\\ntables, 94\\nusing, 58-60\\nDataFrames, 144-155\\nchanges to, 357\\ncompared with Datasets, 74\\nconverting to Datasets, 166\\ncreating, 50-54\\ndata sources for, 94-112\\nhigher-order functions in, 138-144\\nlazy evaluation and, 29\\nmemory management for, 167\\nreading Avro files into, 104\\nreading binary files into, 110\\nreading CSV files into, 102\\nreading data into, 93\\nreading image files into, 108\\nreading JSON files into, 100\\nreading ORC files into, 107\\nreading Parquet files into, 97\\nSQL explain commands, 358\\nstreaming, 214\\ntransformations, 222\\nwriting into Avro files, 105\\nwriting into CSV files, 103\\nwriting into JSON files, 101\\nwriting to ORC files, 108\\nwriting to Parquet files, 99\\nwriting to Spark SQL tables, 99\\nDataFrameWriter\\nabout, 5\\nas a data source for DataFrames and SQL\\ntables, 96\\nusing, 58-60\\nDataset API\\nabout, 16, 69\\nchanges to, 357\\ncreating Datasets, 71\\nDataset operations, 72\\nexample of, 74\\ntyped objects, 69\\nDatasets\\ncompared with DataFrames, 74\\nconverting DataFrames to, 166\\nIndex | 363', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 386}), Document(page_content='costs of using, 170\\ncreating, 71\\nencoders, 168\\nJavaBeans for, 158\\nmemory management for, 167\\noperations, 72\\nsingle API, 157-160\\nSpark SQL and, 157-172\\nworking with, 160-167\\nDataSourceV2, 349-351\\ndateFormat property, 101, 103\\nday() function, 65\\nDDL (Data Definition Language) string, 51\\ndebugging Spark, 204\\ndecision trees, 308-313\\ndeduplicating data, 281\\ndelete actions, 271, 281\\ndeleting user-related data, 280\\nDelta Lake\\nabout, 273\\nbuilding lakehouses with Apache Spark and,\\n274-283\\nconfiguring Apache Spark with, 274\\nloading data into tables, 275\\nloading data streams into tables, 277\\nDenseVector, 298\\ndense_rank() function, 151\\ndependencies, 44\\ndeployment modes, 12, 330-335\\ndeprecated languages, 357\\ndescribe() method, 68\\ndevelopers, Spark and, 14-17\\ndirectories, 21\\ndiscretized streams, 9\\nDISK_ONLY storage level, 184\\ndistributed data, partitions and, 12\\ndistributed execution, 10-14\\ndistributed hyperparameter tuning, 337-340\\ndistributed state management, 236\\ndot (.) notation, 72\\ndownloading Spark, 19-22\\nDPP (dynamic partition pruning), 343-345\\ndriver, 10\\ndriver programs, 10\\ndrop() method, 64, 152\\ndropping columns, 63, 152\\nDropwizard Metrics, 224\\nDSL (domain-specific language), 44\\nDStream API, 209DStreams (discretized streams), 9\\ndynamic resource allocation, 177\\nE\\nease of use, of Spark, 5\\nThe Elements of Statistical Learning (Hastie,\\nTibshirani, and Friedman), 309\\nelement_at() function, 139, 139\\nencoders (Datasets), 168\\nencoding, one-hot, 297\\nend-to-end exactly-once guarantees, 221\\nensemble approach, 313\\nEnvironment tab (Spark UI), 203\\nerrors, fixing, 280\\nestimators, 290, 295\\nestimator_name.fit() method, 295\\nETL (extract, transform, and load), 15\\nevaluating models, 302-306\\nevaluation order, 115\\nevent-time timeouts, 259\\nevent-time windows, aggregations with,\\n239-246\\nexactly-once guarantees, 221\\nexamples file, 21\\nexecution, order of, 323\\nexecutors, 12\\nExecutors tab (Spark UI), 200\\nexists() function, 143\\nexperiments, 325\\nexplode() function, 138, 235\\nexport patterns, for real-time inference, 334\\nexpr() function, 145\\nexpressions, in DataFrames, 54\\nextensibility, of Spark, 5\\nexternal data sources, 113-155\\nApache Cassandra, 137\\nAzure Cosmos DB, 134\\ncommon DataFrames, 144-155\\nhigher-order functions in DataFrames and\\nSpark SQL, 138-144\\nJDBC database, 129-131\\nMongoDB, 137\\nMS SQL Server, 136\\nMySQL, 133\\nPostgreSQL, 132\\nSnowflake, 137\\nSpark SQL operations, 144-155\\nSQL database, 129-131\\nTableau, 122-129\\n364 | Index', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 387}), Document(page_content='F\\nfault tolerance, 2, 9, 15, 185, 209, 222\\nfault-tolerant state management, 236\\nfile formats\\nabout, 76\\nCSV files, 102\\ndata lakes and, 269\\nsupport for diversity of, 269\\nfiles\\nabout, 21\\nreading from, 226\\nStructured Streaming and, 226\\nwriting to, 227\\nfilesystems, 89, 269\\nfilter() method, 28, 29, 61, 72, 73, 143, 157, 162,\\n170, 215, 235\\nfiltering, DataFrames and, 61\\nfit() method, 296\\nfitting, 295\\nflatMap() method, 157, 170, 215, 235\\nflatMapGroupsWithState(), 253, 256, 261\\nflatten() function, 139\\nfmin(), 339\\nforeach() method, 216, 230, 233-234\\nforeachBatch() method, 216, 230, 281\\nformat() method, 94, 96, 100\\nfrequentItems() method, 68\\nFriedman, Jerome, The Elements of Statistical\\nLearning, 309\\nfrom_json() function, 138\\nfunctional programming, higher-order func‐\\ntions and, 162-167\\nfunctionality, changed, 357-359\\nG\\ngarbage collection, 167, 178, 199\\nGDPR (General Data Protection Regulation),\\n280\\ngeneralization, with flatMapGroupsWithState(),\\n261\\ngeneric rows, 69\\ngetter methods, 70\\nget_json_object() function, 138\\nGFS (Google File System), 1\\nGhemawat, Sanjay, The Google File System, 268\\nglobal aggregations, 238\\nglobal temporary views, 92\\nGobioff, Howard, The Google File System, 268\\nGoogle, 1The Google File System (Ghemawat, Gobioff,\\nand Leung), 268\\nGraphFrames, 9\\ngraphical user interface, 31\\nGraphX library, 6, 9\\nGridSearchCV, 339\\nGROUP BY statement, 138\\ngroupBy() method, 30, 66, 73, 157, 182, 187,\\n244, 337\\ngroupByKey(), 254, 256\\ngrouped aggregate Pandas UDFs, 116\\ngrouped aggregations, 238\\ngrouped map Pandas UDFs, 116\\nH\\nHadoop, 2, 268\\nHadoop YARN, 12\\nHastie, Trevor, The Elements of Statistical\\nLearning, 309\\nHBase, 5\\nHDFS (Hadoop Distributed File System), 2, 268\\nhigh-level structured APIs, 25\\nhigher-order functions, 138-144, 162-167\\nHive, 89, 113-155\\nHive ORC SerDe (serialization and deserializa‐\\ntion) tables, 107\\nHiveContext object, 11\\nHiveServer2, 120\\nHyperopt, 339\\nhyperparameter configurations, 317\\nhyperparameter tuning\\nabout, 307\\ndistributed, 337-340\\nk-fold cross-validation, 316-319\\noptimizing pipelines, 320-321\\ntree-based models, 307-316\\nI\\nid column (StreamingQuery), 224\\nignoreExtension property, 106\\nimages, as a data source for DataFrames and\\nSQL tables, 108\\nincremental execution, 234\\nincrementalization, 211\\ninferSchema property, 103\\ninner joins, 248-252\\ninput and output sources\\ndefining, 213\\nfile formats\\nIndex | 365', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 388}), Document(page_content='about, 76\\nCSV files, 102\\ndata lakes and, 269\\nsupport for diverse workloads, 269\\ninputRowsPerSecond column (Streaming‐\\nQuery), 224\\ninstalling R, 21\\ninteractive groups, managing using timeouts,\\n257-261\\ninteractive shell, 274\\nisolation, of data lakes, 270\\niterator support, in Pandas UDFs, 355\\nJ\\nJava, 55, 157-160\\nJava Serialization, 184\\njava.op.Serializable, 158\\nJavaBean class, 71\\nJavaBeans, for Datasets, 158\\nJDBC database, 89, 129-131\\nJDK (Java Development Kit), 41\\nJoblib, 338\\njobs, 26, 27\\nJobs tab (Spark UI), 198\\njoin operations\\nabout, 187\\nbroadcast hash join (BHJ), 188\\nshuffle sort merge (SMJ), 189-197\\njoin(), 182, 187\\njoins\\nabout, 148\\nBHJ (broadcast hash join), 349\\nSHJ (shuffle hash join), 349\\nSMJ (shuffle sort merge join), 349\\nSNLJ (shuffle-and-replicate nested loop\\njoin), 349\\nSpark SQL, 348\\nJSON (JavaScript Object Notation) files\\nabout, 53, 100\\nas a data source for DataFrames and SQL\\ntables, 100\\nK\\nk-fold cross-validation, 316-319\\nKafka Integration Guide, 229\\nKaggle, 335\\nKarau, Holden, xv\\nKay, Alan, 3\\nKoalas, 340Konwinski, Andy, xv\\nKryo serialization library, 168, 170, 184\\nKubernetes, 12, 21\\nKuo, Kevin, Mastering Spark with R, xvi, 285\\nL\\nlakehouses\\nApache Hudi, 272\\nApache Iceberg, 272\\nbuilding with Apache Spark and Delta Lake,\\n274-283\\nDelta Lake, 273\\nfeatures of , 271\\nlambdas, 170\\nlanguages, 357\\nlatency, 209, 330\\nlazy evaluation, 28-30\\nleaf node, of decision trees, 308\\nlearning, 295\\nLending Club Loan Data, 274\\nLeung, Shun-Tak, The Google File System, 268\\nlibrary versioning, 323\\nlineage, 29\\nlinear regression, 294\\nLinearRegressionModel, 295, 306\\nLinux Foundation, 273\\nload() method, 94\\nloading and saving data\\ndata streams into Delta Lake tables, 277\\nfile formats\\nabout, 76\\nCSV files, 102\\ndata lakes and, 269\\nsupport for diversity of, 269\\nfilesystems, 89, 269\\ninto Delta Lake tables, 275\\nstructured data, using Spark, 91\\nloading models, 306\\nlocal machine, 23\\nlog-normally distributed, 305\\nLog4j.properties.template, 37\\nlogging, 325, 327\\nLogical optimization phase (Spark SQL), 81\\nlogistic regression, 287\\nLogisticRegressionModel, 306\\nlowerBound property, 130\\nlr.fit() method, 296\\nLuraschi, Javier, Mastering Spark with R, xvi,\\n285\\n366 | Index', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 389}), Document(page_content='M\\nmachine learning (ML)\\nabout, 286\\nbuilding models using estimators, 295\\ncreating pipelines, 296-302\\ncreating test data sets, 291-293\\ncreating training data sets, 291-293\\ndata ingestion, 290\\ndesigning pipelines, 289-307\\nevaluating models, 302-306\\nexploration, 290\\nhyperparameter tuning, 307-321\\nlinear regression, 294\\nloading models, 306\\nreasons for using Spark, 289\\nsaving models, 306\\nsupervised, 286\\nunsupervised, 288\\nwith MLlib, 285-321\\nmachine learning engineers, xv\\nmanaged stateful transformations, 237\\nmanaged tables, 89\\nmap functions, 139\\nmap() method, 73, 138, 157, 162, 163, 170, 215,\\n235\\nmap-side-only join, 188\\nmapGroupsWithState(), 253, 256, 261\\nmapInPandas() method, 336, 356\\nmapPartitions(), 119\\nmap_concat() function, 139\\nmap_form_arrays() function, 139\\nmap_from_entries() function, 139\\nMastering Spark with R (Luraschi, Kuo, and\\nRuiz), xvi, 285\\nMatrix object, 292\\nMaven, 133, 134\\nmax() method, 67\\nmean() method, 239\\nmemory management, for Datasets and Data‐\\nFrames, 167\\nMEMORY_AND_DISK storage level, 184\\nMEMORY_AND_DISK_SER storage level, 184\\nMEMORY_ONLY storage level, 184\\nMEMORY_ONLY_SER storage level, 184\\nmerge(), upserting change data to tables using,\\n281\\nMesos (see Apache Mesos)\\nmetadata, 93, 326\\nmetrics, 224, 326micro-batch architecture, Spark Streaming, 208\\nmin() method, 67\\nmitigating costs, 170\\nMLeap, 334\\nMLflow, 323, 324-330\\nMLflow Model Registry, 332\\nMLflow Models, 332\\nMLflow Projects, 330\\nMLflow Tracking, 325-330\\nMLlib library\\nabout, xv, 6, 7\\n(see also machine learning (ML))\\nmachine learning (ML) with, 285-321\\nmodel deployment options with, 330\\nmodel.transform(), 332\\nmodels\\nabout, 326\\nbuilding using estimators, 295\\nevaluating, 302-306\\nloading, 306\\nmanaging, 323-330\\nsaving, 306\\ntree-based, 307-316\\nmodels component (MLflow), 324\\nmodifications, 151-155\\nmodularity, of Spark, 5\\nMongoDB, 137\\nmonth() function, 65\\nMR (MapReduce), 1\\nMS SQL Server, 136\\nmultiline property, 101, 103\\nmultitenant environment, 177\\nMySQL database, 86, 133\\nN\\nNaive Bayes algorithm, 287\\nnarrow dependencies, 30\\nNetflix, 272\\nnon-MLlib models, leveraging Spark for,\\n336-340\\nnon-SQL based analytics, for databases, 268\\nnon-time based streaming aggregations, 238\\nnull checking, 115\\nnumInputRows column (StreamingQuery), 224\\nnumPartitions property, 130\\nNumPy, 14\\nO\\nobjects, 69\\nIndex | 367', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 390}), Document(page_content='off-Java heap memory, 168\\nOFF_HEAP storage level, 184\\nOHE (one-hot encoding), 297\\nOLAP (online analytical processing), 267\\nOLTP (online transaction processing), 267\\nOnce mode (Structured Streaming), 217\\nOneHotEncoder, 298\\nONNX (Open Neural Network Exchange), 334\\nOOM (out-of-memory) exceptions, 67\\nopen() method, 233\\nopenness, of storage solutions, 265\\noperation history, auditing data changes with,\\n282\\noptimizing Spark applications, 173-205\\noption() method, 94, 96, 100\\noptional actions, 281\\nORC, as a data source for DataFrames and SQL\\ntables, 106\\norder of execution, 323\\norderBy() method, 30, 66\\nouter joins, 252\\noutput modes (Structured Streaming), 212, 215,\\n245\\noutput sink, 215\\noverwrite(), 306\\nP\\nPageRank algorithm, 9\\nPandas\\nabout, 340\\nFunction APIs, 354, 356\\nuser-defined functions (UDFs), 115-117,\\n336, 354, 355\\nparallel operations, 323\\nparallelism parameter, 320\\nparallelism, maximizing, 180\\nparallelize(), 119\\nparameters, 326\\nParamGridBuilder, 317\\nParquet\\nabout, 60, 96\\nas a data source for DataFrames and SQL\\ntables, 97-100\\nreading files into DataFrames, 97\\nreading files into Spark SQL tables, 97\\nwriting DataFrames to, 99\\npartitionColumn property, 130\\npartitions\\nabout, 44creating, 181\\ndistributed data and, 12\\nimportance of, 130\\nnumber of for shuffles, 262\\nshuffle, 182\\nperformance, 262, 265\\npersist(), 183-187\\npersistence, caching data and, 183-187\\nPhysical planning phase (Spark SQL), 81\\npickle serialization library (Python), 115\\nPipeline API, 289\\npipelines\\ncreating, 296-302\\ndefined, 290\\ndesigning, 289-307\\noptimizing, 320-321\\npivoting, 153\\nPMML (Predictive Model Markup Language),\\n335\\nport 4040, 197\\nPostgreSQL database, 86, 132\\nprocess() method, 233\\nprocessedRowsPerSecond column (Streaming‐\\nQuery), 224\\nprocessing details, specifying, 216\\nprocessing engine, data lakes and, 269\\nprocessing-time timeouts, 257\\nProcessingTime trigger, 217\\nProject Hydrogen, 15, 351\\nProject Tungsten, 4, 16, 82, 119, 167\\nprojections, DataFrames and, 61\\nprojects component (MLflow), 324\\npublishing metrics using Dropwizard Metrics,\\n224\\nPyPI repository, 20\\nPySpark, 115-117, 354\\nPySpark shell, 22-25\\nPython\\ncolumns and, 55\\ndata types, 49\\ntype hints, 354\\nQ\\nqueries\\nmonitoring active, 223-225\\nsnapshots of tables, 283\\nstarting, 218\\nStructured Streaming, 213-225\\nwith Spark SQL Shell, 119\\n368 | Index', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 391}), Document(page_content='R\\nR library, 21\\nR2, 302-306\\nR2D3, 309\\nrandom forests, 313-316\\nrandomSplit(), 293\\nranking functions, 149\\nRDBSs (relational database management sys‐\\ntems), 1\\nRDD (Resilient Distributed Dataset), 5, 16, 43,\\n75\\nrdd.getNumPartitions(), 13\\nread(), 29\\nreading\\nAvro files into DataFrames, 104\\nAvro files into Spark SQL tables, 105\\nbinary files into DataFrames, 110\\nCSV files into DataFrames, 102\\nCSV files into Spark SQL tables, 102\\nfrom data lakes, 269\\nfrom databases, 267\\nfrom files, 226\\nfrom Kafka, 228\\nimage files into DataFrames, 108\\nJSON files into DataFrames, 100\\nJSON files into Spark SQL tables, 100\\nORC files into DataFrames, 107\\nORC files into Spark SQL tables, 107\\nParquet files into DataFrames, 97\\nParquet files into SQL tables, 97\\ntables into DataFrames, 93\\nREADME.md file, 21\\nreal-time inference, export patterns for, 334\\nreceivers, 169\\nrecord Name property, 106\\nrecord-at-a-time processing model, 207\\nrecordNamespace property, 106\\nredesigning Pandas UDFs, 354\\nreduce() function, 144, 162\\nreduceByKey(), 187\\nregistry component (MLflow), 324\\nregression\\ndecision trees, 308-313\\nlinear, 294\\nlogistic, 287\\nrandom forests, 313-316\\nrename() method, 153\\nrenaming columns, 63, 153\\nREST API, 324reverse() function, 139\\nRFormula, 300\\nRISELab, 3\\nRMSE (root-mean-square error), 302\\nrollup(), 235\\nroot, of decision trees, 308\\nRow objects, 57\\nrows\\ngeneric, 69\\nin DataFrames, 57\\nrandom feature selection by, 313\\nRuiz, Edgar, Mastering Spark with R, xvi, 285\\nrunID column (StreamingQuery), 224\\nrunning Spark SQL queries, 120\\nruns, 325\\nruntime architecture (Spark), 11, 74, 96, 170,\\n203, 234, 345\\nS\\nsample data, 160, 162-167\\nsampleBy() method, 68\\nsave() method, 96\\nsaveAsTable() method, 96\\nsaving models, 306\\nsbt (Scala build tool), 40\\nScala\\nbuilding standalone applications in, 40\\ncase classes in, 71\\ncolumns and, 55\\nsingle API for, 157-160\\nusing, 22-25\\nScala shell, 23, 274\\nscalability\\nof databases, 268\\nof storage solutions, 265\\nSpark, 177-182\\nscalar Pandas UDFs, 116\\nSCD (slowly changing dimension), 271\\nschedulers, 351\\nschema enforcement/governance, 271, 278-279\\nschema() method, 94\\nSchemaRDDs, 43\\nschemas, 50-54, 279\\nscikit-learn, 289, 310, 312, 336, 339\\nsecond-generation Tungsten engine, 167\\nselect() method, 28, 61, 73, 162, 215, 235\\nselectExpr() function, 138\\nsemantic guarantees, with watermarks, 245\\nsep property, 103\\nIndex | 369', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 392}), Document(page_content='sequence() function, 139\\nSerDe (serialization and deserialization), 169\\nShark, 113\\nshells\\nPySpark, 22-25\\nScala, 23, 274\\nSpark, 25, 85, 119\\nSpark SQL, 119\\nSHJ (shuffle hash join), 349\\nshuffle partitions, 182\\nshuffle service, 179\\nshuffle() function, 139\\nshuffles, 187, 262\\nSIMD (Single Instruction, Multiple Data), 167\\nsingular API, 157-160\\nsinks\\ncustom, 230-234\\noptions for, 222\\nstreaming, 226-234\\nskew, 199\\nsklearn, 332\\nslice() function, 139\\nSMJ (shuffle sort merge join), 189-197, 349\\nsnapshots, querying, 283\\nSNLJ (shuffle-and-replicate nested loop join),\\n349\\nSnowflake, 137\\nsoftware, user in this book, xviii\\nsortBy(), 187\\nsource options, 222\\nsource rate limits, setting, 262\\nSpark\\nabout, xvi, 4\\napplication concepts, 25-28\\nbuilding lakehouses with Delta Lake and,\\n274-283\\ncommunity adoption/expansion of, 16\\ndesign characteristics of, 4\\ndevelopers and, 14-17\\ndirectories, 21\\ndistributed execution, 10-14\\ndownloading, 19-22\\nearly years of, 3\\nease of use of, 5\\nevolution of, 1\\nextensibility of, 5\\nfiles, 21\\ninternal format compared with Java Object\\nFormat, 168leveraging for non-MLlib models, 336-340\\nmodularity of, 5\\nscaling, 177\\nspeed of, 4\\nstructuring, 44-47\\nuses for, 14-17\\nSpark + AI Summit, 16\\nSpark APIs, 16\\nSpark Cassandra Connector, 231\\nSpark Core, 343\\nSpark shell, 25, 85, 119\\nSpark SQL\\nabout, 6, 7, 76, 343\\nApache Hive and, 113-155\\nbasic query examples, 84-89\\nCatalyst optimizer, 77-82\\nDatasets and, 157-172\\nevaluation order, 115\\nhigher-order functions in, 138-144\\nJDBC database, 89, 129-131\\njoins, 348\\nnull checking, 115\\noperations, 144-155\\ntables, 89-94\\nusage and interface, 83\\nuser-defined functions (UDFs), 114\\nusing in Spark applications, 84-89\\nviews, 89-94\\nSpark Thrift JDBC/ODBC server, 121\\nSpark Tuning Guide, 320\\nSpark UI, 31-33, 197, 197-205\\nSpark web UI (see web UI)\\nspark-shell, 31\\nspark-submit script, 21, 174, 179, 197, 352\\nspark.executor.memory, 178\\nspark.local.dir option, 182\\nspark.ml package, 8, 312\\nspark.mllib package, 8\\nspark.read(), 247\\nspark.read.csv() function, 60\\nspark.readStream(), 247, 333\\nspark.sql programmatic interface, 85\\nSparkConf object, 11\\nSparkContext object, 11, 263\\nSparkling Water, 335\\nsparklyr, 21\\nSparkR project, 21\\nSparkSession, 10, 26, 85, 92\\nSparseVector, 298\\n370 | Index', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 393}), Document(page_content='speed, of Spark, 4\\nSQL databases, 90, 129-131\\nSQL tab (Spark UI), 202\\nSQL tables\\nabout, 60\\ncaching, 93\\ncreating, 119\\ndata sources for, 94-112\\ninserting data into, 120\\nreading Avro files into, 105\\nreading CSV files into, 102\\nreading JSON files into, 100\\nreading ORC files into, 107\\nreading Parquet files into, 97\\nwriting DataFrames to, 99\\nsql() method, 84\\nSQLContext object, 11\\nSQLLine CLI, 120\\nstages, 26, 28\\nStages tab (Spark UI), 198\\nstandalone applications\\nbuilding in Scala, 40\\nCounting M&Ms example, 35-39\\nScala/Java project using Maven coordinates,\\n274\\nStandalone cluster manager, 12\\nstar syntax, 281\\nstart() method, 218, 221\\nstart-thriftserver.sh, 121\\nstat() method, 68\\nstateful streaming aggregations, 238-246\\nstateful transformations, 215, 235\\nstateless transformations, 215, 235\\nstatic resource allocation, 177\\nstddev() , 239\\nstorage\\ndata lakes and, 269\\nimportance of optimal, 265\\nlevels of, 184\\nStorage tab (Spark UI), 200\\nstream-static joins, 246\\nstream-stream joins, 248-252\\nstreaming (see Structured Streaming)\\nstreaming DataFrame, 214\\nstreaming joins, 246, 248-252\\nstreaming queries, multiple, 263\\nstreaming sources, custom, 230-234\\nstreaming state, 234\\nStreamingContext object, 11StreamingQuery, 223\\nStreamingQueryListener, 225\\nstreams, loading into Delta Lake tables, 277\\nStringIndexer, 298, 320\\nStructured APIs\\nabout, 43\\nDataFrame API, 47-69\\nDataset API, 69-74\\nResilient Distributed Dataset (RDD), 43\\nSpark SQL, 76-82\\nstructuring Spark, 44-47\\nstructured data, 49\\nStructured Streaming, 207-264\\nabout, 6, 8, 352\\nAPIs, 8\\narbitrary stateful computations, 253-261\\ncheckpointing, 217\\ndata engineers and, 15\\ndata sources, 226-234\\ndata transformations, 234-237\\nloading data streams into Delta Lake tables,\\n277\\nmicro-batch stream processing, 208\\nMLflow and, 333\\nperformance tuning, 262\\nphilosophy of, 210\\nprogramming model of, 211-213\\nqueries, 213-225\\nsinks, 226-234\\nstateful streaming aggregations, 238-246\\nstream processing engine, 207-211\\nstreaming joins, 246-252\\nSTS (Spark Thrift Server), 119\\nsum() method, 67, 239\\nsupervised machine learning, 286\\nSupport Vector Machines, 287\\nsupported languages, 357\\nT\\nTableau, querying with, 122-129\\ntables\\nabout, 89\\n(see also SQL tables)\\ncreating SQL databases and, 90\\nDelta Lake, 275, 281\\nreading into DataFrames, 93\\nunmanaged compared with managed, 89\\ntabs (Spark UI), 197-205\\ntake() method, 73\\nIndex | 371', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 394}), Document(page_content='tasks, 26, 28\\ntemporary views, 92\\nTensorFlow, 332, 336\\ntest data sets, creating, 291-293\\nthird-party Spark packages, 5\\nThrift JDBC/ODBC server, 119\\nthroughput, 330\\nTibshirani, Robert, The Elements of Statistical\\nLearning, 309\\ntimeouts, managing interactive groups using,\\n257-261\\nto_date() function, 64\\nto_json() function, 138\\nto_timestamp() function, 64\\ntracking component (MLflow), 325-330\\ntraining data, 291-293\\ntransaction support, 265, 271\\ntransform() function, 142, 293\\ntransformations\\nabout, 28-30\\ndata, 279-282\\nDataFrames and, 61, 222\\nnarrow, 30\\nsample data, 162-167\\nstateful, 215, 235\\nstateless, 215, 235\\nStructured Streaming and, 234-237\\nwide, 30\\ntransformers, 290, 293\\ntree-based models, 307-316\\ntriggering details, 217\\ntuning Spark, 173-205\\ntuples, 50\\ntyped APIs, 69\\ntyped objects, 69\\nU\\nUber Engineering, 272\\nUDFs (see user-defined functions (UDFs))\\nunion() method, 147, 357\\nunionAll() method, 357\\nunions, 147\\nunmanaged stateful transformations, 237\\nunmanaged tables, 89\\nuntyped APIs, 69\\nuntyped objects, 69\\nUpdate mode (Structured Streaming), 212, 215,\\n245\\nupdating data, 280upperBound property, 130\\nupserting, 271, 281\\nuser-defined functions (UDFs), 114-117, 239,\\n336\\nuser-related data, deleting, 280\\nutility functions, 138\\nV\\nvalidation data set, 316\\nvariables, shared (see shared variables)\\nVectorAssembler transformer, 293\\nVectorIndexer, 300\\nvectorized ORC reader, 106\\nvectorized reader, 106\\nvectorized UDFs (see Pandas, user-defined\\nfunctions (UDFs))\\nviews\\nabout, 89\\ncreating, 91\\nmetadata, 93\\nW\\nwatermark delay, 243\\nwatermarks\\nhandling late data with, 243\\ninner joins with optional, 248-252\\nouter joins with, 252\\nsemantic guarantees with, 245\\nweb UI, 31, 197\\nWendell, Patrick, xv\\nwhere() method, 61, 235\\nwhole-stage code generation, 119\\nwide dependencies, 30\\nwide transformations (see shuffle partitions)\\nwindow() function, 149, 240\\nwindowed transformations, 210\\nwindowing, 149-151\\nwithColumn() method, 64, 152\\nwithWatermark(), 244\\nworkloads, 265, 269, 271\\nwriting\\nDataFrames into Avro files, 105\\nDataFrames into CSV files, 103\\nDataFrames into JSON files, 101\\nDataFrames to ORC files, 108\\nDataFrames to Parquet files, 99\\nDataFrames to Spark SQL tables, 99\\nfrom databases, 267\\nto data lakes, 269\\n372 | Index', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 395}), Document(page_content='to files, 227\\nto Kafka, 229\\nto storage systems, 230\\nX\\nXGBoost, 323, 335\\nXGBoost4J-Spark library, 335Y\\nY ahoo!, 2\\nYARN (see Hadoop YARN)\\nyear() function, 65\\nZ\\nZaharia, Matei, xv\\nIndex | 373', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 396}), Document(page_content='About the Authors\\nJules S. Damji  is a senior developer advocate at Databricks and an MLflow contribu‐\\ntor. He is a hands-on developer with over 20 years of experience and has worked as a\\nsoftware engineer at leading companies such as Sun Microsystems, Netscape,\\n@Home, Loudcloud/Opsware, Verisign, ProQuest, and Hortonworks, building large-\\nscale distributed systems. He holds a B.Sc. and an M.Sc. in computer science and an\\nMA in political advocacy and communication from Oregon State University, Cal\\nState, and Johns Hopkins University, respectively.\\nBrooke Wenig  is a machine learning practice lead at Databricks. She leads a team of\\ndata scientists who develop large-scale machine learning pipelines for customers, as\\nwell as teaching courses on distributed machine learning best practices. Previously,\\nshe was a principal data science consultant at Databricks. She holds an M.S. in com‐\\nputer science from UCLA with a focus on distributed machine learning.\\nTathagata Das  is a staff software engineer at Databricks, an Apache Spark committer,\\nand a member of the Apache Spark Project Management Committee (PMC). He is\\none of the original developers of Apache Spark, the lead developer of Spark Stream‐\\ning (DStreams), and is currently one of the core developers of Structured Streaming\\nand Delta Lake. Tathagata holds an M.S. in computer science from UC Berkeley.\\nDenny Lee  is a staff developer advocate at Databricks who has been working with\\nApache Spark since 0.6. He is a hands-on distributed systems and data sciences engi‐\\nneer with extensive experience developing internet-scale infrastructure, data plat‐\\nforms, and predictive analytics systems for both on-premises and cloud\\nenvironments. He also has an M.S. in biomedical informatics from Oregon Health\\nand Sciences University and has architected and implemented powerful data solu‐\\ntions for enterprise healthcare customers.\\nColophon\\nThe animal on the cover of Learning Spark , Second Edition, is the small-spotted cat‐\\nshark ( Scyliorhinus canicula ), an abundant species in the shallow waters of the Medi‐\\nterranean Sea and in the Atlantic, off the coast of Europe and northern Africa. It is a\\nsmall, slender shark with a blunt head, oval eyes, and a rounded snout. The dorsal\\nsurface is grayish-brown and patterned with many small dark and sometimes lighter\\nspots. Like other sharks, its skin texture is formed of “dermal denticles, ” tiny “teeth”\\nthat grow all in one direction (like fish scales), forming a surface that’s both hydrody‐\\nnamic as well as resistant to injuries and parasites.\\nThis night-feeding shark grows to about 3 feet long, weighs an average of 3 pounds at\\nmaturity, and in the wild can live up to 12 years. It feeds mostly on mollusks, crusta‐\\nceans, cephalopods, and polychaete worms, though it also eats other fish. This species', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 397}), Document(page_content='exhibits some social behaviors, especially when young, and a 2014 study conducted\\nby the University of Exeter found that individuals displayed differing social personali‐\\nties. Across changes in habitat, some sharks preferred staying in conspicuous groups,\\nwhile others remained alone, camouflaged at the bottom of the habitat. These sociali‐\\nzation behaviors also reflect a variability in strategies for safety, either through num‐\\nbers or via camouflage.\\nThis catshark is oviparous (egg-laying), and females deposit 18-20 small egg cases\\neach year. These hard-shelled cases have tendrils that catch on seaweed at the ocean\\nfloor; each case contains one young shark. The young hatch after about nine months.\\nBecause the small-spotted catshark is undesirable to commercial fisheries, popula‐\\ntions are currently stable and the species is listed by the IUCN as being of Least Con‐\\ncern. Many of the animals on O’Reilly covers are endangered; all of them are\\nimportant to the world.\\nThe cover illustration is by Karen Montgomery, based on a black and white engraving\\nfrom J. G. Wood’s Animate Creation  (1885). The cover fonts are Gilroy Semibold and\\nGuardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad\\nCondensed; and the code font is Dalton Maag’s Ubuntu Mono.', metadata={'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 398}), Document(page_content='MURPH\\nTRAINING \\n13 WEEK PROGRAM\\n', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 0}), Document(page_content='Murph is one of the longest Hero WODs and arguably the hardest. \\nThe workout is: \\n1 mile run\\n100 pull-ups\\n200 push-ups\\n300 squats\\n1 mile run\\nMost athletes need more than 40 minutes to complete Murph, and few show\\nup prepared to work consistently for that amount of time. To get through\\nMurph in one piece, this training program will address endurance as well as\\nstrength and speed. Similar to how a runner trains for a marathon, this\\nprogram will help prepare you for Murph by incorporating the individual\\nmovements, gradually increasing the number of reps so that your body\\nbecomes accustomed to longer periods of time under tension. While the\\ntraining plan will not have you do 300 squats in one day, it will prepare you for\\nhandling this volume during the actual event.\\nAbout Murph\\nAbout the Training\\n2\\n   @PROGRESSIONLIFE\\n', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 1}), Document(page_content='Facing reps in the triple digits is intimidating, even for experienced\\nCrossFitters. To help build an athlete’s endurance and confidence, this\\nprogram will incorporate generous rest periods at first. As the program\\nprogresses, volume of reps will increase, as rest periods decrease. If you are a\\nfast runner, we will encourage you to pace yourself. Resist the urge to PR your\\nfirst mile. Instead, use a pace that you could realistically sustain for a 10K race.\\nIf you plan to wear a weighted vest, a tight fit is crucial. Find a vest that feels\\nlike it’s a part of you during every movement. Murph is brutal on the body, but\\nit’s just as tough on the brain. While building endurance is essential, self-\\nawareness and mental fortitude is just as important. It’s not enough to simply\\n“embrace the suck.” You need to understand and anticipate it. This program\\nwill prepare you for this.\\n   @PROGRESSIONLIFE\\n3Things to Consider\\nPlanning Your Attack\\nTHE TIME TO START PREPPING IS NOW!\\xa0\\nHERE IS YOUR PLAN!\\nSEE YOU ON MEMORIAL DAY!\\n', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 2}), Document(page_content='   @PROGRESSIONLIFE\\n4Coaches Notes\\nSCALING\\nVESTS\\nPULL-UPS & PUSH-UPSThis plan is written for a relatively fit individual.\\nPlease scale appropriately to your fitness level or consult your primary care provider\\nbefore beginning this exercise program.   \\nIf you plan on wearing a vest I suggest you start wearing it week 3, during the\\nmovement prep.Take it off as needed.  \\nDo not wear it while running until week 6 and again, take it off as needed. 1 interval on,\\n1 interval off is a good way to start with vested running.\\nThere are many scaling options for pull-ups and push-ups.\\nIf you currently struggle with pull-ups and/or push-ups talk to a coach about an\\nappropriate plan for you.\\nSQUATS\\nHEALTHY HABITSRUNNING\\nIf you are new to running, stick to week 1 conditioning workouts until you feel ready\\nto move on.\\nIf you are not familiar with the CrossFit style air squat, please clink this LINK to watch a\\nvideo for proper form.\\nTo get the most out of this training program and to have the best results during the\\nMurph event, we recommend eating a majority of real food: Lean protein sources,\\nvegetables, nuts and seeds, healthy complex carbohydrates, and limit sugar intake. We\\nalso recommend drinking water and sleeping at a minimum of 7 hours per night when\\npossible. If you need guidance from a Progression nutrition coach, click this LINK\\n', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 3}), Document(page_content='   @PROGRESSIONLIFE50 1\\n0 2\\n0 3Murph 1.1\\nMurph 1.2\\nMurph 1.3W E E K    1  \\nPART A: \\nMOVEMENT PREP\\n \\n3 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n10 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n4x 400m Runs\\n \\nRest 3 minutes\\n between each runWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-upsPART A:\\nMOVEMENT PREP\\n \\n3 Rounds\\nNOT FOR TIME\\n \\n5 push-ups\\n5 pull-ups\\n5 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n2x 800m runs\\n \\nRest 4 minutes \\nbetween each run\\nPART A:\\nMovement Prep\\n \\n3 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n5 squats\\n5 push-ups\\n10 air squats\\n5 push-upsPART B:\\nCONDITIONING\\n \\n4x 400m runs\\n \\nRest 3 minutes\\nbetween each run\\n \\nRecord interval times\\n', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 4}), Document(page_content='   @PROGRESSIONLIFE6 0 4\\n0 5\\n0 6Murph 2.1\\nMurph 2.2\\nMurph 2.3W E E K    2  \\nPART A: \\nMOVEMENT PREP\\n \\n3 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n10 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n4x 400m Runs\\n \\nRest 3 minutes\\n between each runWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-upsPART A:\\nMOVEMENT PREP\\n \\n3 Rounds\\nNOT FOR TIME\\n \\n5 push-ups\\n5 pull-ups\\n5 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n2x 800m runs\\n \\nRest 4 minutes \\nbetween each run\\nPART A:\\nMovement Prep\\n \\n3 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n5 air squats\\n5 push-ups\\n10 air squats\\n5 push-upsPART B:\\nCONDITIONING\\n \\n4x 400m runs\\n \\nRest 3 minutes\\nbetween each run\\n \\nRecord interval times\\n', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 5}), Document(page_content='   @PROGRESSIONLIFE7 0 7\\n0 8\\n0 9Murph 3.1\\nMurph 3.2\\nMurph 3.3W E E K    3  \\nPART A: \\nMOVEMENT PREP\\n \\n4 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n10 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n5x 400m Runs\\n \\nRest 3 minutes\\n between each runWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-upsPART A:\\nMOVEMENT PREP\\n \\n4 Rounds\\nNOT FOR TIME\\n \\n5 push-ups\\n5 pull-ups\\n5 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n3x 800m runs\\n \\nRest 4 minutes \\nbetween each run\\nPART A:\\nMovement Prep\\n \\n4 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n5 air squats\\n5 push-ups\\n10 air squats\\n5 push-upsPART B:\\nCONDITIONING\\n \\n5x 400m runs\\n \\nRest 3 minutes\\nbetween each run\\n \\nRecord interval times\\n', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 6}), Document(page_content='   @PROGRESSIONLIFE8 1 0\\n1 1\\n1 2Murph 4.1\\nMurph 4.2\\nMurph 4.3W E E K    4  \\nPART A: \\nMOVEMENT PREP\\n \\n4 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n10 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n5x 400m Runs\\n \\nRest 2 minutes\\n between each runWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-upsPART A:\\nMOVEMENT PREP\\n \\n4 Rounds\\nNOT FOR TIME\\n \\n5 push-ups\\n5 pull-ups\\n5 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n3x 800m runs\\n \\nRest 3 minutes \\nbetween each run\\nPART A:\\nMovement Prep\\n \\n4 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n5 air squats\\n5 push-ups\\n10 air squats\\n5 push-upsPART B:\\nCONDITIONING\\n \\n5x 400m runs\\n \\nRest 2 minutes\\nbetween each run\\n \\nRecord interval times\\n', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 7}), Document(page_content='   @PROGRESSIONLIFE9 1 3\\n1 4\\n1 5Murph 5.1\\nMurph 5.2\\nMurph 5.3W E E K    5  \\nPART A: \\nMOVEMENT PREP\\n \\n5 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n10 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n5x 400m Runs\\n \\nRest 2 minutes\\n between each runWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-upsPART A:\\nMOVEMENT PREP\\n \\n5 Rounds\\nNOT FOR TIME\\n \\n5 push-ups\\n5 pull-ups\\n5 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n4x 800m runs\\n \\nRest 3 minutes \\nbetween each run\\nPART A:\\nMovement Prep\\n \\n5 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n5 air squats\\n5 push-ups\\n10 air squats\\n5 push-upsPART B:\\nCONDITIONING\\n \\n6x 400m runs\\n \\nRest 2 minutes\\nbetween each run\\n \\nRecord interval times\\n', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 8}), Document(page_content='   @PROGRESSIONLIFE10 1 6\\n1 7\\n1 8Murph 6.1\\nMurph 6.2\\nMurph 6.3W E E K    6  \\nPART A: \\nMOVEMENT PREP\\n \\n5 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n10 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n6x 400m Runs\\n \\nRest 2 minutes\\n between each runWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-upsPART A:\\nMOVEMENT PREP\\n \\n5 Rounds\\nNOT FOR TIME\\n \\n5 push-ups\\n5 pull-ups\\n5 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n4x 800m runs\\n \\nRest 3 minutes \\nbetween each run\\nPART A:\\nMovement Prep\\n \\n5 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n5 air squats\\n5 push-ups\\n10 air squats\\n5 push-upsPART B:\\nCONDITIONING\\n \\n6x 400m runs\\n \\nRest 2 minutes\\nbetween each run\\n \\nRecord interval times\\n', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 9}), Document(page_content='   @PROGRESSIONLIFE11 1 9\\n2 0\\n2 1Murph 7.1\\nMurph 7.2\\nMurph 7.3W E E K    7  \\nPART A: \\nMOVEMENT PREP\\n \\n6 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n10 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n6x 400m Runs\\n \\nRest 2 minutes\\n between each runWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-upsPART A:\\nMOVEMENT PREP\\n \\n6 Rounds\\nNOT FOR TIME\\n \\n5 push-ups\\n5 pull-ups\\n5 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n4x 800m runs\\n \\nRest 3 minutes \\nbetween each run\\nPART A:\\nMovement Prep\\n \\n6 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n5 air squats\\n5 push-ups\\n10 air squats\\n5 push-upsPART B:\\nCONDITIONING\\n \\n6x 400m runs\\n \\nRest 2 minutes\\nbetween each run\\n \\nRecord interval times\\n', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 10}), Document(page_content='2 2\\n2 3\\n2 4\\n   @PROGRESSIONLIFE12Murph 8.1\\nMurph 8.2\\nMurph 8.3W E E K    8 :  r e c o v e r y\\nw e e k  \\nPART A: \\nMOVEMENT PREP\\n \\n3 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n10 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n4x 400m Runs\\n \\nRest 2 minutes\\n between each runWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-upsPART A:\\nMOVEMENT PREP\\n \\n3 Rounds\\nNOT FOR TIME\\n \\n5 push-ups\\n5 pull-ups\\n5 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n2x 800m runs\\n \\nRest 3 minutes \\nbetween each run\\nPART A:\\nMovement Prep\\n \\n3 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n5 air squats\\n5 push-ups\\n10 air squats\\n5 push-upsPART B:\\nCONDITIONING\\n \\n4x 400m runs\\n \\nRest 2 minutes\\nbetween each run\\n \\nRecord interval times', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 11}), Document(page_content='2 5\\n2 6\\n2 7\\n   @PROGRESSIONLIFE13Murph 9.1\\nMurph 9.2\\nMurph 9.3W E E K    9\\nPART A: \\nMOVEMENT PREP\\n \\n7 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n10 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n6x 400m Runs\\n \\nRest 1 minute\\n between each runWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-upsPART A:\\nMOVEMENT PREP\\n \\n7 Rounds\\nNOT FOR TIME\\n \\n5 push-ups\\n5 pull-ups\\n5 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n4x 800m runs\\n \\nRest 2 minutes \\nbetween each run\\nPART A:\\nMovement Prep\\n \\n7 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n5 air squats\\n5 push-ups\\n10 air squats\\n5 push-upsPART B:\\nCONDITIONING\\n \\n5x 400m runs\\n \\nRest 1 minute\\nbetween each run\\n \\nRecord interval times', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 12}), Document(page_content='2 8\\n2 9\\n3 0\\n   @PROGRESSIONLIFE14Murph 10.1\\nMurph 10.2\\nMurph 10.3W E E K    1 0\\nPART A: \\nMOVEMENT PREP\\n \\n10 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n10 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n4x 400m Runs\\n \\nRest 1 minute\\n between each runWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-upsPART A:\\nMOVEMENT PREP\\n \\n5 Rounds\\nNOT FOR TIME\\n \\n5 push-ups\\n5 pull-ups\\n5 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n2x 800m runs\\n \\nRest 2 minutes \\nbetween each run\\nPART A:\\nMovement Prep\\n \\n10 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n5 air squats\\n5 push-ups\\n10 air squats\\n5 push-upsPART B:\\nCONDITIONING\\n \\n2x 400m runs\\n \\nRest 2 minutes\\nbetween each run\\n \\nRecord interval times', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 13}), Document(page_content='3 1\\n3 2\\n3 3\\n   @PROGRESSIONLIFE15Murph 11.1\\nMurph 11.2\\nMurph 11.3W E E K    1 1\\nPART A: \\nMOVEMENT PREP\\n \\n12 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n10 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n2x 400m runs\\n \\nRest 2 minutes\\n between each runWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-upsPART A:\\nMOVEMENT PREP\\n \\n5 Rounds\\nNOT FOR TIME\\n \\n5 push-ups\\n5 pull-ups\\n5 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n2x 1 mile runs\\n \\nRest 5 minutes \\nbetween each run\\nPART A:\\nMovement Prep\\n \\n10 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n5 air squats\\n5 push-ups\\n10 air squats\\n5 push-upsPART B:\\nCONDITIONING\\n \\n4x 400m runs\\n \\nRest 2 minutes\\nbetween each run\\n \\nRecord interval times', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 14}), Document(page_content='3 4\\n3 5\\n3 6\\n   @PROGRESSIONLIFE16Murph 12.1\\nMurph 12.2\\nMurph 12.3W E E K    1 2 :  T A P E R\\nS T A R T S\\nPART A: \\nMOVEMENT PREP\\n \\n10 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n10 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n4x 400m runs\\n \\nRest 2 minutes\\n between each runWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-upsPART A:\\nMOVEMENT PREP\\n \\n5 Rounds\\nNOT FOR TIME\\n \\n5 push-ups\\n5 pull-ups\\n5 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n4x 800m runs\\n \\nRest 2 minutes \\nbetween each run\\nPART A:\\nMovement Prep\\n \\n5 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n5 air squats\\n5 push-ups\\n10 air squats\\n5 push-upsPART B:\\nCONDITIONING\\n \\n4x 400m runs\\n \\nRest 2 minutes\\nbetween each run\\n \\nRecord interval times', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 15}), Document(page_content='3 7\\n3 8\\n3 9\\n   @PROGRESSIONLIFE17Murph 13.1\\nMurph 13.2\\nMurph 13.3W E E K    1 3 :  T A P E R  \\nPART A: \\nMOVEMENT PREP\\n \\n3 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n10 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n2x 1 mile runs\\n \\nRest 5 minutes\\n between each runWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-ups\\nWARM UP:\\n3 Rounds: \\nNOT FOR TIME\\n \\n10 Sampson stretch,\\n5 each leg\\n10 Overhead squats\\n10 sit-ups\\n10 supermans\\n10 push-ups\\n10 pull-upsPART A:\\nMOVEMENT PREP\\n \\n3 Rounds\\nNOT FOR TIME\\n \\n5 push-ups\\n5 pull-ups\\n5 push-ups\\n15 air squatsPART B: \\nCONDITIONING\\n \\n2x 800m runs\\n \\nRest 4 minutes \\nbetween each run\\nPART A:\\nMovement Prep\\n \\n3 Rounds\\nNOT FOR TIME\\n \\n5 pull-ups\\n5 air squats\\n5 push-ups\\n10 air squats\\n5 push-upsPART B:\\nCONDITIONING\\n \\n4x 400m runs\\n \\nRest 2 minutes\\nbetween each run\\n \\nRecord interval times', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 16}), Document(page_content='   @PROGRESSIONLIFE18 Murph W E E K    1 4 :  m u r p h  \\n1 mile run\\n100 pull-ups\\n200 push-ups\\n300 air squats\\n1 mile run', metadata={'source': 'docs\\\\pdfs\\\\Murph-training.pdf', 'page': 17})]\n",
      "432\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"./docs/pdfs\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "pdfs = loader.load()\n",
    "print(pdfs)\n",
    "print(len(pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain.schema.Document'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(pdfs[0]))\n",
    "print(type(pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Attention Is All You Need\\nAshish Vaswani\\x03\\nGoogle Brain\\navaswani@google.comNoam Shazeer\\x03\\nGoogle Brain\\nnoam@google.comNiki Parmar\\x03\\nGoogle Research\\nnikip@google.comJakob Uszkoreit\\x03\\nGoogle Research\\nusz@google.com\\nLlion Jones\\x03\\nGoogle Research\\nllion@google.comAidan N. Gomez\\x03y\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser\\x03\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin\\x03z\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\nyWork performed while at Google Brain.\\nzWork performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v5  [cs.CL]  6 Dec 2017' metadata={'source': 'docs\\\\pdfs\\\\attention_is_all_you_need.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pdfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "txt_loader = DirectoryLoader(\"./docs/new_articles\", glob=\"*.txt\", loader_cls=TextLoader)\n",
    "txt_docs = txt_loader.load()\n",
    "print(len(txt_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = pdfs + txt_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453\n"
     ]
    }
   ],
   "source": [
    "print(len(all_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the texts into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1295"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "persist_dir = './db'\n",
    "\n",
    "# using OpneAI embeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vectordb = Chroma.from_documents(texts, embedding, persist_directory=persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persists the db to disk\n",
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(persist_directory=persist_dir, embedding_function=embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a rertrieval QA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "retreiever = vectordb.as_retriever()\n",
    "docs = retreiever.get_relevant_documents(\"How much money did Pando raise?\")\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'similarity'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retreiever.search_type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(temperature=0.0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retreiever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cite Sources\n",
    "def process_llm_responses(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print(\"\\n\\nSources:\")\n",
    "    for source in llm_response['source_documents']:\n",
    "        source_pg = source.metadata.get('source', None)\n",
    "        page = source.metadata.get('page', None)\n",
    "        print(f\"{source_pg} - {page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pando raised $30 million in a Series B round.\n",
      "\n",
      "\n",
      "Sources:\n",
      "docs\\new_articles\\05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt - None\n",
      "docs\\new_articles\\05-03-ai-powered-supply-chain-startup-pando-lands-30m-investment.txt - None\n",
      "docs\\new_articles\\05-07-3one4-capital-driven-by-contrarian-bets-raises-200-million-new-fund.txt - None\n",
      "docs\\new_articles\\05-07-fintech-space-continues-to-be-competitive-and-drama-filled.txt - None\n"
     ]
    }
   ],
   "source": [
    "query = \"How much money did Pando raise?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_responses(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You can calculate the number of cores on your Spark cluster by looking at the number of executors and the number of cores per executor.\n",
      "\n",
      "\n",
      "Sources:\n",
      "docs\\pdfs\\LearningSpark2.0.pdf - 37\n",
      "docs\\pdfs\\LearningSpark2.0.pdf - 285\n",
      "docs\\pdfs\\LearningSpark2.0.pdf - 204\n",
      "docs\\pdfs\\LearningSpark2.0.pdf - 203\n"
     ]
    }
   ],
   "source": [
    "# Ask a Spark question\n",
    "query = \"How do I calculate the number of cores on my spark cluster?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_responses(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'docs\\\\pdfs\\\\LearningSpark2.0.pdf', 'page': 37}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response[\"source_documents\"][0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The best way to combine two data sources together in Spark is to use the DataFrame-based APIs and domain-specific language (DSL) queries. This allows developers to treat the data as a structured table and issue queries against it as they would a static table.\n",
      "\n",
      "\n",
      "Sources:\n",
      "docs\\pdfs\\LearningSpark2.0.pdf - 38\n",
      "docs\\pdfs\\LearningSpark2.0.pdf - 39\n",
      "docs\\pdfs\\LearningSpark2.0.pdf - 31\n",
      "docs\\pdfs\\LearningSpark2.0.pdf - 290\n"
     ]
    }
   ],
   "source": [
    "# Ask a Spark question\n",
    "query = \"Whats the best way to combine two data sources together in Spark?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_responses(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There is no set number of pushups, situps, pull-ups, and squats you should do everyday to prepare for The Murph. It is best to follow the specific workout program outlined in the Murph training plan.\n",
      "\n",
      "\n",
      "Sources:\n",
      "docs\\pdfs\\Murph-training.pdf - 14\n",
      "docs\\pdfs\\Murph-training.pdf - 15\n",
      "docs\\pdfs\\Murph-training.pdf - 4\n",
      "docs\\pdfs\\Murph-training.pdf - 6\n"
     ]
    }
   ],
   "source": [
    "# Ask a Murph question\n",
    "query = \"How many pushups, situps, pull-ups, and squats should I do everyday to prepare for The Murph?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_responses(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Transformer model is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. It outperforms other models in quality while being more parallelizable and requiring significantly less training cost.\n",
      "\n",
      "\n",
      "Sources:\n",
      "docs\\pdfs\\attention_is_all_you_need.pdf - 1\n",
      "docs\\pdfs\\attention_is_all_you_need.pdf - 0\n",
      "docs\\pdfs\\attention_is_all_you_need.pdf - 7\n",
      "docs\\pdfs\\attention_is_all_you_need.pdf - 2\n"
     ]
    }
   ],
   "source": [
    "# Ask a Transformer question\n",
    "query = \"How does the Transformer model work compared to other deep learning models?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_responses(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ChatGPT is a general-purpose chatbot that uses artificial intelligence to generate text after a user enters a prompt, developed by tech startup OpenAI. The chatbot uses GPT-4, a large language model that uses deep learning to produce human-like text. It was released on November 30, 2022 and has been regularly updated with new GPT models, the most recent being GPT-4. It can be used for a range of tasks, including programming, TV scripts, scientific concepts, email replies, listicles, blog ideas, summarization, debugging code, complex problem solving, and more. It has an API that was released on March 1, 2023, and users can save their chats.\n",
      "\n",
      "\n",
      "Sources:\n",
      "docs\\new_articles\\05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt - None\n",
      "docs\\new_articles\\05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt - None\n",
      "docs\\new_articles\\05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt - None\n",
      "docs\\new_articles\\05-03-chatgpt-everything-you-need-to-know-about-the-ai-powered-chatbot.txt - None\n"
     ]
    }
   ],
   "source": [
    "# Ask a ChatGPT question\n",
    "query = \"Tell me everything about ChatGPT\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_responses(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
